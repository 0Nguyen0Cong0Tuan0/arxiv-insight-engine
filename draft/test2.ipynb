{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05bff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ebd847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1\n",
      "\n",
      "## A Survey on 3D Gaussian Splatting\n",
      "\n",
      "\n",
      "Guikun Chen, and Wenguan Wang, _Senior Member, IEEE_\n",
      "\n",
      "\n",
      "**Abstract** —3D Gaussian splatting (GS) has emerged as a transformative technique in explicit radiance field and computer graphics.\n",
      "This innovative approach, characterized by the use of millions of learnable 3D Gaussians, represents a significant departure from\n",
      "mainstream neural radiance field approaches, which predominantly use implicit, coordinate-based models to map spatial coordinates\n",
      "to pixel values. 3D GS, with its explicit scene representation and differentiable rendering algorithm, not only promises real-time\n",
      "rendering capability but also introduces unprecedented levels of editability. This positions 3D GS as a potential game-changer for the\n",
      "next generation of 3D reconstruction and representation. In the present paper, we provide the first systematic overview of the recent\n",
      "developments and critical contributions in the domain of 3D GS. We begin with a detailed exploration of the underlying principles and\n",
      "the driving forces behind the emergence of 3D GS, laying the groundwork for understanding its significance. A focal point of our\n",
      "discussion is the practical applicability of 3D GS. By enabling unprecedented rendering speed, 3D GS opens up a plethora of\n",
      "applications, ranging from virtual reality to interactive media and beyond. This is complemented by a comparative analysis of leading\n",
      "3D GS models, evaluated across various benchmark tasks to highlight their performance and practical utility. The survey concludes by\n",
      "identifying current challenges and suggesting potential avenues for future research. Through this survey, we aim to provide a valuable\n",
      "resource for both newcomers and seasoned researchers, fostering further exploration and advancement in explicit radiance field.\n",
      "\n",
      "\n",
      "**Index Terms** —3D Gaussian Splatting, Explicit Radiance Field, Real-time Rendering, Scene Understanding\n",
      "\n",
      "\n",
      "✦\n",
      "\n",
      "\n",
      "\n",
      "**1** **I** **NTRODUCTION**\n",
      "\n",
      "# T is to convert a collection of views or videos capturing HE objective of image based 3D scene reconstruction\n",
      "\n",
      "a scene into a digital 3D model that can be computationally processed, analyzed, and manipulated. This hard\n",
      "and long-standing problem is fundamental for machines\n",
      "to comprehend the complexity of real-world environments,\n",
      "facilitating a wide array of applications such as 3D modeling\n",
      "and animation, robot navigation, historical preservation,\n",
      "augmented/virtual reality, and autonomous driving.\n",
      "The journey of 3D scene reconstruction began long\n",
      "before the surge of deep learning, with early endeavors\n",
      "focusing on light fields and basic scene reconstruction methods [1]–[3]. These early attempts, however, were limited by\n",
      "their reliance on dense sampling and structured capture,\n",
      "leading to significant challenges in handling complex scenes\n",
      "and lighting conditions. The emergence of structure-frommotion [4] and subsequent advancements in multi-view\n",
      "stereo [5] algorithms provided a more robust framework for\n",
      "3D scene reconstruction. Despite these advancements, such\n",
      "methods struggled with novel-view synthesis and texture\n",
      "loss. NeRF represents a quantum leap in this progression.\n",
      "By leveraging deep neural networks, NeRF enabled the\n",
      "direct mapping of spatial coordinates to color and density.\n",
      "The success of NeRF hinged on its ability to create continuous, volumetric scene functions, producing results with\n",
      "unprecedented fidelity. However, as with any burgeoning\n",
      "technology, this implementation came at a cost: **i** ) Computational Intensity. NeRF based methods are computationally\n",
      "intensive [6]–[9], often requiring extensive training times\n",
      "\n",
      "\n",
      "_•_ _G. Chen and W. Wang are with College of Computer Science and_\n",
      "_Technology, Zhejiang University (Email: guikunchen@gmail.com, wen-_\n",
      "_guanwang.ai@gmail.com)_\n",
      "\n",
      "_•_ _Corresponding Author: Wenguan Wang_\n",
      "\n",
      "_•_ _Paper List:_ _[https://github.com/guikunchen/Awesome3DGS](https://github.com/guikunchen/Awesome3DGS)_\n",
      "\n",
      "_•_ _[Benchmarks: https://github.com/guikunchen/3DGS-Benchmarks](https://github.com/guikunchen/3DGS-Benchmarks)_\n",
      "\n",
      "\n",
      "\n",
      "Fig. 1. The number of published papers and official GitHub stars on 3D\n",
      "[GS. The set of statistics is sourced from # Papers and # GitHub Stars.](https://github.com/Awesome3DGS/3D-Gaussian-Splatting-Papers)\n",
      "\n",
      "\n",
      "and substantial resources for rendering, especially for highresolution outputs. **ii** ) Editability. Manipulating scenes represented implicitly is challenging, since direct modifications\n",
      "to the neural network’s weights are not intuitively related to\n",
      "changes in geometric or appearance properties of the scene.\n",
      "It is in this context that 3D Gaussian splatting (GS) [10]\n",
      "emerges, not merely as an incremental improvement but as\n",
      "a paradigm-shifting approach that redefines the boundaries\n",
      "of scene representation and rendering. While NeRF excelled\n",
      "in creating photorealistic images, the need for faster, more\n",
      "efficient rendering methods was becoming increasingly apparent, especially for applications ( _e.g_ ., virtual reality and\n",
      "autonomous driving) that are highly sensitive to latency.\n",
      "3D GS addressed this need by introducing an advanced,\n",
      "explicit scene representation that models a scene using\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2\n",
      "\n",
      "\n",
      "\n",
      "millions of learnable 3D Gaussians in space. Unlike the implicit, coordinate-based models [11], [12], 3D GS employs an\n",
      "explicit representation and highly parallelized workflows,\n",
      "facilitating more efficient computation and rendering. The\n",
      "innovation of 3D GS lies in its unique blend of the benefits of differentiable pipelines and point-based rendering\n",
      "techniques [13]–[17]. By representing scenes with learnable\n",
      "3D Gaussians, it preserves the strong fitting capability of\n",
      "continuous volumetric radiance fields, essential for highquality image synthesis, while simultaneously avoiding\n",
      "the computational overhead associated with NeRF based\n",
      "methods ( _e.g_ ., computationally expensive ray-marching, and\n",
      "unnecessary calculations in empty space).\n",
      "The introduction of 3D GS is not just a technical advancement; it represents a fundamental shift in how we approach\n",
      "scene representation and rendering in computer vision and\n",
      "graphics. By enabling real-time rendering capabilities without compromising on visual quality, 3D GS opens up a\n",
      "plethora of possibilities for applications ranging from virtual reality and augmented reality to real-time cinematic\n",
      "rendering and beyond [18]–[21]. This technology holds the\n",
      "promise of not only enhancing existing applications but\n",
      "also enabling new ones that were previously unfeasible\n",
      "due to computational constraints. Furthermore, 3D GS’s\n",
      "explicit scene representation offers unprecedented flexibility\n",
      "to control the objects and scene dynamics, a crucial factor in\n",
      "complex scenarios involving intricate geometries and varying lighting conditions [22]–[24]. This level of editability,\n",
      "combined with the efficiency of the training and rendering\n",
      "process, positions 3D GS as a transformative force in shaping future developments in relevant fields.\n",
      "In an effort to assist readers in keeping pace with the\n",
      "swift evolution of 3D GS, we provide the first survey on 3D\n",
      "GS, which presents a systematic and timely collection of the\n",
      "most significant literature on the topic. Given that 3D GS\n",
      "is a very recent innovation (Fig. 1), this survey focuses in\n",
      "particular on its principles, and the diverse developments\n",
      "and contributions that have emerged since its introduction.\n",
      "The selected follow-up works are primarily sourced from\n",
      "top-tier conferences, to provide a thorough and up-to-date\n",
      "(Dec. 2024) analysis of the theoretical foundations, remarkable developments, and burgeoning applications of 3D GS.\n",
      "Acknowledging the nascent yet rapidly evolving nature of\n",
      "3D GS, this survey is inevitably a biased view, but we strive\n",
      "to offer a balanced perspective that reflects both the current\n",
      "state and the future potential of this field. Our aim is to\n",
      "encapsulate the primary research trends and serve as a valuable resource for both researchers and practitioners eager to\n",
      "understand and contribute to this rapidly evolving domain.\n",
      "The distinctions of this survey from existing literature [25]–\n",
      "\n",
      "[28] are evident in the following aspects:\n",
      "\n",
      "_•_ We provide the first systematic and comprehensive review\n",
      "that examines 3D GS from a macro-level perspective by\n",
      "establishing clear taxonomies and frameworks. This highlevel systematization helps researchers identify trends and\n",
      "potential directions that might not be apparent from paperspecific reviews. Our organizational structure serves as a\n",
      "roadmap for understanding how different approaches relate\n",
      "to and build upon each other within the 3D GS ecosystem.\n",
      "\n",
      "_•_ This paper is the first and only survey to thoroughly delve\n",
      "into the theoretical background and fundamental principles\n",
      "\n",
      "\n",
      "\n",
      "Fig. 2. Structure of the overall review.\n",
      "\n",
      "\n",
      "of 3D GS. The comprehensive coverage makes the field more\n",
      "approachable for newcomers while providing valuable insights for experienced researchers.\n",
      "\n",
      "_•_ To ensure our survey remains relevant and offer longterm value in this rapidly evolving field, we maintain two\n",
      "[dynamic GitHub repositories: one that follows our survey’s](https://github.com/guikunchen/Awesome3DGS)\n",
      "[organizational structure and another that includes compre-](https://github.com/guikunchen/3DGS-Benchmarks)\n",
      "hensive performance comparisons with analysis data.\n",
      "\n",
      "A summary of the structure of this article can be found\n",
      "in Fig. 2, which is presented as follows: Sec. 2 provides\n",
      "a brief background on problem formulation, terminology,\n",
      "and related research domains. Sec. 3 introduces the essential\n",
      "insights of 3D GS, encompassing the rendering process with\n",
      "learned 3D Gaussians and the optimization details ( _i.e_ ., how\n",
      "to learn 3D Gaussians) of 3D GS. Sec. 4 presents several\n",
      "fruitful directions that aim to improve the capabilities of\n",
      "the original 3D GS. Sec. 5 unveils the diverse application\n",
      "areas and tasks where 3D GS has made significant impacts,\n",
      "showcasing its versatility. Sec. 6 conducts performance comparison and analysis. Finally, Sec. 7 and 8 highlight the open\n",
      "questions for further research and conclude the survey.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3\n",
      "\n",
      "\n",
      "\n",
      "**2** **B** **ACKGROUND**\n",
      "\n",
      "\n",
      "In this section, we first provide a brief formulation of\n",
      "radiance fields (Sec. 2.1), including both implicit and explicit\n",
      "ones. Sec. 2.2 further establishes linkages with relevant rendering algorithms and terminologies. For a comprehensive\n",
      "overview of radiance fields, scene reconstruction and representation, and rendering methods, please see the excellent\n",
      "surveys [29]–[33] for more insights.\n",
      "\n",
      "\n",
      "**2.1** **Radiance Field**\n",
      "\n",
      "\n",
      "_•_ **Implicit Radiance Field.** An implicit radiance field represents light distribution in a scene without explicitly defining\n",
      "the geometry of the scene. In the deep learning era, neural\n",
      "networks are often used to learn a continuous volumetric\n",
      "scene representation [34], [35]. The most prominent example\n",
      "is NeRF [12]. In NeRF (Fig. 3a), one or more MLPs are used\n",
      "to map a set of spatial coordinates ( _x, y, z_ ) and viewing\n",
      "directions ( _θ, ϕ_ ) to color _c_ and volume density _σ_ :\n",
      "\n",
      "\n",
      "( _c, σ_ ) _←_ MLP ( _x, y, z, θ, ϕ_ ) _._ (1)\n",
      "\n",
      "\n",
      "This format allows for a differentiable and compact representation of complex scenes, albeit often at the cost of high\n",
      "computational load due to volumetric ray marching. Note\n",
      "that typically, the color _c_ is direction-dependent, whereas\n",
      "the volume density _σ_ is not [12].\n",
      "\n",
      "_•_ **Explicit Radiance Field.** An explicit radiance field directly\n",
      "represents the distribution of light in a discrete spatial structure, such as a voxel grid or a set of points [36], [37]. Each\n",
      "element in this structure stores the radiance information\n",
      "for its respective location. This allows for direct and often\n",
      "faster access to radiance data but at the cost of higher\n",
      "memory usage and potentially lower resolution. Similar to\n",
      "the implicit radiance field, the explicit one is written as:\n",
      "\n",
      "\n",
      "( _c, σ_ ) _←_ DataStructure ( _x, y, z, θ, ϕ_ ) _,_ (2)\n",
      "\n",
      "\n",
      "where DataStructure could be in the format of volumes,\n",
      "point clouds, _etc_ . DataStructure encodes directional color\n",
      "in two main ways. One is encoding high-dimensional features that are subsequently decoded by a lightweight MLP.\n",
      "Another one is directly storing coefficients of directional\n",
      "basis functions, such as spherical harmonics or spherical\n",
      "Gaussians, where the final color is computed as a function\n",
      "of these coefficients and the viewing direction.\n",
      "\n",
      "_•_ **3D Gaussian Splatting: Best-of-Both Worlds.** 3D GS [10]\n",
      "is an explicit radiance field with the advantages of implicit\n",
      "radiance fields. Concretely, it leverages the strengths of both\n",
      "paradigms by utilizing _learnable_ 3D Gaussians as the basis\n",
      "elements of DataStructure. Note that 3D GS encodes the\n",
      "opacity _α_ directly for each Gaussian, as opposed to approaches of first establishing density _σ_ and then computing\n",
      "opacity based on that density. As in previous reconstruction\n",
      "work, 3D Gaussians are optimized under the supervision of\n",
      "multi-view images to represent the scene. Such a 3D Gaussian based differentiable pipeline combines the benefits of\n",
      "neural network based optimization and explicit, structured\n",
      "data storage. This hybrid approach aims to achieve realtime, high-quality rendering and requires less training time,\n",
      "particularly for complex scenes and high-resolution outputs.\n",
      "\n",
      "\n",
      "\n",
      "**2.2** **Context and Terminology**\n",
      "\n",
      "_•_ **Volumetric rendering** aims to transform a 3D volumetric\n",
      "representation into an image by integrating radiance along\n",
      "camera rays. A camera ray _**r**_ ( _t_ ) can be parameterized as:\n",
      "_**r**_ ( _t_ )= _**o**_ + _t_ _**d**_ _, t_ _∈_ [ _t_ near _, t_ far ] _,_ where _**o**_ represents the ray origin\n",
      "(camera center), _**d**_ is the ray direction, and _t_ indicates the\n",
      "distance along the ray between near and far clipping planes.\n",
      "The pixel color _C_ ( _**r**_ ) is computed through a line integral\n",
      "along the ray _**r**_ ( _t_ ), mathematically expressed as [12]:\n",
      "\n",
      "\n",
      "_t_ far\n",
      "_C_ ( _**r**_ ) = _T_ ( _t_ ) _σ_ ( _**r**_ ( _t_ )) _c_ ( _**r**_ ( _t_ ) _,_ _**d**_ ) _dt,_ (3)\n",
      "� _t_ near\n",
      "\n",
      "\n",
      "where _σ_ ( _**r**_ ( _t_ )) is the volume density at point _**r**_ ( _t_ ), _c_ ( _**r**_ ( _t_ ) _,_ _**d**_ )\n",
      "is the color at that point, and _T_ ( _t_ ) is the transmittance. Raymarching directly approximates the volumetric rendering\n",
      "integral by systematically “stepping” along a ray and sampling the scene’s properties at discrete intervals. NeRF [12]\n",
      "shares the same spirit of ray-marching and introduces\n",
      "importance sampling and positional encoding to improve\n",
      "the quality of synthesized images. While providing highquality results, ray-marching is computationally expensive,\n",
      "especially for high-resolution images.\n",
      "\n",
      "_•_ **Point-based rendering** represents another class of rendering algorithms, of which 3D GS introduces a notable implementation. Its simplest form [38] rasterizes point clouds\n",
      "with a fixed size, which introduces drawbacks such as holes\n",
      "and rendering artifacts. Seminal works addressed these\n",
      "limitations through various methods, including: **i** ) splatting\n",
      "point primitives with a spatial extent [14], [15], [39], [40],\n",
      "and **ii** ) more recently, embedding neural features directly\n",
      "into points for subsequent network-based rendering [41],\n",
      "\n",
      "[42]. 3D GS uses 3D Gaussian as the point primitive that\n",
      "contains explicit attributes ( _e.g_ ., color and opacity) instead of\n",
      "implicit neural features. The rendering approach, _i.e_ ., pointbased _α_ -blending (exemplified in Eq. 5), shares the same\n",
      "image formation model as NeRF-style volumetric rendering\n",
      "(Eq. 3) [10], but demonstrates substantial speed advantages.\n",
      "This advantage originates from fundamental algorithmic\n",
      "differences. NeRFs approximate a line integral along a ray\n",
      "for each pixel, requiring expensive sampling. Point-based\n",
      "methods render point clouds using rasterization, which inherently benefits from parallel computational strategies [43].\n",
      "\n",
      "\n",
      "**3** **3D G** **AUSSIAN** **S** **PLATTING** **: P** **RINCIPLES**\n",
      "\n",
      "\n",
      "3D GS offers a breakthrough in real-time, high-resolution\n",
      "image rendering, without relying on deep neural networks.\n",
      "This section aims to provide essential insights of 3D GS. We\n",
      "first elaborate on how 3D GS synthesizes an image given\n",
      "well-constructed 3D Gaussians in Sec. 3.1, _i.e_ ., the forward\n",
      "process of 3D GS. Then, we introduce how to obtain wellconstructed 3D Gaussians for a given scene in Sec. 3.2, _i.e_ .,\n",
      "the optimization process of 3D GS.\n",
      "\n",
      "\n",
      "**3.1** **Rendering with Learned 3D Gaussians**\n",
      "\n",
      "\n",
      "Consider a scene represented by (millions of) optimized\n",
      "3D Gaussians. The objective is to generate an image from\n",
      "a specified camera pose. Recall that NeRFs approach this\n",
      "task through computationally demanding volumetric raymarching, sampling 3D space points per pixel. Such a\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_•_ **Rendering by Pixels.** Before delving into the final version\n",
      "of 3D GS which utilizes several techniques to boost parallel\n",
      "computation, we first elaborate on its simpler form to offer\n",
      "insights into its basic working mechanism. Given the position of a pixel _**x**_, its distance to all overlapping Gaussians,\n",
      "_i.e_ ., the depths of these Gaussians, can be computed through\n",
      "the viewing transformation matrix _**W**_, forming a sorted list\n",
      "of Gaussians _N_ . Then, _α_ -blending is adopted to compute\n",
      "the final color of this pixel:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_c_ & _σ_\n",
      "\n",
      "\n",
      "\n",
      "Image Space\n",
      "\n",
      "\n",
      "\n",
      "Splatting\n",
      "\n",
      "\n",
      "(b) 3D GS\n",
      "\n",
      "\n",
      "\n",
      "(a) NeRF\n",
      "\n",
      "\n",
      "\n",
      "_n−_ 1\n",
      "�\n",
      "\n",
      "_j_ =1\n",
      "\n",
      "\n",
      "\n",
      "Fig. 3. NeRFs _vs_ . 3D GS. (a) NeRF samples along the ray and then\n",
      "queries the MLP to obtain corresponding colors and densities, which\n",
      "can be seen as a _backward_ mapping (ray tracing). (b) In contrast, 3D GS\n",
      "projects all 3D Gaussians into the image space ( _i.e_ ., splatting) and then\n",
      "performs parallel rendering, which can be viewed as a _forward_ mapping\n",
      "(rasterization). Best viewed in color.\n",
      "\n",
      "\n",
      "paradigm struggles with high-resolution image synthesis,\n",
      "failing to achieve real-time rendering, especially for platforms with limited computing resources [10]. By contrast,\n",
      "3D GS begins by projecting these 3D Gaussians onto a pixelbased image plane, a process termed “splatting” [39], [40]\n",
      "(see Fig. 3b). Afterwards, 3D GS sorts these Gaussians and\n",
      "computes the value for each pixel. As shown in Fig. 3, the\n",
      "rendering of NeRFs and 3D GS can be viewed as an inverse\n",
      "process of each other. In what follows, we begin with the\n",
      "definition of a 3D Gaussian, which is the minimal element\n",
      "of the scene representation in 3D GS. Next, we describe how\n",
      "these 3D Gaussians can be used for differentiable rendering.\n",
      "Finally, we introduce the acceleration technique used in 3D\n",
      "GS, which is the key to fast rendering.\n",
      "\n",
      "_•_ **Properties of 3D Gaussian.** A 3D Gaussian is characterized by its center (position) _**µ**_, opacity _α_, 3D covariance\n",
      "matrix **Σ**, and color _c_ . _c_ is represented by spherical harmonics for view-dependent appearance. All the properties\n",
      "are learnable and optimized through back-propagation.\n",
      "\n",
      "_•_ **Frustum Culling.** Given a specified camera pose, this step\n",
      "determines which 3D Gaussians are outside the camera’s\n",
      "frustum. By doing so, 3D Gaussians outside the given view\n",
      "will not be involved in the subsequent computation.\n",
      "\n",
      "_•_ **Splatting.** In this step, 3D Gaussians (ellipsoids) in 3D\n",
      "space are projected into 2D image space (ellipses). The projection proceeds through two transformations: first, transforming 3D Gaussians from world coordinates to camera\n",
      "coordinates using the viewing transformation, and subsequently splatting these Gaussians into 2D image space via\n",
      "an approximation of the projective transformation. Mathematically, given the 3D covariance matrix **Σ** describing a 3D\n",
      "Gaussian’s spatial distribution, and the viewing transformation matrix _**W**_, the 2D covariance matrix **Σ** _[′]_ characterizing\n",
      "the projected 2D Gaussian is computed through:\n",
      "\n",
      "\n",
      "**Σ** _[′]_ = _**JW**_ **Σ** _**W**_ _[⊤]_ _**J**_ _[⊤]_ _,_ (4)\n",
      "\n",
      "\n",
      "where _**J**_ is the Jacobian of the affine approximation of the\n",
      "projective transformation [10], [39]. One might wonder why\n",
      "the standard camera intrinsics based projective transformation is not used here. This is because its mappings are not\n",
      "affine and therefore cannot directly project **Σ** . 3D GS adopts\n",
      "an affine one proposed in [39] which approximates the projective transformation using the first two terms (including\n",
      "_**J**_ ) of the Taylor expansion (see Sec. 4.4 in [39]).\n",
      "\n",
      "\n",
      "\n",
      "_C_ =\n",
      "\n",
      "\n",
      "\n",
      "_|N |_\n",
      "� _c_ _n_ _α_ _n_ _[′]_\n",
      "\n",
      "\n",
      "_n_ =1\n",
      "\n",
      "\n",
      "\n",
      "� 1 _−_ _α_ _j′_ � _,_ (5)\n",
      "\n",
      "\n",
      "\n",
      "where _c_ _n_ is the learned color. The final opacity _α_ _n_ _[′]_ [is the]\n",
      "multiplication result of the learned opacity _α_ _n_ and the\n",
      "Gaussian, defined as follows:\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "_α_ _n_ _[′]_ [=] _[ α]_ _[n]_ _[×]_ [ exp] � _−_ _n_ [)] _[⊤]_ **[Σ]** _[′−]_ _n_ [1] ( _**x**_ _[′]_ _−_ _**µ**_ _[′]_ _n_ [)] � _,_ (6)\n",
      "\n",
      "2 [(] _**[x]**_ _[′]_ _[ −]_ _**[µ]**_ _[′]_\n",
      "\n",
      "\n",
      "\n",
      "where _**x**_ _[′]_ and _**µ**_ _[′]_ _n_ [are coordinates in the projected space. It]\n",
      "is a reasonable concern that the rendering process described\n",
      "could be slower compared to NeRFs, given that generating\n",
      "the required sorted list is hard to parallelize. Indeed, this\n",
      "concern is justified; rendering speeds can be significantly\n",
      "impacted when utilizing such a simplistic, pixel-by-pixel\n",
      "approach. To achieve real-time rendering, 3D GS makes\n",
      "several concessions to accommodate parallel computation.\n",
      "\n",
      "_•_ **Tiles (Patches).** To avoid the cost computation of deriving\n",
      "Gaussians for each pixel, 3D GS shifts the precision from\n",
      "pixel-level to patch-level detail, which is inspired by tilebased rasterization [43]. Concretely, 3D GS initially divides\n",
      "the image into multiple non-overlapping patches (tiles).\n",
      "Fig. 4b provides an illustration of tiles. Each tile comprises\n",
      "16 _×_ 16 pixels as suggested in [10]. 3D GS further determines\n",
      "which **tiles** intersect with these projected Gaussians. Given\n",
      "that a projected Gaussian may cover several tiles, a logical\n",
      "method involves replicating the Gaussian, assigning each\n",
      "copy an identifier ( _i.e_ ., a tile ID) for the relevant tile.\n",
      "\n",
      "_•_ **Parallel Rendering.** After replication, 3D GS combines\n",
      "the respective tile ID with the depth value obtained from\n",
      "the view transformation for each Gaussian. This results in\n",
      "an unsorted list of bytes where the upper bits represent\n",
      "the tile ID and the lower bits signify depth. By doing so,\n",
      "the sorted list can be directly utilized for rendering ( _i.e_ .,\n",
      "alpha compositing). Fig. 4c and Fig. 4d provide the visual\n",
      "demonstration of such concepts. It’s worth highlighting that\n",
      "rendering each tile and pixel occurs **independently**, making\n",
      "this process highly suitable for parallel computations. An\n",
      "additional benefit is that each tile’s pixels can access a\n",
      "common **shared memory** and maintain an **uniform read**\n",
      "**sequence** (Fig. 5), enabling parallel execution of alpha compositing with increased efficiency. In the official implementation of the original paper [10], the framework regards the\n",
      "processing of tiles and pixels as analogous to the blocks and\n",
      "threads, respectively, in CUDA programming architecture.\n",
      "In a nutshell, 3D GS introduces several approximations\n",
      "during rendering to enhance computational efficiency while\n",
      "maintaining a high standard of image synthesis quality.\n",
      "\n",
      "\n",
      "**3.2** **Optimization of 3D Gaussian Splatting**\n",
      "\n",
      "\n",
      "At the heart of 3D GS lies an optimization procedure devised to construct a copious collection of 3D Gaussians that\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5\n",
      "\n",
      "\n",
      "\n",
      "(a)\n",
      "\n",
      "\n",
      "\n",
      "Splatting\n",
      "\n",
      "\n",
      "\n",
      "Image Space 3D Gaussians (c) (d)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sorted 2D Gaussians\n",
      "\n",
      "\n",
      "Tile 1 Depth\n",
      "\n",
      "\n",
      "Tile 1 Depth\n",
      "\n",
      "\n",
      "\n",
      "Tile1\n",
      "\n",
      "\n",
      "\n",
      "_C_ 1 _C_ 2\n",
      "\n",
      "\n",
      "_C_ 3 _C_ 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Replication\n",
      "\n",
      "\n",
      "**Tile 1** **Depth**\n",
      "\n",
      "\n",
      "**Tile 2** **Depth**\n",
      "\n",
      "\n",
      "|Tile 2|Depth|\n",
      "|---|---|\n",
      "|Tile 3|Depth|\n",
      "|Tile 4|Depth|\n",
      "|Tile 3|Depth|\n",
      "\n",
      "\n",
      "|Tile 2|Depth|\n",
      "|---|---|\n",
      "|Tile 3|Depth|\n",
      "|Tile 3|Depth|\n",
      "|Tile 4|Depth|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|Tile 1|Depth|\n",
      "|---|---|\n",
      "|Tile 1|Depth|\n",
      "\n",
      "\n",
      "|Tile 1|Depth|\n",
      "|---|---|\n",
      "|**Tile 1**|**Depth**|\n",
      "\n",
      "\n",
      "\n",
      "through the list of Gaussians just once. The computation for the red\n",
      "\n",
      "|Col1|Parallel Rendering|\n",
      "|---|---|\n",
      "||Parallel Rendering<br>_C_1 =_α′_<br>1_c_1 +_ α′_<br>1_c_2(1_ −α′_<br>1)<br>_C_2 =_α′_<br>2_c_1 +_ α′_<br>2_c_2(1_ −α′_<br>2)<br>_C_3 =_α′_<br>3_c_1 +_ α′_<br>3_c_2(1_ −α′_<br>3)<br>_C_4 =_α′_<br>4_c_1 +_ α′_<br>4_c_2(1_ −α′_<br>4)|\n",
      "|**Depth**<br>**Depth**<br>Tile3<br>Tile4<br>Depth<br>Tile 3<br>Depth<br>Tile 4<br>Depth<br>Tile 4<br>Depth<br>Tile 3<br>_C_3 =_α′_<br>3_c_1 +_ α′_<br>3_c_2(<br>_C_4 =_α′_<br>4_c_1 +_ α′_<br>4_c_2(<br>Fig. 4. An illustration of the forward process of 3D GS (see Sec. 3.1). (a) The splatting step projects 3D Gaussians into image space.<br>divides the image into multiple non-overlapping patches,_ i.e_., tiles. (c) 3D GS replicates the Gaussians which cover several tiles, assig<br>copy an identifer,_ i.e_., a tile ID. (d) By rendering the sorted Gaussians, we can obtain all pixels within the tile. Note that the computational<br>for pixels and tiles are independent and can be done in parallel. Best viewed in color.<br>**Depth**<br>**Tile 1**<br>Sorted 2D Gaussians<br>Tile1<br>Depth<br>Tile 1<br>Depth<br>Tile 1<br>Shared<br>……<br>**Depth**<br>**Tile 1**<br>Uniform Read<br>Sequence<br>Overlap<br>Non-overlap<br>Parallel Access<br>Pass<br>Eq.<br>Pass<br>Pass<br>5<br>Fig. 5. An illustration of the tile based parallel (at the pixel-level) ren-<br>dering. All the pixels within a tile (Tile1 here) access the same ordered<br>Gaussian list stored in a shared memory for rendering. As the system<br>processes each Gaussian sequentially, every pixel in the tile evaluates<br>the Gaussian’s contribution according to the distance (_i.e_., the exp term<br>in Eq. 6). Therefore, the rendering for a tile can be completed by iterating<br><br>which would not adhere to the physical interpreta<br>ically associated with covariance matrices. To cir<br>this issue, 3D GS chooses to optimize a quaternio<br>a 3D vector**_ s_**. Here**_ q_** and**_ s_** represent rotation a<br>respectively. This approach allows the covariance<br>to be reconstructed as follows:<br>**Σ** =**_ RSS_**_⊤_**_R_**_⊤,_<br>where**_ R_** is the rotation matrix derived from the qu<br>**_q_**, and**_ S_** is the scaling matrix given by diag(**_s_**).<br>there is a complex computational graph to obtain th<br>_α_,_ i.e_.,**_ q_** and**_ s_**_ →_**Σ**,** Σ**_ →_**Σ**_′_, and** Σ**_′ →α_. To avoid<br>of automatic differentiation, 3D GS derives the grad<br>**_q_** and**_ s_** so as to compute them directly during opti|_C_3 =_α′_<br>3_c_1 +_ α′_<br>3_c_2(<br>_C_4 =_α′_<br>4_c_1 +_ α′_<br>4_c_2(|\n",
      "\n",
      "Gaussian follows a similar way and is omitted here for simplicity.\n",
      "\n",
      "\n",
      "accurately captures the scene’s essence, thereby facilitating\n",
      "free-viewpoint rendering. On the one hand, the properties\n",
      "of 3D Gaussians should be optimized via differentiable\n",
      "rasterization to fit the textures of a given scene. On the\n",
      "other hand, the number of 3D Gaussians that can represent a\n",
      "given scene well is unknown in advance. We will introduce\n",
      "how to optimize the properties of each Gaussian in Sec. 3.2.1\n",
      "and how to adaptively control the density of the Gaussians\n",
      "in Sec. 3.2.2. The two procedures are interleaved within the\n",
      "optimization workflow. Since there are many manually set\n",
      "hyperparameters in the optimization process, we omit the\n",
      "notations of most hyperparameters for clarity.\n",
      "\n",
      "\n",
      "_3.2.1_ _Parameter Optimization_\n",
      "\n",
      "\n",
      "_•_ **Loss Function.** Once the synthesis of the image is completed, the difference between the rendered image and\n",
      "ground truth can be measured. All the learnable parameters\n",
      "are optimized by stochastic gradient descent using the _ℓ_ 1\n",
      "and D-SSIM loss functions:\n",
      "\n",
      "\n",
      "_L_ = (1 _−_ _λ_ ) _L_ 1 + _λL_ D-SSIM _,_ (7)\n",
      "\n",
      "\n",
      "where _λ ∈_ [0 _,_ 1] is a weighting factor.\n",
      "\n",
      "_•_ **Parameter Update.** Most properties of a 3D Gaussian\n",
      "can be optimized directly through back-propagation. It is\n",
      "essential to note that directly optimizing the covariance\n",
      "matrix **Σ** can result in a non-positive semi-definite matrix,\n",
      "\n",
      "\n",
      "\n",
      "_3.2.2_ _Density Control_\n",
      "\n",
      "_•_ **Initialization.** 3D GS starts with the initial set of sparse\n",
      "points from SfM or random initialization. Note that a good\n",
      "initialization is essential to convergence and reconstruction\n",
      "quality [44]. Afterwards, point densification and pruning are\n",
      "adopted to control the density of 3D Gaussians.\n",
      "\n",
      "_•_ **Point Densification.** In the point densification phase, 3D\n",
      "GS adaptively increases the density of Gaussians to better\n",
      "capture the details of a scene. This process focuses on areas\n",
      "with missing geometric features or regions where Gaussians\n",
      "are too spread out. The densification procedure will be\n",
      "performed at regular intervals ( _i.e_ ., after a certain number of\n",
      "training iterations), focusing on those Gaussians with large\n",
      "view-space positional gradients ( _i.e_ ., above a specific threshold). It involves either cloning small Gaussians in underreconstructed areas or splitting large Gaussians in overreconstructed regions. For cloning, a copy of the Gaussian\n",
      "is created and moved towards the positional gradient. For\n",
      "splitting, a large Gaussian is replaced with two smaller ones,\n",
      "reducing their scale by a specific factor. This step seeks an\n",
      "optimal distribution and representation of Gaussians in 3D\n",
      "space, enhancing the overall quality of the reconstruction.\n",
      "\n",
      "_•_ **Point Pruning.** The point pruning stage involves the\n",
      "removal of superfluous or less impactful Gaussians, which\n",
      "can be viewed as a regularization process. It is executed by\n",
      "eliminating Gaussians that are virtually transparent (with _α_\n",
      "below a specified threshold) and those that are excessively\n",
      "large in either world-space or view-space. In addition, to\n",
      "prevent unjustified increases in Gaussian density near input\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6\n",
      "\n",
      "\n",
      "\n",
      "cameras, the alpha value of the Gaussians is set close to\n",
      "zero after a certain number of iterations. This allows for a\n",
      "controlled increase in the density of necessary Gaussians\n",
      "while enabling the culling of redundant ones. The process\n",
      "not only helps in conserving computational resources but\n",
      "also ensures that the Gaussians in the model remain precise\n",
      "and effective for the representation of the scene.\n",
      "\n",
      "\n",
      "**4** **3D G** **AUSSIAN** **S** **PLATTING** **: D** **IRECTIONS**\n",
      "\n",
      "\n",
      "Though 3D GS has achieved impressive milestones, significant room for improvement remains, _e.g_ ., data and hardware\n",
      "requirement, rendering and optimization algorithm, and applications in downstream tasks. In the subsequent sections,\n",
      "we seek to elaborate on select extended versions. These are:\n",
      "**i** ) 3D GS for Sparse Input [45]–[55] (Sec. 4.1), **ii** ) Memoryefficient 3D GS [56]–[64] (Sec. 4.2), **iii** ) Photorealistic 3D\n",
      "GS [65]–[80] (Sec. 4.3), **iv** ) Improved Optimization Algorithms [22], [77], [81]–[86] (Sec. 4.4), **v** ) 3D Gaussian with\n",
      "More Properties [87]–[93] (Sec. 4.5), **vi** ) Hybrid Representation [94]–[96] (Sec. 4.6), and **vii** ) New Rendering Algorithm\n",
      "(Sec. 4.7). While we have carefully selected several key\n",
      "directions, we acknowledge that it is inevitably a biased\n",
      "[view. A more comprehensive collection is given in Github.](https://github.com/guikunchen/Awesome3DGS)\n",
      "\n",
      "\n",
      "**4.1** **3D GS for Sparse Input**\n",
      "\n",
      "\n",
      "A notable issue of 3D GS is the emergence of artifacts in\n",
      "areas with insufficient observational data. This challenge is\n",
      "a prevalent limitation in radiance field rendering, where\n",
      "sparse data often leads to inaccuracies in reconstruction.\n",
      "From a practical perspective, reconstructing scenes from\n",
      "limited viewpoints is of significant interest, particularly for\n",
      "the potential to enhance functionality with minimal input.\n",
      "Existing methods can be categorized into two primary\n",
      "groups. **i) Regularization** based methods introduce additional constraints such as depth information to enhance\n",
      "the detail and global consistency [46], [49], [51], [55]. For\n",
      "example, DNGaussian [49] introduced a depth-regularized\n",
      "approach to address the challenge of geometry degradation\n",
      "in sparse input. FSGS [46] devised a Gaussian Unpooling\n",
      "process for initialization and also introduced depth regularization. MVSplat [51] proposed a cost volume representation so as to provide geometry cues. Unfortunately, when\n",
      "dealing with a limited number of views, or even just one,\n",
      "the efficacy of regularization techniques tends to diminish,\n",
      "which leads to **ii) generalizability** based methods that use\n",
      "learned priors [47], [48], [53], [97]. One approach involves\n",
      "synthesizing additional views through generative models,\n",
      "which can be seamlessly integrated into existing reconstruction pipelines [98]. However, this augmentation strategy is\n",
      "computationally intensive and inherently bounded by the\n",
      "capabilities of the used generative model. Another wellknown paradigm employs feed-forward Gaussian model to\n",
      "directly generates the properties of a set of 3D Gaussians.\n",
      "This paradigm typically requires multiple views for training\n",
      "but can reconstruct 3D scenes with only one input image.\n",
      "For instance, PixelSplat [47] proposed to sample Gaussians\n",
      "from dense probability distributions. Splatter Image [48] introduced a 2D image-to-image network that maps an input\n",
      "image to a 3D Gaussian per pixel. However, as the generated\n",
      "\n",
      "\n",
      "\n",
      "pixel-aligned Gaussians are distributed nearly evenly in the\n",
      "space, they struggle to represent high-frequency details and\n",
      "smoother regions with an appropriate number of Gaussians.\n",
      "\n",
      "The challenge of 3D GS for sparse inputs centers on\n",
      "the modeling of priors, whether through depth information,\n",
      "generative models, or feed-forward Gaussian models. The\n",
      "fundamental trade-off lies between overfitting to available\n",
      "views and using learned priors for generalization. Future\n",
      "research could explore adaptive mechanisms for controlling\n",
      "this trade-off, potentially through learned confidence measures, context-aware prior selection, user preferences, _etc_ . In\n",
      "addition, while current methods focus on static scenes, extending these approaches to dynamic scenarios presents an\n",
      "exciting frontier for investigation, particularly in handling\n",
      "temporal consistency and motion-induced artifacts.\n",
      "\n",
      "\n",
      "**4.2** **Memory-efficient 3D GS**\n",
      "\n",
      "\n",
      "While 3D GS demonstrates remarkable capabilities, its scalability poses significant challenges, particularly when juxtaposed with NeRF-based methods. The latter benefits from\n",
      "the simplicity of storing merely the parameters of a learned\n",
      "MLP. This scalability issue becomes increasingly acute in\n",
      "the context of large-scale scene management, where the\n",
      "computational and memory demands escalate substantially.\n",
      "Consequently, there is an urgent need to optimize memory\n",
      "usage in both model training and storage.\n",
      "Recent research has pursued two primary directions to\n",
      "address memory efficiency. First, several approaches focus\n",
      "on **reducing the number of 3D Gaussians** [58], [62], [63].\n",
      "These methods either employ strategic pruning of lowimpact Gaussians, such as the volume-based masking [58],\n",
      "or represent neighboring Gaussians using the same properties stored within a “local anchor” obtained by clustering [22], hash-grid [62], _etc_ . Second, researchers have developed methods for **compressing Gaussian’s properties** [58],\n",
      "\n",
      "[61], [62]. For instance, Niedermayr _et al_ . [61] compressed\n",
      "color and Gaussian parameters into compact codebooks,\n",
      "using sensitivity measures for effective quantization and\n",
      "fine-tuning. HAC [62] predicted the probability of each\n",
      "quantized attribute using Gaussian distributions and then\n",
      "devise an adaptive quantization module. These directions\n",
      "are not mutually exclusive; instead, one framework might\n",
      "use a hybrid approach combining multiple strategies.\n",
      "\n",
      "While current compression techniques have achieved\n",
      "significant storage reduction ratios (often by factors of 1020 _×_ ), several challenges remain. The field particularly needs\n",
      "advances in memory efficiency during the training phase,\n",
      "potentially through quantization-aware training protocols,\n",
      "the development of scene-agnostic, reusable codebooks, _etc_ .\n",
      "Furthermore, optimizing the trade-off between compression\n",
      "efficiency and visual fidelity remains an open problem.\n",
      "\n",
      "\n",
      "**4.3** **Photorealistic 3D GS**\n",
      "\n",
      "\n",
      "The current rendering pipeline of 3D GS (Sec. 3.1) is straightforward and involves several drawbacks. For instance, the\n",
      "simple visibility algorithm may lead to a drastic switch\n",
      "in the depth/blending order of Gaussians [10]. The visual\n",
      "fidelity of rendered images, including aspects such as aliasing, reflections, and artifacts, can be further optimized.\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7\n",
      "\n",
      "\n",
      "\n",
      "Recent research has focused on addressing three main\n",
      "aspects of visual quality, with aliasing being specific to 3D\n",
      "GS’s rendering algorithm, while reflection and blur handling represent broader challenges in 3D reconstruction. **i)**\n",
      "**Aliasing** . Due to the discrete sampling paradigm (viewing\n",
      "each pixel as a single point instead of an area), 3D GS is\n",
      "susceptible to aliasing when dealing with varying resolutions, which leads to blurring or jagged edges. Solutions\n",
      "emerged at both training and inference stages. Researchers\n",
      "developed training-time improvements from the sampling\n",
      "rate perspective and introduced schemes such as multi-scale\n",
      "Gaussians [67], 2D Mip filter [65], and conditioned logistic\n",
      "function [78]. Inference-time solutions, such as 2D scaleadaptive filtering [80], offer enhanced fidelity that can be\n",
      "integrated into any existing 3D GS frameworks. **ii) Reflec-**\n",
      "**tion** . Achieving realistic rendering of reflective materials is\n",
      "a hard, long-standing problem in 3D scene reconstruction.\n",
      "Recent works have introduced various approaches to model\n",
      "reflective materials [68], [73], [99] and enable relightable\n",
      "Gaussian representation [23], though achieving physically\n",
      "accurate specular effects remains challenging. **iii) Blur** .\n",
      "While 3D GS excels on carefully curated datasets, realworld captures often suffer from blurs such as motion blur\n",
      "and defocus blur. Recent approaches explicitly incorporated\n",
      "blur modeling during training, employing techniques such\n",
      "as coarse-to-fine kernel optimization [74] and photometric\n",
      "bundle adjustment [75] to address this challenge.\n",
      "\n",
      "While the approximations made in 3D GS (Sec. 3.1)\n",
      "contribute to its computational efficiency, they also lead to\n",
      "aliasing, difficulties in illumination estimation, _etc_ . Current\n",
      "solutions, though impressive, typically address individual\n",
      "problems rather than providing a universal solution. A practical intermediate approach involves first detecting specific\n",
      "issues ( _e.g_ ., aliasing, blur) and then applying targeted optimization strategies. The ultimate goal remains developing\n",
      "an advanced reconstruction system that overcomes these\n",
      "limitations, either through fundamental improvements to\n",
      "3D GS or through brand-new architectures.\n",
      "\n",
      "\n",
      "**4.4** **Improved Optimization Algorithms**\n",
      "\n",
      "\n",
      "The optimization of 3D GS presents several challenges that\n",
      "affect the quality of reconstruction. These include issues\n",
      "with convergence speed, visual artifacts from improper\n",
      "Gaussians, and the need for better regularization during\n",
      "optimization. The raw optimization method (Sec. 3.2) might\n",
      "lead to overreconstruction in some regions while underrepresenting others, resulting in blur and visual inconsistencies.\n",
      "\n",
      "Three main directions stand out for improving the optimization of 3D GS. **i) Additional Regularization** ( _e.g_ .,\n",
      "frequency [84] and geometry [22], [77]). Geometry-aware\n",
      "approaches have been particularly successful, preserving\n",
      "scene structure through the incorporation of local anchor\n",
      "points [22], depth and surface constraints [100]–[102], Gaussian volumes [103], _etc_ . **ii) Optimization Procedure En-**\n",
      "**hancement** [44], [101], [104]. While the original strategy of\n",
      "density control (Sec. 3.2.2) has proven valuable, considerable room for improvement remains. For example, GaussianPro [44] addresses the challenge of dense initialization\n",
      "in texture-less surfaces and large-scale scenes through an\n",
      "advanced Gaussian densification strategy. **iii) Constraint**\n",
      "\n",
      "\n",
      "\n",
      "**Relaxation** . Reliance on external tools/algorithms can introduce errors and cap the system’s performance potential. For\n",
      "instance, SfM, commonly used in the initialization process,\n",
      "is error-prone and struggle with complex scenes. Recent\n",
      "works have begun exploring COLMAP-free approaches\n",
      "utilizing stream continuity [81], [105], potentially enabling\n",
      "learning from internet-scale unposed video datasets.\n",
      "\n",
      "Though impressive, existing methods primarily concentrate on optimizing Gaussians to accurately reconstruct\n",
      "scenes from scratch, neglecting a challenging yet promising\n",
      "solution which reconstructs scenes in a few-shot manner\n",
      "through established “meta representations”. Such solution\n",
      "could enable adaptive meta-learning strategies that combine\n",
      "scene-specific and general knowledge. See “learning physical priors from large-scale data” in Sec. 7 for further insights.\n",
      "\n",
      "\n",
      "**4.5** **3D Gaussian with More Properties**\n",
      "\n",
      "\n",
      "Despite impressive, the properties of 3D Gaussian (Sec. 3.1)\n",
      "are designed to be used for novel-view synthesis only. By\n",
      "augmenting 3D Gaussian with additional properties, such as\n",
      "linguistic [87]–[89], semantic/instance [90]–[92], and spatialtemporal [93] properties, 3D GS demonstrates its considerable potential to revolutionize various domains.\n",
      "Here we list several interesting applications using 3D\n",
      "Gaussians with specially designed properties. **i** ) **Language**\n",
      "**Embedded Scene Representation** [87]–[89]. Due to the high\n",
      "computational and memory demands of current languageembedded scene representations, Shi _et al_ . [87] proposed\n",
      "a quantization scheme that augments 3D Gaussian with\n",
      "streamlined language embeddings instead of the original high-dimensional embeddings. This method also mitigated semantic ambiguity and enhanced the precision of\n",
      "open-vocabulary querying by smoothing out semantic features across different views, guided by uncertainty values.\n",
      "**ii** ) **Scene Understanding and Editing** [90]–[92]. Feature\n",
      "3DGS [90] integrated 3D GS with feature field distillation from 2D foundation models. By learning a lowerdimensional feature field and applying a lightweight convolutional decoder for upsampling, Feature 3DGS achieved\n",
      "faster training and rendering speeds while enabling highquality feature field distillation, supporting applications\n",
      "like semantic segmentation and language-guided editing.\n",
      "**iii** ) **Spatiotemporal Modeling** [93], [106]. To capture the\n",
      "complex spatial and temporal dynamics of 3D scenes, Yang\n",
      "_et al_ . [93] conceptualized spacetime as a unified entity and\n",
      "approximates the spatiotemporal volume of dynamic scenes\n",
      "using a collection of 4D Gaussians. The proposed 4D Gaussian representation and corresponding rendering pipeline\n",
      "are capable of modeling arbitrary rotations in space and\n",
      "time and allow for end-to-end training.\n",
      "\n",
      "\n",
      "**4.6** **Hybrid Representation**\n",
      "\n",
      "\n",
      "Rather than augmenting 3D Gaussian with additional properties, another promising avenue of adapting to downstream tasks is to introduce structured information ( _e.g_ .,\n",
      "spatial MLPs and grids) tailored for specific applications.\n",
      "Next we showcase various fascinating uses of 3D GS\n",
      "with specially devised structured information. **i** ) **Facial Ex-**\n",
      "**pression Modeling** . Considering the challenge of creating\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8\n",
      "\n",
      "\n",
      "\n",
      "high-fidelity 3D head avatars under sparse view conditions, Gaussian Head Avatar [96] introduced controllable 3D\n",
      "Gaussians and an MLP-based deformation field. Concretely,\n",
      "it captured detailed facial expressions and dynamics by\n",
      "optimizing neutral 3D Gaussians alongside the deformation field, thus ensuring both detail fidelity and expression\n",
      "accuracy. **ii** ) **Spatiotemporal Modeling** . Yang _et al_ . [94]\n",
      "proposed to reconstruct dynamic scenes with deformable\n",
      "3D Gaussians. The deformable 3D Gaussians are learned in a\n",
      "canonical space, coupled with a deformation field ( _i.e_ ., a spatial MLP) that models the spatial-temporal dynamics. The\n",
      "proposed method also incorporated an annealing smoothing training mechanism to enhance temporal smoothness\n",
      "without additional computational costs. **iii** ) **Style Transfer** .\n",
      "Saroha _et al_ . [107] proposed GS in style, an advanced approach for real-time neural scene stylization. To maintain a\n",
      "cohesive stylized appearance across multiple views without\n",
      "compromising on rendering speed, they used pre-trained 3D\n",
      "Gaussians coupled with a multi-resolution hash grid and a\n",
      "small MLP to produce stylized views. In a nutshell, incorporating structured information can serve as a complementary\n",
      "part for adapting to tasks that are incompatible with the\n",
      "sparsity and disorder of 3D Gaussians.\n",
      "\n",
      "\n",
      "**4.7** **New Rendering Algorithm for 3D Gaussians**\n",
      "\n",
      "\n",
      "While the rasterization-based pipeline of 3D GS offers impressive real-time performance, it still suffers from the inherent limitations, including inefficient handling of highlydistorted cameras (crucial for robotics), secondary rays (for\n",
      "optical effects like reflections and shadows), and stochastic\n",
      "ray sampling (needed in various existing pipelines). In\n",
      "addition, the assumptions that Gaussians do not overlap\n",
      "and can be sorted accurately using only centers are often\n",
      "violated in practice, leading to temporal artifacts when\n",
      "camera movement changes sorting order.\n",
      "\n",
      "Recent works [108]–[110] explored ray tracing based\n",
      "rendering algorithms as an alternative. For instance, GaussianTracer [108] introduced a new ray tracing implementation for Gaussian primitives, and devised several accelerating strategies according to the uneven density and interleaved nature of Gaussians. EVER [109] deivsed a physically\n",
      "accurate, constant density ellipsoid representation that allows for the exact computation of the volume rendering integral, rather than relying on somewhat satisfactory approximations. This advancement eliminates popping artifacts.\n",
      "\n",
      "Thanks to the fundamental paradigm shift, several exciting possibilities might emerge, including advanced optical\n",
      "effects (reflection, refraction, shadows, global illumination,\n",
      "_etc_ .), support for complex camera models (highly-distorted\n",
      "lenses, rolling shutter effects, _etc_ .), physically accurate rendering with true directional appearance evaluation ( _vs_ . tile\n",
      "based approximation), and more. While these capabilities\n",
      "currently come with additional computational costs, they\n",
      "provide essential building blocks for future research in\n",
      "inverse rendering, physical material modeling, relighting,\n",
      "and complex scene reconstruction.\n",
      "\n",
      "\n",
      "**5** **A** **PPLICATION** **A** **REAS AND** **T** **ASKS**\n",
      "\n",
      "\n",
      "Building on the rapid advancements in 3D GS, a wide range\n",
      "of innovative applications has emerged across multiple do\n",
      "\n",
      "\n",
      "mains (Fig. 6) such as robotics (Sec. 5.1), dynamic scene\n",
      "reconstruction and representation (Sec. 5.2), generation and\n",
      "editing (Sec. 5.3), avatar (Sec. 5.4), medical systems (Sec. 5.5),\n",
      "large-scale scene reconstruction (Sec. 5.6), physics (Sec. 5.7),\n",
      "and even other scientific disciplines [24], [174]–[176]. Here,\n",
      "we highlight key examples that underscore the transformative impact and potential of 3D GS and offer a more\n",
      "[comprehensive collection in Github.](https://github.com/guikunchen/Awesome3DGS)\n",
      "\n",
      "\n",
      "**5.1** **Robotics**\n",
      "\n",
      "\n",
      "The evolution of scene representation in robotics has been\n",
      "profoundly shaped by the emergence of NeRF, which revolutionized dense mapping and environmental interaction\n",
      "through implicit neural models. However, NeRF’s computational cost poses a critical bottleneck for real-time robotic\n",
      "applications. The shift from implicit to explicit representation not only accelerates optimization but also unlocks\n",
      "direct access to spatial and structural scene data, making 3D\n",
      "GS a transformative tool for robotics. Its ability to balance\n",
      "high-fidelity reconstruction with computational efficiency\n",
      "positions 3D GS as a cornerstone for advancing robotic\n",
      "perception, manipulation, and navigation in dynamic, realworld environments.\n",
      "\n",
      "The integration of GS into robotic systems has yielded\n",
      "significant advancements across three core domains. In\n",
      "**SLAM**, GS-based methods [111]–[117], [123], [124], [177]–\n",
      "\n",
      "[182] excel in real-time dense mapping but face inherent\n",
      "trade-offs. Visual SLAM frameworks, particularly RGBD variants [112], [114], [178], leverage depth supervision\n",
      "for geometric fidelity but falter in low-texture or motiondegraded environments. RGB-only approaches [113], [115],\n",
      "\n",
      "[183] circumvent depth sensors but grapple with scale ambiguity and drift. Multi-sensor fusion strategies, such as\n",
      "LiDAR integration [159], [177], [182], enhance robustness in\n",
      "unstructured settings at the cost of calibration complexity.\n",
      "Semantic SLAM [116], [117], [123] extends scene understanding through object-level semantics but struggles with\n",
      "scalability due to lighting sensitivity in color-based methods\n",
      "or computational overhead in feature-based methods. 3D\n",
      "GS based **manipulation** [118]–[122] bypasses the need for\n",
      "auxiliary pose estimation in NeRF-based methods, enabling\n",
      "rapid single-stage tasks like grasping in static environments\n",
      "via geometric and semantic attributes encoded in Gaussian properties. Multi-stage manipulation [118], [120], where\n",
      "environmental dynamics demand real-time map updates,\n",
      "requires explicit modeling of dynamic adjustments ( _e.g_ .,\n",
      "object motions and interactions), material compliance, _etc_ .\n",
      "\n",
      "The advancement of 3D GS in robotics faces three pivotal\n",
      "challenges. First, adaptability in dynamic and unstructured\n",
      "environments remains critical: real-world scenes are rarely\n",
      "static, requiring systems to continuously update representations amid motion, occlusions, and sensor noise without sacrificing accuracy. Second, current semantic mapping methods rely on costly, scene-specific optimization processes,\n",
      "limiting generalizability and scalability for real-world deployment. Third, unlike NeRF based systems which can\n",
      "use MLP parameters as input features for downstream\n",
      "decision-making, 3D Gaussians’ inherent lack of spatial\n",
      "order complicates feature aggregation, with no standardized\n",
      "framework yet established. Bridging the gap between high\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fig. 6. Typical applications benefited from GS (Sec. 5). Some images are borrowed from [132], [135], [146], [154], [160], [166] and redrawn.\n",
      "\n",
      "\n",
      "\n",
      "fidelity reconstruction and actionable semantic/physical understanding will define the next frontier for 3D GS, moving\n",
      "beyond passive mapping towards embodied intelligence.\n",
      "\n",
      "\n",
      "**5.2** **Dynamic Scene Reconstruction**\n",
      "\n",
      "\n",
      "Dynamic scene reconstruction refers to the process of capturing and representing the three-dimensional structure and\n",
      "appearance of a scene that changes over time [184]–[187].\n",
      "This involves creating a digital model that accurately reflects\n",
      "the geometry, motion, and visual aspects of the objects in\n",
      "the scene as they evolve. Dynamic scene reconstruction is\n",
      "crucial in various applications, _e.g_ ., VR/AR, 3D animation,\n",
      "and autonomous driving [188]–[190].\n",
      "The key to adapt 3D GS to dynamic scenes is the\n",
      "modeling of temporal dimension which allows for the\n",
      "representation of scenes that change over time. 3D GS\n",
      "based methods [93]–[95], [106], [125]–[130], [191]–[199] for\n",
      "dynamic scene reconstruction can generally be divided into\n",
      "two main categories as discussed in Sec. 4.5 and Sec. 4.6. The\n",
      "first category utilizes **additional fields** like spatial MLPs or\n",
      "grids to **model deformation** (Sec. 4.6). For example, Yang _et_\n",
      "_al_ . [94] first proposed deformable 3D Gaussians tailored for\n",
      "dynamic scenes. These 3D Gaussians are learned in a canonical space and can be used to model spatial-temporal deformation with an implicit deformation field (implemented\n",
      "as an MLP). GaGS [132] devised the voxelization of a set\n",
      "of Gaussian distributions, followed by the use of sparse\n",
      "convolutions to extract geometry-aware features, which are\n",
      "then utilized for deformation learning. On the other hand,\n",
      "the second category is based on the idea that scene changes\n",
      "\n",
      "\n",
      "\n",
      "can be **encoded into the 3D Gaussian representation** with a\n",
      "specially designed rendering process (Sec. 4.5). For instance,\n",
      "Luiten _et al_ . [125] introduced dynamic 3D Gaussians to\n",
      "model dynamic scenes by keeping the properties of 3D\n",
      "Gaussians unchanged over time while allowing their positions and orientations to change. Yang _et al_ . [93] designed a\n",
      "4D Gaussian representation, where additional properties are\n",
      "used to represent 4D rotations and spherindrical harmonics,\n",
      "to approximate the spatial-temporal volume of scenes.\n",
      "\n",
      "While 3D GS advances dynamic scene reconstruction by\n",
      "modeling per-Gaussian deformations, its reliance on finegrained primitives limits scalability and robustness. Current\n",
      "methods struggle to balance computational efficiency and\n",
      "precision: small-scale reconstructions unify dynamic and\n",
      "static elements but become intractable in large environments, often requiring manual priors to segment regions —–\n",
      "a barrier in unstructured settings. Furthermore, the absence\n",
      "of object-level motion reasoning leads to artifacts and poor\n",
      "generalization over long sequences. Future work might prioritize object-centric frameworks that hierarchically group\n",
      "Gaussians into persistent entities, enabling efficient largescale reconstruction through inherent motion disentanglement (dynamic _vs_ . static).\n",
      "\n",
      "\n",
      "**5.3** **Generation and Editing**\n",
      "\n",
      "\n",
      "Content generation and editing represent two fundamental\n",
      "and inherently interconnected capabilities in modern AI\n",
      "systems. While generation enables the synthesis of novel\n",
      "digital content from scratch or conditional inputs [200]–\n",
      "\n",
      "[202], editing provides the crucial ability to refine, adapt,\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10\n",
      "\n",
      "\n",
      "\n",
      "and manipulate existing content with precise control [203].\n",
      "Together, these capabilities revolutionize creative workflows\n",
      "by combining initial content creation with iterative refinement, enabling applications from professional content production to interactive consumer tools.\n",
      "\n",
      "Recent advances in generation [133]–[138], [204]–[227]\n",
      "have led to the emergence of three main approaches. **Op-**\n",
      "**timization** based methods [133], [134], [204] distill diffusion\n",
      "priors (gradients) to guide 3D model updates with the score\n",
      "functions. While these methods demonstrate impressive fidelity, they face significant computational overhead due to\n",
      "the necessity of comparing multiple viewpoints during the\n",
      "optimization process. **Reconstruction** based methods [135],\n",
      "\n",
      "[225], [227] reframe the generation problem as a multiview reconstruction task utilizing pre-trained multi-view\n",
      "diffusion models. Although this approach offers an intuitive\n",
      "and straightforward solution, it grapples with fundamental\n",
      "limitations in maintaining view consistency. The lack of\n",
      "strict geometric constraints across different viewpoints often\n",
      "results in inconsistent surface geometry and degraded texture quality, particularly in regions with complex visual features. **Direct 3D generation** methods train diffusion models\n",
      "on 3D representations [138], [220], [226]. While the learned\n",
      "3D diffusion models facilitate multi-view consistency, the\n",
      "demanding computational costs impede the expansion of\n",
      "training scales necessary for improved generative diversity.\n",
      "\n",
      "Current editing works [90]–[92], [126]–[128], [140]–[143],\n",
      "\n",
      "[228]–[239] fall into two primary classes. The first class leverages **2D image-editing** models ( _e.g_ ., diffusion-based editors)\n",
      "to iteratively refine 3D Gaussians. Early efforts [141], [142],\n",
      "\n",
      "[233] adopt optimization- or reconstruction-based strategies\n",
      "akin to methods in generation, but introduce task-specific\n",
      "control signals. However, naively applying 2D edits independently across views often introduces multi-view inconsistencies. Subsequent works [140], [238]–[240] mitigate this\n",
      "through iterative refinement or cross-view attention, albeit\n",
      "at increased computational costs for alignment. A notable\n",
      "challenge is unintended object deformations, attributed to\n",
      "the weak 3D geometric priors in 2D editing models and\n",
      "the difficulty of reconciling 2D edits with underlying 3D\n",
      "structures. The second class exploits the explicit nature of\n",
      "3D GS to enable **direct manipulation** based on embedded\n",
      "properties such as semantics [91], [92], [143], [232] and key\n",
      "points [128]. However, this class remains underexplored\n",
      "due to essentail challenges: the lack of inherent ordering\n",
      "of Gaussians complicates the design of efficient indexing\n",
      "schemes, while editing attributes ( _e.g_ ., texture and geometry) requires careful regularization and alignment to preserve plausibility.\n",
      "\n",
      "\n",
      "**5.4** **Avatar**\n",
      "\n",
      "\n",
      "Avatars, the digital representations of users in virtual\n",
      "spaces, bridge physical and digital realms, enabling immersive interaction, identity expression, and remote collaboration. Spanning entertainment (gaming, virtual influencers),\n",
      "enterprise (AI agents, virtual meetings), healthcare, and education, they underpin metaverse economies. Advances in\n",
      "AR and VR amplify their role in redefining social, industrial,\n",
      "and creative landscapes.\n",
      "\n",
      "3D GS has emerged as a powerful tool for human avatar\n",
      "reconstruction, primarily advancing along two directions:\n",
      "\n",
      "\n",
      "\n",
      "full-body modeling and head-centric modeling. For **full-**\n",
      "**body avatars** [139], [144]–[147], [241]–[252], the current\n",
      "methods typically anchor 3D Gaussians in a canonical space\n",
      "and deform them via parametric body models ( _e.g_ ., SMPL)\n",
      "or cage-based rigging to model dynamic motions. These\n",
      "approaches adopt a hybrid deformation strategy: linear\n",
      "blend skinning handles rigid skeletal transformations such\n",
      "as joint rotations, while pose-conditioned deformation fields\n",
      "account for secondary non-rigid effects like muscle jiggles.\n",
      "For **head avatars** [23], [148]–[151], [253]–[256], the emphasis\n",
      "shifts to modeling intricate facial expressions, fine-grained\n",
      "geometry (e.g., wrinkles, hair [257]), and dynamic speechdriven animations. Techniques mainly combine parametric\n",
      "morphable face models ( _e.g_ ., FLAME) with deformable 3D\n",
      "Gaussians, employing diffusion strategies and expressionaware deformation fields to disentangle rigid head poses\n",
      "from non-rigid facial movements. Both directions exploit\n",
      "the speed advantage and editability of 3D GS to enable\n",
      "efficient training, real-time rendering, and precise control\n",
      "over deformations, while addressing challenges in crossframe correspondence, topology flexibility, and multi-view\n",
      "consistency.\n",
      "\n",
      "Reconstruction in challenging scenes ( _e.g_ ., occlusions,\n",
      "sparse single-view inputs, or loose clothing) and enhancing\n",
      "avatar interactivity represent critical challenges and opportunities. Parametric model-free methods, which bypass predefined priors by learning skinning weights directly from\n",
      "data, show promise for such scenarios. Complementary to\n",
      "this, generative models can mitigate ambiguities inherent\n",
      "in underconstrained settings. Further integrating physicsbased constraints might bridge the gap between static reconstructions and responsive, lifelike interactions, unlocking\n",
      "applications in AR, embodied AI, _etc_ .\n",
      "\n",
      "\n",
      "**5.5** **Endoscopic Scene Reconstruction**\n",
      "\n",
      "\n",
      "Surgical 3D reconstruction represents a fundamental task\n",
      "in robot-assisted minimally invasive surgery, aimed at enhancing intraoperative navigation, preoperative planning,\n",
      "and educational simulations through precise modeling of\n",
      "dynamic surgical scenes. Pioneering the integration of dynamic radiance fields into this domain, recent advancements\n",
      "have focused on surmounting the inherent challenges of\n",
      "single-viewpoint video reconstructions such as occlusions\n",
      "by surgical instruments and sparse viewpoint diversity\n",
      "within the confined spaces of endoscopic exploration [258]–\n",
      "\n",
      "[260]. Despite the progress, the call for high fidelity in tissue\n",
      "deformability and topological variation remains, coupled\n",
      "with the pressing demand for faster rendering to bridge\n",
      "the utility in applications sensitive to latency [152]–[154].\n",
      "This synthesis of immediacy and precision in reconstructing\n",
      "deformable tissues from endoscopic videos is essential in\n",
      "propelling robotic surgery towards reduced patient trauma\n",
      "and AR/VR applications, ultimately fostering a more intuitive surgical environment and nurturing the future of\n",
      "surgical automation and robotic proficiency.\n",
      "Endoscopic scene reconstruction introduces distinct\n",
      "challenges compared to general dynamic scenes, including\n",
      "sparse training data from limited camera mobility in narrow\n",
      "cavities, frequent tool occlusions obscuring critical regions,\n",
      "and single-view geometry ambiguities. Existing approaches\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11\n",
      "\n",
      "\n",
      "\n",
      "mainly used additional depth guidance to infer the geometry of tissues [152]–[154]. For instance, EndoGS [154]\n",
      "integrated depth-guided supervision with spatial-temporal\n",
      "weight masks and surface-aligned regularization terms to\n",
      "enhance the quality and speed of 3D tissue rendering\n",
      "while addressing tool occlusion. EndoGaussian [153] introduced two new strategies: holistic Gaussian initialization for\n",
      "dense initialization and spatiotemporal Gaussian tracking\n",
      "for modeling surface dynamics. Zhao _et al_ . [155] argued that\n",
      "these methods suffer from under-reconstruction and proposed to alleviate this problem from frequency perspectives.\n",
      "In addition, EndoGSLAM [156] and Gaussian Pancake [157]\n",
      "devised SLAM systems for endoscopic scenes and showed\n",
      "significant speed advantages.\n",
      "\n",
      "Advancing endoscopic 3D reconstruction requires targeted efforts in both data and dynamics modeling. Data limitations arise from single-viewpoint videos, which produce\n",
      "ill-posed reconstruction problems due to instrument occlusions and constrained camera mobility, leaving critical tissue\n",
      "regions unobserved. While depth estimators provide temporary workarounds, integrating multi-view camera systems\n",
      "addresses the root cause. In addition, existing datasets often\n",
      "feature truncated sequences ( _e.g_ ., 4 _∼_ 8 _s_ in EndoNeRF [258]),\n",
      "which fail to capture prolonged tissue deformation dynamics or complex surgical workflows. Extending temporal coverage to include longer, clinically representative sequences\n",
      "would benefit downstream applications as aforementioned.\n",
      "Modeling limitations persist in current methods, which often represent tissue dynamics at the Gaussian level rather\n",
      "than object- or 3D region-level. This reduces their capacity\n",
      "to encode semantically meaningful anatomical interactions\n",
      "and deserves further explorations.\n",
      "\n",
      "\n",
      "**5.6** **Large-scale Scene Reconstruction**\n",
      "\n",
      "\n",
      "Large-scale scene reconstruction is a critical component in\n",
      "fields such as autonomous driving, aerial surveying, and\n",
      "AR/VR, demanding both photorealistic visual quality and\n",
      "real-time rendering capabilities. Before the emergence of\n",
      "3D GS, the task has been approached using NeRF based\n",
      "methods, which, while effective for smaller scenes, often fall\n",
      "short in detail and rendering speed when scaled to larger\n",
      "areas ( _e.g_ ., over 1.5 _km_ [2] ). Though 3D GS has demonstrated\n",
      "considerable advantages over NeRFs, the direct application\n",
      "of 3D GS to large-scale environments introduces significant challenges. 3D GS requires an immense number of\n",
      "Gaussians to maintain visual quality over extensive areas,\n",
      "leading to prohibitive GPU memory demands and considerable computational burdens during rendering. For instance,\n",
      "a scene spanning 2.7 _km_ [2] may require over 20 million\n",
      "Gaussians, pushing the limits of even the most advanced\n",
      "hardware ( _e.g_ ., NVIDIA A100 with 40GB memory) [163].\n",
      "To address the highlighted challenges, researchers have\n",
      "made significant strides in two key areas: **i** ) For **training**, a\n",
      "divide-and-conquer strategy [162]–[165] has been adopted,\n",
      "which segments a large scene into multiple, independent\n",
      "cells. This facilitates parallel optimization for expansive\n",
      "environments. With the same spirit, Zhao _et al_ . [161] proposed a distributed implementation of 3D GS training. An\n",
      "additional challenge lies in maintaining visual quality, as\n",
      "large-scale scenes often feature texture-less surfaces that can\n",
      "\n",
      "\n",
      "\n",
      "hamper the effectiveness of optimization such as Gaussian\n",
      "initialization and density control (Sec. 3.2). Enhancing the\n",
      "optimization algorithm presents a viable solution to mitigate this issue [44], [164]. **ii** ) Regarding **rendering**, the\n",
      "adoption of the Level of Details (LoD) technique from\n",
      "computer graphics has proven instrumental. LoD adjusts\n",
      "the complexity of 3D scenes to balance visual quality with\n",
      "computational efficiency. Current implementations involve\n",
      "feeding only the essential Gaussians to the rasterizer [164],\n",
      "or designing explicit LoD structures like the Octree [165]\n",
      "and hierarchy [162]. Furthermore, integrating extra input\n",
      "modalities like LiDAR can further enhanced the reconstruction process [158]–[160].\n",
      "One prominent challenge in large-scale scene reconstruction lies in handling sparse or incomplete capture\n",
      "data, which can be mitigated through few-shot adaptation\n",
      "schemes (see Sec. 4.1) or generalizable priors (see “learning\n",
      "physical priors from large-scale data” in Sec. 7). Meanwhile,\n",
      "memory and computational bottlenecks can be addressed\n",
      "via distributed learning strategies [161], such as parameter\n",
      "partitioning across GPU clusters and parallel batched multiview optimization.\n",
      "\n",
      "\n",
      "**5.7** **Physics**\n",
      "\n",
      "\n",
      "The simulation of complex real-world dynamics, such as\n",
      "seed dispersal or fluid motion, is pivotal for applications\n",
      "spanning virtual reality, animation, and scientific modeling,\n",
      "where realism hinges on accurate physical behavior. Advances in video diffusion models have driven progress in\n",
      "4D content generation, yet these methods might produce\n",
      "visually plausible results that violate fundamental physical\n",
      "laws. 3D GS emerges as a promising solution by embedding\n",
      "physical constraints and properties into scene representations, enabling both visually convincing and physically\n",
      "coherent simulations.\n",
      "\n",
      "Existing methods differ in how they formulate and integrate physics-based priors into their frameworks. The most\n",
      "common approach is employing physics simulation engines\n",
      "( _e.g_ ., MLS-MPM [268]) to guide the dynamics generation.\n",
      "The material point method [268] and position based dynamics [269] — numerical methods used in computer graphics\n",
      "for simulating deformations in materials like fluids, granular media, and fracturing solids — have been extensively\n",
      "explored by the community through various customizations [21], [143], [166]–[171]. Analytical material models,\n",
      "such as mass-spring systems, have also demonstrated success in approximating deformations by explicitly encoding\n",
      "material properties into 3D Gaussians [172]. Across these\n",
      "methods, 3D Gaussians are treated as discrete particles (with\n",
      "one exception [173] using a continuous representation) and\n",
      "serve as computational units within the chosen simulator.\n",
      "Unknown material properties or physical parameters are\n",
      "typically learned through video-based supervision from\n",
      "conditional generative models.\n",
      "\n",
      "Despite advancements in physics based 3D GS frameworks, critical limitations persist. Current systems struggle\n",
      "to unify diverse physical behaviors ( _e.g_ ., rigid, elastic, or\n",
      "soft-body dynamics) into cohesive simulations, handle complex multi-object interactions without manual intervention,\n",
      "and model scene-level interactions such as environmental\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12\n",
      "\n",
      "\n",
      "TABLE 1\n",
      "Comparison of **localization** methods (§6.1) on Replica [261] (static scenes), in terms of absolute trajectory error (ATE, cm). (The three best\n",
      "scores are marked in **red**, **blue**, and **green**, respectively. These notes also apply to the other tables.)\n",
      "\n",
      "\n",
      "|Method|GS|Room0 Room1 Room2 Office0 Office1 Office2 Offci e3 Office4|Avarage|\n",
      "|---|---|---|---|\n",
      "|iMAP [262]<br>[ICCV21]<br>Vox-Fusion [263]<br>[ISMAR22]<br>NICE-SLAM [264]<br>[CVPR22]<br>ESLAM [265]<br>[CVPR23]<br>Point-SLAM [266]<br>[ICCV23]<br>Co-SLAM [267]<br>[CVPR23]<br>Gaussian-SLAM [114]<br>[arXiv]<br>GSSLAM [113]<br>[CVPR24]<br>GS-SLAM [111]<br>[CVPR24]<br>SplaTAM [112]<br>[CVPR24]|✓<br>✓<br>✓<br>✓|3.12<br>2.54<br>2.31<br>1.69<br>1.03<br>3.99<br>4.05<br>1.93<br>1.37<br>4.70<br>1.47<br>8.48<br>2.04<br>2.58<br>1.11<br>2.94<br>0.97<br>1.31<br>1.07<br>0.88<br>1.00<br>1.06<br>1.10<br>1.13<br>0.71<br>0.70<br>0.52<br>0.57<br>0.55<br>0.58<br>0.72<br>0.63<br>0.61<br>0.41<br>0.37<br>0.38<br>0.48<br>0.54<br>0.69<br>0.72<br>0.70<br>0.95<br>1.35<br>0.59<br>0.55<br>2.03<br>1.56<br>0.72<br>3.35<br>8.74<br>3.13<br>1.11<br>0.81<br>0.78<br>1.08<br>7.21<br>0.47<br>0.43<br>0.31<br>0.70<br>0.57<br>0.31<br>0.31<br>3.20<br>0.48<br>0.53<br>0.33<br>0.52<br>0.41<br>0.59<br>0.46<br>0.70<br>0.31<br>0.40<br>0.29<br>0.47<br>0.27<br>0.29<br>0.32<br>0.55|2.58<br>3.09<br>1.06<br>0.63<br>0.52<br>1.00<br>3.27<br>0.79<br>0.50<br>0.36|\n",
      "\n",
      "\n",
      "\n",
      "TABLE 2\n",
      "Collection of representative datasets for 3D GS. Here PC represents\n",
      "point clouds.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|Name|Type # Sample|Task|\n",
      "|---|---|---|\n",
      "|Tanks&Temples [270][TOG17]<br>RealEstate10K [271][TOG18]<br>DeepBlending [272][TOG18]<br>LLFF [273][TOG19]<br>NeRF [12][ECCV20]<br>ACID [274][ICCV21]<br>Mip-NeRF 360 [8][CVPR22]|RGB<br>14<br>RGB<br>1,000<br>RGB<br>19<br>RGB<br>8<br>RGB<br>8<br>RGB<br>700+<br>RGB<br>9|Novel View<br>Synthesis|\n",
      "|TUM RGB-D [275][IROS12]<br>KITTI [276][CVPR12]<br>ScanNet [277][CVPR17]<br>Replica [261][arXiv19]<br>Waymo [278][CVPR20]<br>nuScenes [279][CVPR20]<br>RLBench [280][RA-L20]<br>Robomimic [281][CoRL22]|RGB-D<br>39<br>RGB-D&PC<br>11<br>RGB-D<br>1,513<br>RGB-D<br>18<br>RGB-D&PC<br>1,150<br>RGB-D&PC<br>1,000<br>RGB<br>100<br>RGB<br>800|Robotics|\n",
      "|D-NeRF [184][CVPR21]<br>HyperNeRF [185][TOG21]<br>NeRF-DS [282][CVPR23]|RGB<br>8<br>RGB<br>6<br>RGB<br>8|Dynamic Scene<br>Reconstruction|\n",
      "|CoNeRF [283][CVPR22]<br>SPIn-NeRF [284][CVPR23]<br>Tensor4D [285][CVPR23]<br>OmniObject3D [286][CVPR23]<br>Objaverse [287][CVPR23]|RGB<br>7<br>RGB<br>10<br>RGB<br>4<br>3D Object<br>6,000<br>3D Object<br>800K+|Generation<br>and Editing|\n",
      "|People-Snapshot [288][CVPR18]<br>VOCASET [289][CVPR19]<br>THUman [290][ICCV19]<br>THUman2.0 [291][CVPR21]<br>ZJU-Mocap [292][CVPR21]<br>H3DS [293][ICCV21]<br>THUman3.0 [294][TPAMI22]|RGB<br>24<br>RGB<br>12<br>RGB<br>200<br>RGB-D<br>500<br>RGB<br>9<br>RGB<br>23<br>3D Scan<br>20|Avatar|\n",
      "|SCARED [295][MICCAI19]<br>EndoNeRF [258][MICCAI22]<br>X3D [296][CVPR24]|RGB-D<br>9<br>RGB<br>2<br>X-ray<br>15|Medical|\n",
      "|CityNeRF [297][ECCV22]<br>Waymo Block-NeRF [298][CVPR22]<br>UrbanBIS [299][SIGGR23]<br>GauU-Scene [160][arXiv24]|RGB<br>12<br>RGB&PC<br>1<br>RGB&PC<br>6<br>RGB&PC<br>1|Large-scale<br>Reconstrction|\n",
      "\n",
      "\n",
      "\n",
      "feedback and dynamic lighting changes. Integrating adaptive physics engines capable of multi-object and multimaterial interactions, developing new simulation architectures that are compatible with priors learned from largescale data, and expanding datasets to encompass diverse\n",
      "materials and dynamic scenarios are equally vital.\n",
      "\n",
      "\n",
      "**6** **P** **ERFORMANCE** **C** **OMPARISON**\n",
      "\n",
      "\n",
      "In this section, we provide more empirical evidence by\n",
      "presenting the performance of several 3D GS algorithms\n",
      "that we previously discussed. The diverse applications of\n",
      "3D GS across numerous tasks, coupled with the customtailored algorithmic designs for each task, render a uniform\n",
      "\n",
      "\n",
      "\n",
      "comparison of all 3D GS algorithms across a single task or\n",
      "dataset impracticable. For comprehensiveness, we provide a\n",
      "collection of representative datasets in Table 2 according to\n",
      "our analysis in Sec. 5. Due to the limited space, we have chosen several representative tasks for an in-depth performance\n",
      "evaluation. The performance scores are primarily sourced\n",
      "from the original papers, except where indicated otherwise.\n",
      "[We also maintain a Github repository for this section.](https://github.com/guikunchen/3DGS-Benchmarks)\n",
      "\n",
      "\n",
      "**6.1** **Performance Benchmarking: Localization**\n",
      "\n",
      "\n",
      "The localization task in SLAM involves determining the\n",
      "precise position and orientation of a robot or device within\n",
      "an environment, typically using sensor data.\n",
      "\n",
      "_•_ **Dataset:** Replica [261] dataset is a collection of 18 highly\n",
      "detailed 3D indoor scenes. These scenes are not only visually\n",
      "realistic but also offer comprehensive data including dense\n",
      "meshes, high-quality HDR textures, and detailed semantic\n",
      "information for each element. Following [262], three sequences about rooms and five sequences about offices are\n",
      "used for the evaluation.\n",
      "\n",
      "_•_ **Benchmarking Algorithms:** For performance comparison,\n",
      "we involve four recent 3D GS based algorithms [111]–[114]\n",
      "and six typical SLAM methods [262]–[267].\n",
      "\n",
      "_•_ **Evaluation Metric:** The root mean square error (RMSE)\n",
      "of the absolute trajectory error (ATE) is a commonly used\n",
      "metric in evaluating SLAM systems [275], which measures\n",
      "the root mean square of the Euclidean distances between the\n",
      "estimated and true positions over the entire trajectory.\n",
      "\n",
      "_•_ **Result:** As shown in Table 1, the recent 3D Gaussians\n",
      "based localization algorithms have a clear advantage over\n",
      "existing NeRF based dense visual SLAM. For example,\n",
      "SplaTAM [112] achieves a trajectory error improvement of\n",
      "_∼_ **50** %, decreasing it from 0.52cm to **0.36cm** compared to\n",
      "the previous state-of-the-art (SOTA) [266]. We attribute this\n",
      "to the dense and accurate 3D Gaussians reconstructed for\n",
      "\n",
      "scenes, which can handle the noise of real sensors. This\n",
      "reveals that effective scene representations can improve the\n",
      "accuracy of localization tasks.\n",
      "\n",
      "\n",
      "**6.2** **Performance Benchmarking: Static Scenes**\n",
      "\n",
      "\n",
      "Rendering focuses on transforming computer-readable information ( _e.g_ ., 3D objects in the scene) to pixel-based\n",
      "images. This section focuses on evaluating the quality of\n",
      "rendering results in static scenes.\n",
      "\n",
      "_•_ **Dataset:** The same dataset as in Sec. 6.1, _i.e_ ., Replica [261],\n",
      "is used for comparison. The testing views are the same as\n",
      "those collected by [262].\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 13\n",
      "\n",
      "\n",
      "TABLE 3\n",
      "Comparison of **mapping** methods (§6.2) on Replica [261] (static scenes), in terms of PSNR, SSIM, and LPIPS. The results for FPS are taken\n",
      "from [113] using one 4090 GPU.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "|Method|GS|Metric|Room0 Room1 Room2 Office0 Office1 Office2 Office3 Office4|Avarage|FPS|\n",
      "|---|---|---|---|---|---|\n",
      "|NICE-SLAM [264] [CVPR22]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|22.12<br>22.47<br>24.52<br>29.07<br>30.34<br>19.66<br>22.23<br>24.94<br>0.69<br>0.76<br>0.81<br>0.87<br>0.89<br>0.80<br>0.80<br>0.86<br>0.33<br>0.27<br>0.21<br>0.23<br>0.18<br>0.23<br>0.21<br>0.20|24.42<br>0.81<br>0.23|0.54|\n",
      "|Vox-Fusion [263] [ISMAR22]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|22.39<br>22.36<br>23.92<br>27.79<br>29.83<br>20.33<br>23.47<br>25.21<br>0.68<br>0.75<br>0.80<br>0.86<br>0.88<br>0.79<br>0.80<br>0.85<br>0.30<br>0.27<br>0.23<br>0.24<br>0.18<br>0.24<br>0.21<br>0.20|24.41<br>0.80<br>0.24|2.17|\n",
      "|Point-SLAM [266] [ICCV23]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|32.40<br>34.08<br>35.50<br>38.26<br>39.16<br>33.99<br>33.48<br>33.49<br>0.97<br>0.98<br>0.98<br>0.98<br>0.99<br>0.96<br>0.96<br>0.98<br>0.11<br>0.12<br>0.11<br>0.10<br>0.12<br>0.16<br>0.13<br>0.14|35.17<br>0.97<br>0.12|1.33|\n",
      "|SplaTAM [112] [CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|32.86<br>33.89<br>35.25<br>38.26<br>39.17<br>31.97<br>29.70<br>31.81<br>0.98<br>0.97<br>0.98<br>0.98<br>0.98<br>0.97<br>0.95<br>0.95<br>0.07<br>0.10<br>0.08<br>0.09<br>0.09<br>0.10<br>0.12<br>0.15|34.11<br>0.97<br>0.10|-|\n",
      "|GS-SLAM [111] [CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|31.56<br>32.86<br>32.59<br>38.70<br>41.17<br>32.36<br>32.03<br>32.92<br>0.97<br>0.97<br>0.97<br>0.99<br>0.99<br>0.98<br>0.97<br>0.97<br>0.09<br>0.07<br>0.09<br>0.05<br>0.03<br>0.09<br>0.11<br>0.11|34.27<br>0.97<br>0.08|-|\n",
      "|GSSLAM [113] [CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|34.83<br>36.43<br>37.49<br>39.95<br>42.09<br>36.24<br>36.70<br>36.07<br>0.95<br>0.96<br>0.96<br>0.97<br>0.98<br>0.96<br>0.96<br>0.96<br>0.07<br>0.08<br>0.07<br>0.07<br>0.06<br>0.08<br>0.07<br>0.10|37.50<br>0.96<br>0.07|769|\n",
      "|Gaussian-SLAM [114][arXiv]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|34.31<br>37.28<br>38.18<br>43.97<br>43.56<br>37.39<br>36.48<br>40.19<br>0.99<br>0.99<br>0.99<br>1.00<br>0.99<br>0.99<br>0.99<br>1.00<br>0.08<br>0.07<br>0.07<br>0.04<br>0.07<br>0.08<br>0.08<br>0.07|38.90<br>0.99<br>0.07|-|\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "_•_ **Benchmarking Algorithms:** For performance comparison,\n",
      "we involve four recent papers which introduce 3D Gaussians into their systems [111]–[114], as well as three dense\n",
      "SLAM methods [263], [264], [266].\n",
      "\n",
      "_•_ **Evaluation Metric:** Peak signal-to-noise ratio (PSNR),\n",
      "structural similarity (SSIM) [300], and learned perceptual\n",
      "image patch similarity (LPIPS) [301] are used for measuring\n",
      "RGB rendering performance.\n",
      "\n",
      "_•_ **Result:** Table 3 shows that 3D Gaussians based systems\n",
      "generally outperform the three dense SLAM competitors.\n",
      "For example, Gaussian-SLAM [114] establishes new SOTA\n",
      "and outperforms previous methods by a large margin.\n",
      "Compared to Point-SLAM [266], GSSLAM [113] is about\n",
      "**578** times faster in achieving very competitive accuracy.\n",
      "In contrast to previous method [266] that relies on depth\n",
      "information, such as depth-guided ray sampling, for synthesizing novel views, 3D GS based system eliminates this\n",
      "need, allowing for high-fidelity rendering for any views.\n",
      "\n",
      "\n",
      "**6.3** **Performance Benchmarking: Dynamic Scenes**\n",
      "\n",
      "\n",
      "This section focuses on evaluating the rendering quality in\n",
      "dynamic scenes.\n",
      "\n",
      "_•_ **Dataset:** D-NeRF [184] dataset includes videos with 50\n",
      "to 200 frames each, captured from unique viewpoints. It\n",
      "features synthetic, animated objects in complex scenes, with\n",
      "non-Lambertian materials. The dataset provides 50 to 200\n",
      "training images and 20 test images per scene, designed for\n",
      "evaluating models in the monocular setting. The testing\n",
      "views are the same as the original paper [184].\n",
      "\n",
      "_•_ **Benchmarking Algorithms:** For performance comparison,\n",
      "we involve five recent papers that model dynamic scenes\n",
      "with 3D GS [93]–[95], [126], [132], as well as six NeRF based\n",
      "approaches [37], [184], [187], [302]–[304].\n",
      "\n",
      "_•_ **Evaluation Metric:** The same metrics as in Sec. 6.2, _i.e_ .,\n",
      "PSNR, SSIM [300], and LPIPS [301], are used for evaluation.\n",
      "\n",
      "_•_ **Result:** From Table 4 we can observe that 3D GS based\n",
      "methods outperform existing SOTAs by a clear margin. The\n",
      "static version of 3D GS [10] fails to reconstruct dynamic\n",
      "\n",
      "\n",
      "\n",
      "TABLE 4\n",
      "Comparison of **reconstruction** methods (§6.3) on D-NeRF [184]\n",
      "(dynamic scenes), in terms of PSNR, SSIM, and LPIPS. _[∗]_ denotes\n",
      "results reported in [95].\n",
      "\n",
      "|Method|GS|PSNR↑ SSIM↑ LPIPS↓|\n",
      "|---|---|---|\n",
      "|D-NeRF [184]<br>[CVPR21]<br>TiNeuVox-B [302]<br>[SGA22]<br>KPlanes [37]<br>[CVPR23]<br>HexPlane-Slim [303]<br>[CVPR23]<br>FFDNeRF [187]<br>[ICCV23]<br>MSTH [304]<br>[NeurIPS23]<br>3D GS_∗_[10]<br>[TOG23]<br>4DGS [93]<br>[ICLR24]<br>4D-GS [95]<br>[CVPR24]<br>GaGS [132]<br>[CVPR24]<br>CoGS [126]<br>[CVPR24]<br>D-3DGS [94]<br>[CVPR24]|✓<br>✓<br>✓<br>✓<br>✓<br>✓|30.50<br>0.95<br>0.07<br>32.67<br>0.97<br>0.04<br>31.61<br>0.97<br>-<br>32.68<br>0.97<br>0.02<br>32.68<br>0.97<br>0.02<br>31.34<br>0.98<br>0.02<br>23.19<br>0.93<br>0.08<br>34.09<br>0.98<br>-<br>34.05<br>0.98<br>0.02<br>37.36<br>0.99<br>0.01<br>37.90<br>0.98<br>0.02<br>39.51<br>0.99<br>0.01|\n",
      "\n",
      "\n",
      "\n",
      "scenes, resulting in a sharp drop in performance. By modeling the dynamics, D-3DGS [94] outperforms the SOTA\n",
      "method, FFDNeRF [187], by **6.83** dB in terms of PSNR. These\n",
      "results indicate the effectiveness of introducing additional\n",
      "properties or structured information to model the deformation of Gaussians so as to model the scene dynamics.\n",
      "\n",
      "\n",
      "**6.4** **Performance Benchmarking: Human Avatar**\n",
      "\n",
      "\n",
      "Human avatar modeling aims to create the model of human\n",
      "avatars from a given multi-view video.\n",
      "\n",
      "_•_ **Dataset:** ZJU-MoCap [292] is a prevalent benchmark in human modeling from videos, captured with 23 synchronized\n",
      "cameras at a 1024 _×_ 1024 resolution. Six subjects ( _i.e_ ., 377,\n",
      "386, 387, 392, 393, and 394) are used for evaluation [305].\n",
      "The same testing views following [306] are adopted.\n",
      "\n",
      "_•_ **Benchmarking Algorithms:** For performance comparison,\n",
      "we involve three recent papers which model human avatar\n",
      "with 3D GS [145], [146], [249], as well as six human rendering approaches [292], [305]–[309].\n",
      "\n",
      "_•_ **Evaluation Metric:** PSNR, SSIM [300], and LPIPS* [301]\n",
      "are used for measuring RGB rendering performance. Here\n",
      "LPIPS* equals to LPIPS _×_ 1000.\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14\n",
      "\n",
      "\n",
      "\n",
      "TABLE 5\n",
      "Comparison of **reconstruction** methods (§6.4) on ZJU-MoCap [292]\n",
      "(avatar), in terms of PSNR, SSIM, and LPIPS*. The results for non-GS\n",
      "methods are taken from [146].\n",
      "\n",
      "|Method|GS|PSNR↑ SSIM↑ LPIPS*↓|\n",
      "|---|---|---|\n",
      "|NeuralBody [292] [CVPR21]<br>AnimNeRF [307] [ICCV21]<br>PixelNeRF [308] [ICCV21]<br>NHP [309] [NeurIPS21]<br>HumanNeRF [305] [CVPR22]<br>Instant-NVR [306] [CVPR23]<br>GauHuman [145] [CVPR24]<br>3DGS-Avatar [249] [CVPR24]<br>GART [146] [CVPR24]|✓<br>✓<br>✓|29.03<br>0.96<br>42.47<br>29.77<br>0.96<br>46.89<br>24.71<br>0.89<br>121.86<br>28.25<br>0.95<br>64.77<br>30.66<br>0.97<br>33.38<br>31.01<br>0.97<br>38.45<br>31.34<br>0.97<br>30.51<br>30.61<br>0.97<br>29.58<br>32.22<br>0.98<br>29.21|\n",
      "\n",
      "\n",
      "\n",
      "TABLE 6\n",
      "Comparison of **reconstruction** methods (§6.5) on EndoNeRF [258]\n",
      "(surgical scenes), in terms of PSNR, SSIM, and LPIPS. The results for\n",
      "non-GS methods are taken from [153]. FPS and GPU usage for training\n",
      "(Mem.) are measured using one 4090 GPU [153].\n",
      "\n",
      "|Method|GS|PSNR↑ SSIM↑ LPIPS↓|FPS↑ Mem.↓|\n",
      "|---|---|---|---|\n",
      "|EndoNeRF [258][MICCAI22]<br>EndoSurf [260][MICCAI23]<br>LerPlane-9k [259][MICCAI23]<br>LerPlane-32k [259][MICCAI23]<br>Endo-4DGS [152][MICCAI24]<br>EndoGaussian [153][arXiv]<br>HFGS [155][BMVC24]|✓<br>✓<br>✓|36.06<br>0.93<br>0.09<br>36.53<br>0.95<br>0.07<br>35.00<br>0.93<br>0.08<br>37.38<br>0.95<br>0.05<br>37.00<br>0.96<br>0.05<br>37.85<br>0.96<br>0.05<br>38.14<br>0.97<br>0.03|0.04<br>19GB<br>0.04<br>17GB<br>0.91<br>20GB<br>0.87<br>20GB<br>-<br>4GB<br>195.09<br>2GB<br>-<br>-|\n",
      "\n",
      "\n",
      "\n",
      "_•_ **Result:** Table 5 presents the numerical results of topleading solutions in human avatar modeling. We observe\n",
      "that introducing 3D GS into the framework leads to consistent performance improvements in both rendering quality\n",
      "and speed. For instance, GART [146] outperforms current\n",
      "SOTA, Instant-NVR [306], by **1.21** dB in terms of PSNR.\n",
      "Considering the enhanced fidelity, inference speed and editability, 3D GS based avatar modeling may revolutionize\n",
      "the field of 3D animation, interactive gaming, _etc_ .\n",
      "\n",
      "\n",
      "**6.5** **Performance Benchmarking: Surgical Scenes**\n",
      "\n",
      "\n",
      "3D reconstruction from endoscopic video is critical to\n",
      "robotic-assisted minimally invasive surgery, enabling preoperative planning, training through AR/VR simulations,\n",
      "and intraoperative guidance.\n",
      "\n",
      "_•_ **Dataset:** EndoNeRF [258] dataset presents a specialized\n",
      "collection of stereo camera captures, comprising two samples of in-vivo prostatectomy. It is tailored to represent realworld surgical complexities, including challenging scenes\n",
      "with tool occlusion and pronounced non-rigid deformation.\n",
      "The same testing views as in [260] are used.\n",
      "\n",
      "_•_ **Benchmarking Algorithms:** For performance comparison,\n",
      "we involve three recent papers which reconstruct dynamic\n",
      "3D endoscopic scenes with GS [152], [153], [155], as well as\n",
      "three NeRF-based surgical reconstruction approaches [258]–\n",
      "\n",
      "[260].\n",
      "\n",
      "_•_ **Evaluation Metric:** PSNR, SSIM [300], and LPIPS [301]\n",
      "are adopted for evaluation. In addition, the requirement for\n",
      "GPU memory is also reported.\n",
      "\n",
      "_•_ **Result:** Table 6 shows that introducing the explicit representation of 3D Gaussians leads to several significant improvements. For instance, EndoGaussian [153] outperforms\n",
      "a strong baseline, LerPlane-32k [259], among all metrics. In\n",
      "particular, EndoGaussian demonstrates an approximate 224fold increase in speed while consumes just 10% of the GPU\n",
      "\n",
      "\n",
      "\n",
      "resources. These impressive results attest to the efficiency\n",
      "of GS-based methods, which not only expedite processing\n",
      "but also minimize GPU load, thus easing the demands on\n",
      "hardware. Such attributes are vitally significant for realworld surgical application deployment, where optimized\n",
      "resource usage can be a key determinant of practical utility.\n",
      "\n",
      "\n",
      "**7** **F** **UTURE** **R** **ESEARCH** **D** **IRECTIONS**\n",
      "\n",
      "\n",
      "As impressive as those follow-up works on 3D GS are, and\n",
      "as much as those fields have been or might be revolutionized by 3D GS, there is a general agreement that 3D GS still\n",
      "has considerable room for improvement.\n",
      "\n",
      "_•_ **Physics- and Semantics-aware Scene Representation.** As\n",
      "a new, explicit scene representation technique, 3D Gaussian\n",
      "offers transformative potential beyond merely enhancing\n",
      "novel-view synthesis. It has the potential to pave the way\n",
      "for simultaneous advancements in scene reconstruction and\n",
      "understanding by devising physics- and semantics-aware\n",
      "3D GS systems. While significant progress has been made\n",
      "in physics (Sec. 5.7) and semantics [310]–[315] individually, there remains considerable untapped potential in their\n",
      "synergistic integration. This is poised to revolutionize a\n",
      "range of fields and downstream applications. For instance,\n",
      "incorporating prior knowledge such as the general shape\n",
      "of objects can reduce the need for extensive training viewpoints [47], [48] while improving geometry/surface reconstruction [77], [316]. A critical metric for assessing scene\n",
      "representation is the quality of its generated scenes, which\n",
      "encompasses challenges in geometry, texture, and lighting\n",
      "fidelity [66], [128], [141]. By merging physical principles\n",
      "and semantic information within the 3D GS framework,\n",
      "one can expect that the quality will be enhanced, thereby\n",
      "facilitating dynamics modeling [21], [166], editing [90], [92],\n",
      "generation [133], [134], and beyond. In a nutshell, pursuing\n",
      "this advanced and versatile scene representation opens up\n",
      "new possibilities for innovation in computational creativity\n",
      "and practical applications across diverse domains.\n",
      "\n",
      "_•_ **Learning Physical Priors from Large-scale Data.** As we\n",
      "explore the potential of physics- and semantics-aware scene\n",
      "representations, leveraging large-scale datasets to learn generalizable, physical priors emerges as a promising direction.\n",
      "The goal is to model the inherent physical properties and\n",
      "dynamics embedded within real-world data, transforming\n",
      "them into actionable insights that can be applied across various domains such as robotics and visual effects. Establishing\n",
      "a learning framework for extracting these generalizable\n",
      "priors enables the application of these insights to specific\n",
      "tasks in a few-shot manner. For instance, it allows for rapid\n",
      "adaptation to new objects and environments with minimal\n",
      "data input. Furthermore, integrating physical priors can enhance not only the accuracy and quality of generated scenes\n",
      "but also their interactive and dynamic qualities. This is\n",
      "particularly valuable in AR/VR environments, where users\n",
      "interact with virtual objects that behave in ways consistent\n",
      "with their real-world counterparts. However, the existing\n",
      "body of work on capturing and distilling physics-based\n",
      "knowledge from extensive 2D and 3D datasets remains\n",
      "sparse. Notable efforts in related area include the continuum\n",
      "mechanics based GS systems (Sec. 5.7), and the generalizable\n",
      "Gaussian representation based on multi-view stereo [317].\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 15\n",
      "\n",
      "\n",
      "\n",
      "Further exploration on real2sim and sim2real might offer\n",
      "viable routes for advancements in this field.\n",
      "\n",
      "_•_ **Modeling Internal Structures of Objects with 3D GS.**\n",
      "Despite the ability of 3D GS to produce highly photorealistic\n",
      "renderings, modeling internal structures of objects ( _e.g_ ., for a\n",
      "scanned object in computed tomography) within the current\n",
      "GS framework presents a notable challenge. Due to the\n",
      "splatting and density control process, the current representation of 3D Gaussian is unorganized and cannot align\n",
      "well with the object’s actual internal structures. Moreover,\n",
      "there is a strong preference in various applications to depict\n",
      "objects as volumes ( _e.g_ ., computed tomography). However,\n",
      "the disordered nature of 3D GS makes volume modeling\n",
      "particularly difficult. Li _et al_ . [318] used 3D Gaussians with\n",
      "density control as the basis for the volumetric representation\n",
      "and did not involve the splatting process. X-Gaussian [319]\n",
      "involves the splatting process for fast training and inference but cannot generate volumetric representation. Using\n",
      "3D GS to model the internal structures of objects remains\n",
      "unanswered and deserves further exploration.\n",
      "\n",
      "_•_ **3D GS for Simulation in Autonomous Driving and be-**\n",
      "**yond.** Collecting real-world datasets for autonomous driving is both expensive and logistically challenging, yet crucial\n",
      "for training effective image-based perception systems. To\n",
      "mitigate these issues, simulation emerges as a cost-effective\n",
      "alternative, enabling the generation of synthetic datasets\n",
      "across diverse environments. However, the development of\n",
      "simulators capable of producing photorealistic and diverse\n",
      "synthetic data is fraught with challenges. These include\n",
      "achieving a high level of quality, accommodating various\n",
      "control methods, and accurately simulating a range of lighting conditions. While early efforts [188]–[190] in reconstructing urban/street scenes with 3D GS have been encouraging,\n",
      "they are just the tip of the iceberg in terms of the full capabilities. There remain numerous critical aspects to be explored,\n",
      "such as the integration of user-defined object models, the\n",
      "modeling of physics-aware scene changes ( _e.g_ ., the rotation\n",
      "of vehicle wheels), and the enhancement of controllability\n",
      "and quality ( _e.g_ ., in varying lighting conditions). Mastery\n",
      "of these capabilities would not only advance autonomous\n",
      "systems but also redefine computational understanding of\n",
      "physical spaces — a leap with implications for world models, spatial intelligence, embodied AI, and beyond.\n",
      "\n",
      "_•_ **Empowering 3D GS with More Possibilities.** Despite the\n",
      "significant potential of 3D GS, the full scope of applications\n",
      "for 3D GS remains largely untapped. A promising avenue\n",
      "for exploration involves augmenting 3D Gaussians with additional attributes ( _e.g_ ., linguistic and spatiotemporal properties as mentioned in Sec. 4.5) and introducing structured\n",
      "information (e.g., spatial MLPs and grids as mentioned in\n",
      "Sec. 4.6), tailored for specific applications. Moreover, recent\n",
      "studies have begun to unveil the capability of 3D GS in\n",
      "several domains, _e.g_ ., point cloud registration [320], image representation and compression [60], and fluid synthesis [171]. These findings highlight a significant opportunity\n",
      "for interdisciplinary scholars to explore 3D GS further.\n",
      "\n",
      "\n",
      "**8** **C** **ONCLUSIONS**\n",
      "\n",
      "\n",
      "To the best of our knowledge, this survey presents the\n",
      "first comprehensive overview of 3D GS, a groundbreaking\n",
      "\n",
      "\n",
      "\n",
      "technique revolutionizing explicit radiance fields, computer\n",
      "graphics, and computer vision. It delineates the paradigm\n",
      "shift from traditional NeRF based methods, spotlighting the\n",
      "advantages of 3D GS in real-time rendering and enhanced\n",
      "editability. Our in-depth analysis and extensive quantitative\n",
      "studies demonstrate the superiority of 3D GS in practical\n",
      "applications, particularly those highly sensitive to latency.\n",
      "We offer insights into principles, prospective research directions, and the unresolved challenges within this domain. Overall, 3D GS stands as a transformative technology,\n",
      "poised to significantly influence future advancements in 3D\n",
      "reconstruction and representation. This survey is intended\n",
      "to serve as a foundational resource, propelling further exploration and progress in this rapidly evolving field.\n",
      "\n",
      "\n",
      "**R** **EFERENCES**\n",
      "\n",
      "\n",
      "[1] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen, “The\n",
      "lumigraph,” in _Seminal Graphics Papers: Pushing the Boundaries,_\n",
      "_Volume 2_, 2023, pp. 453–464.\n",
      "\n",
      "[2] M. Levoy and P. Hanrahan, “Light field rendering,” in _Seminal_\n",
      "_Graphics Papers: Pushing the Boundaries, Volume 2_, 2023, pp. 441–\n",
      "452.\n",
      "\n",
      "[3] C. Buehler, M. Bosse, L. McMillan, S. Gortler, and M. Cohen,\n",
      "“Unstructured lumigraph rendering,” in _Seminal Graphics Papers:_\n",
      "_Pushing the Boundaries, Volume 2_, 2023, pp. 497–504.\n",
      "\n",
      "[4] N. Snavely, S. M. Seitz, and R. Szeliski, “Photo tourism: exploring\n",
      "photo collections in 3d,” in _ACM Trans. Graph._, 2006, pp. 835–846.\n",
      "\n",
      "[5] M. Goesele, N. Snavely, B. Curless, H. Hoppe, and S. M. Seitz,\n",
      "“Multi-view stereo for community photo collections,” in _Proc._\n",
      "_IEEE Int. Conf. Comput. Vis._, 2007, pp. 1–8.\n",
      "\n",
      "[6] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. Valentin,\n",
      "“Fastnerf: High-fidelity neural rendering at 200fps,” in _Proc. IEEE_\n",
      "_Int. Conf. Comput. Vis._, 2021, pp. 14 346–14 355.\n",
      "\n",
      "[7] C. Reiser, S. Peng, Y. Liao, and A. Geiger, “Kilonerf: Speeding up\n",
      "neural radiance fields with thousands of tiny mlps,” in _Proc. IEEE_\n",
      "_Int. Conf. Comput. Vis._, 2021, pp. 14 335–14 345.\n",
      "\n",
      "[8] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, “Mip-nerf 360: Unbounded anti-aliased neural radiance\n",
      "fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2022,\n",
      "pp. 5470–5479.\n",
      "\n",
      "[9] T. M¨uller, A. Evans, C. Schied, and A. Keller, “Instant neural\n",
      "graphics primitives with a multiresolution hash encoding,” _ACM_\n",
      "_Trans. Graph._, vol. 41, no. 4, pp. 1–15, 2022.\n",
      "\n",
      "[10] B. Kerbl, G. Kopanas, T. Leimk¨uhler, and G. Drettakis, “3d\n",
      "gaussian splatting for real-time radiance field rendering,” _ACM_\n",
      "_Trans. Graph._, vol. 42, no. 4, 2023.\n",
      "\n",
      "[11] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, and\n",
      "M. Zollhofer, “Deepvoxels: Learning persistent 3d feature embeddings,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2019,\n",
      "pp. 2437–2446.\n",
      "\n",
      "[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural\n",
      "radiance fields for view synthesis,” in _Proc. Eur. Conf. Comput._\n",
      "_Vis._, 2020, pp. 405–421.\n",
      "\n",
      "[13] H. Pfister, M. Zwicker, J. Van Baar, and M. Gross, “Surfels: Surface\n",
      "elements as rendering primitives,” in _Proceedings of the 27th annual_\n",
      "_conference on Computer graphics and interactive techniques_, 2000, pp.\n",
      "335–342.\n",
      "\n",
      "[14] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Surface\n",
      "splatting,” in _Proceedings of the 28th annual conference on Computer_\n",
      "_graphics and interactive techniques_, 2001, pp. 371–378.\n",
      "\n",
      "[15] L. Ren, H. Pfister, and M. Zwicker, “Object space ewa surface\n",
      "splatting: A hardware accelerated approach to high quality point\n",
      "rendering,” in _Comput. Graph. Forum_, no. 3, 2002, pp. 461–470.\n",
      "\n",
      "[16] W. Yifan, F. Serena, S. Wu, C. Oztireli, and O. Sorkine-Hornung, [¨]\n",
      "“Differentiable surface splatting for point-based geometry processing,” _ACM Trans. Graph._, vol. 38, no. 6, pp. 1–14, 2019.\n",
      "\n",
      "[17] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, “Synsin: Endto-end view synthesis from a single image,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2020, pp. 7467–7477.\n",
      "\n",
      "[18] D. Kalkofen, E. Mendez, and D. Schmalstieg, “Comprehensible\n",
      "visualization for augmented reality,” _IEEE Trans. Vis. Comput._\n",
      "_Graph._, vol. 15, no. 2, pp. 193–204, 2008.\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 16\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[19] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty,\n",
      "D. Luebke, and A. Lefohn, “Towards foveated rendering for gazetracked virtual reality,” _ACM Trans. Graph._, vol. 35, no. 6, pp. 1–12,\n",
      "2016.\n",
      "\n",
      "[20] R. Albert, A. Patney, D. Luebke, and J. Kim, “Latency requirements for foveated rendering in virtual reality,” _ACM Transactions_\n",
      "_on Applied Perception_, vol. 14, no. 4, pp. 1–13, 2017.\n",
      "\n",
      "[21] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. Lau,\n",
      "F. Gao, Y. Yang _et al._, “Vr-gs: A physical dynamics-aware interactive gaussian splatting system in virtual reality,” _arXiv preprint_\n",
      "_arXiv:2401.16663_, 2024.\n",
      "\n",
      "[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai,\n",
      "“Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[23] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam, “Relightable\n",
      "gaussian codec avatars,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[24] T. Zhang, K. Huang, W. Zhi, and M. Johnson-Roberson, “Darkgs:\n",
      "Learning neural illumination and 3d gaussians relighting for\n",
      "robotic exploration in the dark,” in _Proc. IEEE/RSJ Int. Conf. Intell._\n",
      "_Robot. Syst._, 2024.\n",
      "\n",
      "[25] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He, “3d gaussian\n",
      "splatting as new era: A survey,” _IEEE Trans. Vis. Comput. Graph._,\n",
      "2024.\n",
      "\n",
      "[26] A. Dalal, D. Hagen, K. G. Robbersmyr, and K. M. Knausg˚ard,\n",
      "“Gaussian splatting: 3d reconstruction and novel view synthesis,\n",
      "a review,” _IEEE Access_, 2024.\n",
      "\n",
      "[27] Y. Bao, T. Ding, J. Huo, Y. Liu, Y. Li, W. Li, Y. Gao, and J. Luo,\n",
      "“3d gaussian splatting: Survey, technologies, challenges, and\n",
      "opportunities,” _arXiv preprint arXiv:2407.17418_, 2024.\n",
      "\n",
      "[28] T. Wu, Y.-J. Yuan, L.-X. Zhang, J. Yang, Y.-P. Cao, L.-Q. Yan, and\n",
      "L. Gao, “Recent advances in 3d gaussian splatting,” _Comput. Vis._\n",
      "_Media_, pp. 1–30, 2024.\n",
      "\n",
      "[29] L. Kobbelt and M. Botsch, “A survey of point-based techniques in\n",
      "computer graphics,” _Comput. Graph._, vol. 28, no. 6, pp. 801–814,\n",
      "2004.\n",
      "\n",
      "[30] Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan,\n",
      "F. Tombari, J. Tompkin, V. Sitzmann, and S. Sridhar, “Neural\n",
      "fields in visual computing and beyond,” in _Comput. Graph. Forum_,\n",
      "no. 2, 2022, pp. 641–676.\n",
      "\n",
      "[31] W. Wang, Y. Yang, and Y. Pan, “Visual knowledge in\n",
      "the big model era: Retrospect and prospect,” _arXiv preprint_\n",
      "_arXiv:2404.04308_, 2024.\n",
      "\n",
      "[32] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk,\n",
      "W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi\n",
      "_et al._, “Advances in neural rendering,” in _Comput. Graph. Forum_,\n",
      "no. 2, 2022, pp. 703–735.\n",
      "\n",
      "[33] X.-F. Han, H. Laga, and M. Bennamoun, “Image-based 3d object\n",
      "reconstruction: State-of-the-art and trends in the deep learning\n",
      "era,” _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 43, no. 5, pp.\n",
      "1578–1604, 2019.\n",
      "\n",
      "[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and\n",
      "A. Geiger, “Occupancy networks: Learning 3d reconstruction in\n",
      "function space,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2019, pp. 4460–4470.\n",
      "\n",
      "[35] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove,\n",
      "“Deepsdf: Learning continuous signed distance functions for\n",
      "shape representation,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2019, pp. 165–174.\n",
      "\n",
      "[36] C. Sun, M. Sun, and H.-T. Chen, “Direct voxel grid optimization:\n",
      "Super-fast convergence for radiance fields reconstruction,” in\n",
      "_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 5459–\n",
      "5469.\n",
      "\n",
      "[37] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and\n",
      "A. Kanazawa, “K-planes: Explicit radiance fields in space, time,\n",
      "and appearance,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recog-_\n",
      "_nit._, 2023, pp. 12 479–12 488.\n",
      "\n",
      "[38] J. P. Grossman and W. J. Dally, “Point sample rendering,” in\n",
      "_Render. Tech._, 1998, pp. 181–192.\n",
      "\n",
      "[39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Ewa volume\n",
      "splatting,” in _Proceedings Visualization, 2001. VIS’01._, 2001, pp. 29–\n",
      "538.\n",
      "\n",
      "[40] ——, “Ewa splatting,” _IEEE Trans. Vis. Comput. Graph._, vol. 8,\n",
      "no. 3, pp. 223–238, 2002.\n",
      "\n",
      "[41] K.-A. Aliev, A. Sevastopolsky, M. Kolos, D. Ulyanov, and V. Lempitsky, “Neural point-based graphics,” in _Proc. Eur. Conf. Comput._\n",
      "_Vis._, 2020, pp. 696–712.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[42] D. R¨uckert, L. Franke, and M. Stamminger, “Adop: Approximate\n",
      "differentiable one-pixel point rendering,” _ACM Trans. Graph._,\n",
      "vol. 41, no. 4, pp. 1–14, 2022.\n",
      "\n",
      "[43] C. Lassner and M. Zollhofer, “Pulsar: Efficient sphere-based neural rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2021, pp. 1440–1449.\n",
      "\n",
      "[44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, and\n",
      "X. Chen, “Gaussianpro: 3d gaussian splatting with progressive\n",
      "propagation,” in _Proc. ACM Int. Conf. Mach. Learn._, 2024.\n",
      "\n",
      "[45] H. Xiong, S. Muttukuru, R. Upadhyay, P. Chari, and A. Kadambi,\n",
      "“Sparsegs: Real-time 360 _{\\_ deg _}_ sparse view synthesis using\n",
      "gaussian splatting,” _arXiv preprint arXiv:2312.00206_, 2023.\n",
      "\n",
      "[46] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, “Fsgs: Real-time fewshot view synthesis using gaussian splatting,” in _Proc. Eur. Conf._\n",
      "_Comput. Vis._, 2024.\n",
      "\n",
      "[47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, “pixelsplat:\n",
      "3d gaussian splats from image pairs for scalable generalizable 3d\n",
      "reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2024.\n",
      "\n",
      "[48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, “Splatter image:\n",
      "Ultra-fast single-view 3d reconstruction,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[49] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu,\n",
      "“Dngaussian: Optimizing sparse-view 3d gaussian radiance\n",
      "fields with global-local depth normalization,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[50] A. Swann, M. Strong, W. K. Do, G. S. Camps, M. Schwager, and\n",
      "M. Kennedy III, “Touch-gs: Visual-tactile supervised 3d gaussian\n",
      "splatting,” _arXiv preprint arXiv:2403.09875_, 2024.\n",
      "\n",
      "[51] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger,\n",
      "T.-J. Cham, and J. Cai, “Mvsplat: Efficient 3d gaussian splatting\n",
      "from sparse multi-view images,” in _Proc. Eur. Conf. Comput. Vis._,\n",
      "2024.\n",
      "\n",
      "[52] C. Wewer, K. Raj, E. Ilg, B. Schiele, and J. E. Lenssen, “latentsplat:\n",
      "Autoencoding variational gaussians for fast generalizable 3d\n",
      "reconstruction,” _arXiv preprint arXiv:2403.16292_, 2024.\n",
      "\n",
      "[53] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen,\n",
      "and G. Wetzstein, “Grm: Large gaussian reconstruction model\n",
      "for efficient 3d reconstruction and generation,” _arXiv preprint_\n",
      "_arXiv:2403.14621_, 2024.\n",
      "\n",
      "[54] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. Wang,\n",
      "“Gamba: Marry gaussian splatting with mamba for single view\n",
      "3d reconstruction,” _arXiv preprint arXiv:2403.18795_, 2024.\n",
      "\n",
      "[55] J. Zhang, J. Li, X. Yu, L. Huang, L. Gu, J. Zheng, and X. Bai, “Corgs: Sparse-view 3d gaussian splatting via co-regularization,” in\n",
      "_Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[56] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. Xu, and Z. Wang, “Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps,” _arXiv preprint arXiv:2311.17245_, 2023.\n",
      "\n",
      "[57] K. Navaneet, K. P. Meibodi, S. A. Koohpayegani, and\n",
      "H. Pirsiavash, “Compact3d: Compressing gaussian splat radiance field models with vector quantization,” _arXiv preprint_\n",
      "_arXiv:2311.18159_, 2023.\n",
      "\n",
      "[58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, “Compact 3d\n",
      "gaussian representation for radiance field,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[59] W. Morgenstern, F. Barthel, A. Hilsmann, and P. Eisert, “Compact\n",
      "3d scene representation via self-organizing gaussian grids,” _arXiv_\n",
      "_preprint arXiv:2312.13299_, 2023.\n",
      "\n",
      "[60] X. Zhang, X. Ge, T. Xu, D. He, Y. Wang, H. Qin, G. Lu, J. Geng,\n",
      "and J. Zhang, “Gaussianimage: 1000 fps image representation\n",
      "and compression by 2d gaussian splatting,” in _Proc. Eur. Conf._\n",
      "_Comput. Vis._, 2024.\n",
      "\n",
      "[61] S. Niedermayr, J. Stumpfegger, and R. Westermann, “Compressed 3d gaussian splatting for accelerated novel view synthesis,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: Hash-grid\n",
      "assisted context for 3d gaussian splatting compression,” in _Proc._\n",
      "_Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[63] P. Papantonakis, G. Kopanas, B. Kerbl, A. Lanvin, and G. Drettakis, “Reducing the memory footprint of 3d gaussian splatting,”\n",
      "in _I3D_, 2024, pp. 1–17.\n",
      "\n",
      "[64] G. Fang and B. Wang, “Mini-splatting: Representing scenes\n",
      "with a constrained number of gaussians,” _arXiv_ _preprint_\n",
      "_arXiv:2403.14166_, 2024.\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 17\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[65] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, “Mipsplatting: Alias-free 3d gaussian splatting,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024, pp. 19 447–19 456.\n",
      "\n",
      "[66] J. Gao, C. Gu, Y. Lin, H. Zhu, X. Cao, L. Zhang, and Y. Yao, “Relightable 3d gaussian: Real-time point cloud relighting with brdf\n",
      "decomposition and ray tracing,” _arXiv preprint arXiv:2311.16043_,\n",
      "2023.\n",
      "\n",
      "[67] Z. Yan, W. F. Low, Y. Chen, and G. H. Lee, “Multi-scale 3d\n",
      "gaussian splatting for anti-aliased rendering,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[68] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma,\n",
      "“Gaussianshader: 3d gaussian splatting with shading functions\n",
      "for reflective surfaces,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[69] B. Lee, H. Lee, X. Sun, U. Ali, and E. Park, “Deblurring 3d\n",
      "gaussian splatting,” _arXiv preprint arXiv:2401.00834_, 2024.\n",
      "\n",
      "[70] D. Malarz, W. Smolak, J. Tabor, S. Tadeja, and P. Spurek, “Gaussian splitting algorithm with color and opacity depended on\n",
      "viewing direction,” _arXiv preprint arXiv:2312.13729_, 2023.\n",
      "\n",
      "[71] L. Bolanos, S.-Y. Su, and H. Rhodin, “Gaussian shadow casting\n",
      "for neural characters,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[72] L. Radl, M. Steiner, M. Parger, A. Weinrauch, B. Kerbl, and\n",
      "M. Steinberger, “Stopthepop: Sorted gaussian splatting for viewconsistent real-time rendering,” _ACM Trans. Graph._, 2024.\n",
      "\n",
      "[73] Z. Yang, X. Gao, Y. Sun, Y. Huang, X. Lyu, W. Zhou, S. Jiao, X. Qi,\n",
      "and X. Jin, “Spec-gaussian: Anisotropic view-dependent appearance for 3d gaussian splatting,” _arXiv preprint arXiv:2402.15870_,\n",
      "2024.\n",
      "\n",
      "[74] C. Peng, Y. Tang, Y. Zhou, N. Wang, X. Liu, D. Li, and R. Chellappa, “Bags: Blur agnostic gaussian splatting through multiscale kernel modeling,” _arXiv preprint arXiv:2403.04926_, 2024.\n",
      "\n",
      "[75] L. Zhao, P. Wang, and P. Liu, “Bad-gaussians: Bundle adjusted\n",
      "deblur gaussian splatting,” _arXiv preprint arXiv:2403.11831_, 2024.\n",
      "\n",
      "[76] H. Dahmani, M. Bennehar, N. Piasco, L. Roldao, and D. Tsishkou,\n",
      "“Swag: Splatting in the wild images with appearanceconditioned gaussians,” _arXiv preprint arXiv:2403.10427_, 2024.\n",
      "\n",
      "[77] Y. Li, C. Lyu, Y. Di, G. Zhai, G. H. Lee, and F. Tombari, “Geogaussian: Geometry-aware gaussian splatting for scene rendering,”\n",
      "_arXiv preprint arXiv:2403.11324_, 2024.\n",
      "\n",
      "[78] Z. Liang, Q. Zhang, W. Hu, Y. Feng, L. Zhu, and K. Jia, “Analyticsplatting: Anti-aliased 3d gaussian splatting via analytic integration,” _arXiv preprint arXiv:2403.11056_, 2024.\n",
      "\n",
      "[79] O. Seiskari, J. Ylilammi, V. Kaatrasalo, P. Rantalankila, M. Turkulainen, J. Kannala, E. Rahtu, and A. Solin, “Gaussian splatting\n",
      "on the move: Blur and rolling shutter compensation for natural\n",
      "camera motion,” _arXiv preprint arXiv:2403.13327_, 2024.\n",
      "\n",
      "[80] X. Song, J. Zheng, S. Yuan, H.-a. Gao, J. Zhao, X. He, W. Gu, and\n",
      "H. Zhao, “Sa-gs: Scale-adaptive gaussian splatting for trainingfree anti-aliasing,” _arXiv preprint arXiv:2403.19615_, 2024.\n",
      "\n",
      "[81] Y. Fu, S. Liu, A. Kulkarni, J. Kautz, A. A. Efros, and X. Wang,\n",
      "“Colmap-free 3d gaussian splatting,” in _Proc. IEEE Conf. Comput._\n",
      "_Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[82] J. Jung, J. Han, H. An, J. Kang, S. Park, and S. Kim, “Relaxing\n",
      "accurate initialization constraint for 3d gaussian splatting,” _arXiv_\n",
      "_preprint arXiv:2403.09413_, 2024.\n",
      "\n",
      "[83] M. Yu, T. Lu, L. Xu, L. Jiang, Y. Xiangli, and B. Dai, “Gsdf: 3dgs\n",
      "meets sdf for improved rendering and reconstruction,” _arXiv_\n",
      "_preprint arXiv:2403.16964_, 2024.\n",
      "\n",
      "[84] J. Zhang, F. Zhan, M. Xu, S. Lu, and E. Xing, “Fregs: 3d gaussian\n",
      "splatting with progressive frequency regularization,” in _Proc._\n",
      "_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[85] L. Huang, J. Bai, J. Guo, and Y. Guo, “Gs++: Error analyzing and\n",
      "optimal gaussian splatting,” _arXiv preprint arXiv:2402.00752_, 2024.\n",
      "\n",
      "[86] J. Li, L. Cheng, Z. Wang, T. Mu, and J. He, “Loopgaussian:\n",
      "Creating 3d cinemagraph with multi-view images via eulerian\n",
      "motion field,” _arXiv preprint arXiv:2404.08966_, 2024.\n",
      "\n",
      "[87] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language embedded 3d gaussians for open-vocabulary scene understanding,”\n",
      "in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[88] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, “Langsplat: 3d\n",
      "language gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis._\n",
      "_Pattern Recognit._, 2024.\n",
      "\n",
      "[89] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, “Fmgs:\n",
      "Foundation model embedded 3d gaussian splatting for holistic\n",
      "3d scene understanding,” _arXiv preprint arXiv:2401.01970_, 2024.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari,\n",
      "S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging\n",
      "3d gaussian splatting to enable distilled feature fields,” in _Proc._\n",
      "_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping:\n",
      "Segment and edit anything in 3d scenes,” in _Proc. Eur. Conf._\n",
      "_Comput. Vis._, 2024.\n",
      "\n",
      "[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. Tian,\n",
      "“Segment any 3d gaussians,” _arXiv preprint arXiv:2312.00860_,\n",
      "2023.\n",
      "\n",
      "[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time\n",
      "photorealistic dynamic scene representation and rendering with\n",
      "4d gaussian splatting,” in _Proc. Int. Conf. Learn. Represent._, 2024.\n",
      "\n",
      "[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, “Deformable 3d gaussians for high-fidelity monocular dynamic scene\n",
      "reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2024.\n",
      "\n",
      "[95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian,\n",
      "and X. Wang, “4d gaussian splatting for real-time dynamic scene\n",
      "rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2024.\n",
      "\n",
      "[96] Y. Xu, B. Chen, Z. Li, H. Zhang, L. Wang, Z. Zheng, and\n",
      "Y. Liu, “Gaussian head avatar: Ultra high-fidelity head avatar\n",
      "via dynamic gaussians,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[97] S. Szymanowicz, E. Insafutdinov, C. Zheng, D. Campbell, J. F.\n",
      "Henriques, C. Rupprecht, and A. Vedaldi, “Flash3d: Feedforward generalisable 3d scene reconstruction from a single\n",
      "image,” _arXiv preprint arXiv:2406.04343_, 2024.\n",
      "\n",
      "[98] K. Sargent, Z. Li, T. Shah, C. Herrmann, H.-X. Yu, Y. Zhang, E. R.\n",
      "Chan, D. Lagun, L. Fei-Fei, D. Sun _et al._, “Zeronvs: Zero-shot 360degree view synthesis from a single image,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024, pp. 9420–9429.\n",
      "\n",
      "[99] J. Meng, H. Li, Y. Wu, Q. Gao, S. Yang, J. Zhang, and S. Ma,\n",
      "“Mirror-3dgs: Incorporating mirror reflections into 3d gaussian\n",
      "splatting,” _arXiv preprint arXiv:2404.01168_, 2024.\n",
      "\n",
      "[100] H. Chen, C. Li, and G. H. Lee, “Neusg: Neural implicit surface reconstruction with 3d gaussian splatting guidance,” _arXiv preprint_\n",
      "_arXiv:2312.00846_, 2023.\n",
      "\n",
      "[101] Z. Yu, T. Sattler, and A. Geiger, “Gaussian opacity fields: Efficient\n",
      "and compact surface reconstruction in unbounded scenes,” _arXiv_\n",
      "_preprint arXiv:2404.10772_, 2024.\n",
      "\n",
      "[102] B. Zhang, C. Fang, R. Shrestha, Y. Liang, X. Long, and P. Tan,\n",
      "“Rade-gs: Rasterizing depth in gaussian splatting,” _arXiv preprint_\n",
      "_arXiv:2406.01467_, 2024.\n",
      "\n",
      "[103] A. Chen, H. Xu, S. Esposito, S. Tang, and A. Geiger,\n",
      "“Lara: Efficient large-baseline radiance fields,” _arXiv preprint_\n",
      "_arXiv:2407.04699_, 2024.\n",
      "\n",
      "[104] E. Ververas, R. A. Potamias, J. Song, J. Deng, and S. Zafeiriou,\n",
      "“Sags: Structure-aware 3d gaussian splatting,” in _Proc. Eur. Conf._\n",
      "_Comput. Vis._, 2024, pp. 221–238.\n",
      "\n",
      "[105] C. Smith, D. Charatan, A. Tewari, and V. Sitzmann, “Flowmap:\n",
      "High-quality camera poses, intrinsics, and depth via gradient\n",
      "descent,” _arXiv preprint arXiv:2404.15259_, 2024.\n",
      "\n",
      "[106] Y. Lin, Z. Dai, S. Zhu, and Y. Yao, “Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[107] A. Saroha, M. Gladkova, C. Curreli, T. Yenamandra, and\n",
      "D. Cremers, “Gaussian splatting in style,” _arXiv_ _preprint_\n",
      "_arXiv:2403.08498_, 2024.\n",
      "\n",
      "[108] N. Moenne-Loccoz, A. Mirzaei, O. Perel, R. de Lutio, J. Martinez Esturo, G. State, S. Fidler, N. Sharp, and Z. Gojcic, “3d\n",
      "gaussian ray tracing: Fast tracing of particle scenes,” _ACM Trans._\n",
      "_Graph._, vol. 43, no. 6, pp. 1–19, 2024.\n",
      "\n",
      "[109] A. Mai, P. Hedman, G. Kopanas, D. Verbin, D. Futschik, Q. Xu,\n",
      "F. Kuester, J. T. Barron, and Y. Zhang, “Ever: Exact volumetric\n",
      "ellipsoid rendering for real-time view synthesis,” _arXiv preprint_\n",
      "_arXiv:2410.01804_, 2024.\n",
      "\n",
      "[110] J. Condor, S. Speierer, L. Bode, A. Bozic, S. Green, P. Didyk, and\n",
      "A. Jarabo, “Don’t splat your gaussians: Volumetric ray-traced\n",
      "primitives for modeling and rendering scattering and emissive\n",
      "media,” _ACM Trans. Graph._, 2025.\n",
      "\n",
      "[111] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li,\n",
      "“Gs-slam: Dense visual slam with 3d gaussian splatting,” in _Proc._\n",
      "_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. Scherer,\n",
      "D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 18\n",
      "\n",
      "\n",
      "\n",
      "gaussians for dense rgb-d slam,” in _Proc. IEEE Conf. Comput. Vis._\n",
      "_Pattern Recognit._, 2024.\n",
      "\n",
      "[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, “Gaussian\n",
      "splatting slam,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2024.\n",
      "\n",
      "[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, “Gaussian-slam:\n",
      "Photo-realistic dense slam with gaussian splatting,” _arXiv preprint_\n",
      "_arXiv:2312.10070_, 2023.\n",
      "\n",
      "[115] H. Huang, L. Li, H. Cheng, and S.-K. Yeung, “Photo-slam: Realtime simultaneous localization and photorealistic mapping for\n",
      "monocular, stereo, and rgb-d cameras,” in _Proc. IEEE Conf. Com-_\n",
      "_put. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[116] M. Li, S. Liu, and H. Zhou, “Sgs-slam: Semantic gaussian splatting for neural dense slam,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[117] Y. Ji, Y. Liu, G. Xie, B. Ma, and Z. Xie, “Neds-slam: A novel\n",
      "neural explicit dense semantic slam framework using 3d gaussian\n",
      "splatting,” _arXiv preprint arXiv:2403.11679_, 2024.\n",
      "\n",
      "[118] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, “Manigaussian: Dynamic gaussian splatting for multi-task robotic manipulation,” _arXiv preprint arXiv:2403.08321_, 2024.\n",
      "\n",
      "[119] J. Abou-Chakra, K. Rana, F. Dayoub, and N. S¨underhauf, “Physically embodied gaussian splatting: A realtime correctable world\n",
      "model for robotics,” in _Proc. Annu. Conf. Robot Learn._, 2024.\n",
      "\n",
      "[120] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. Firoozi,\n",
      "M. D. Kennedy, and M. Schwager, “Splat-mover: Multi-stage,\n",
      "open-vocabulary robotic manipulation via editable gaussian\n",
      "splatting,” in _Proc. Annu. Conf. Robot Learn._, 2024.\n",
      "\n",
      "[121] M. Ji, R.-Z. Qiu, X. Zou, and X. Wang, “Graspsplats: Efficient manipulation with 3d feature splatting,” _arXiv preprint_\n",
      "_arXiv:2409.02084_, 2024.\n",
      "\n",
      "[122] Y. Zheng, X. Chen, Y. Zheng, S. Gu, R. Yang, B. Jin, P. Li, C. Zhong,\n",
      "Z. Wang, L. Liu _et al._, “Gaussiangrasper: 3d language gaussian\n",
      "splatting for open-vocabulary robotic grasping,” _arXiv preprint_\n",
      "_arXiv:2403.09637_, 2024.\n",
      "\n",
      "[123] S. Zhu, R. Qin, G. Wang, J. Liu, and H. Wang, “Semgaussslam: Dense semantic gaussian splatting slam,” _arXiv preprint_\n",
      "_arXiv:2403.07494_, 2024.\n",
      "\n",
      "[124] Z. Peng, T. Shao, Y. Liu, J. Zhou, Y. Yang, J. Wang, and K. Zhou,\n",
      "“Rtg-slam: Real-time 3d reconstruction at scale using gaussian\n",
      "splatting,” _ACM Trans. Graph._, 2024.\n",
      "\n",
      "[125] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, “Dynamic 3d\n",
      "gaussians: Tracking by persistent dynamic view synthesis,” in\n",
      "_Proc. Int. Conf. 3D Vis._, 2024.\n",
      "\n",
      "[126] H. Yu, J. Julin, Z. A. Milacski, K. Niinuma, and L. A. Jeni, “Cogs: [´]\n",
      "Controllable gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis._\n",
      "_Pattern Recognit._, 2024.\n",
      "\n",
      "[127] R. Shao, J. Sun, C. Peng, Z. Zheng, B. Zhou, H. Zhang, and Y. Liu,\n",
      "“Control4d: Efficient 4d portrait editing with text,” in _Proc. IEEE_\n",
      "_Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[128] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi,\n",
      "“Sc-gs: Sparse-controlled gaussian splatting for editable dynamic\n",
      "scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[129] D. Das, C. Wewer, R. Yunus, E. Ilg, and J. E. Lenssen, “Neural\n",
      "parametric gaussians for monocular non-rigid object reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[130] Z. Li, Z. Chen, Z. Li, and Y. Xu, “Spacetime gaussian feature\n",
      "splatting for real-time dynamic view synthesis,” in _Proc. IEEE_\n",
      "_Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[131] J. Sun, H. Jiao, G. Li, Z. Zhang, L. Zhao, and W. Xing, “3dgstream:\n",
      "On-the-fly training of 3d gaussians for efficient streaming of\n",
      "photo-realistic free-viewpoint videos,” in _Proc. IEEE Conf. Com-_\n",
      "_put. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[132] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, and\n",
      "Y. Dai, “3d geometry-aware deformable gaussian splatting for\n",
      "dynamic view synthesis,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[133] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, “Dreamgaussian:\n",
      "Generative gaussian splatting for efficient 3d content creation,”\n",
      "in _Proc. Int. Conf. Learn. Represent._, 2024.\n",
      "\n",
      "[134] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian,\n",
      "and X. Wang, “Gaussiandreamer: Fast generation from text to 3d\n",
      "gaussians by bridging 2d and 3d diffusion models,” in _Proc. IEEE_\n",
      "_Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[135] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, “Lgm:\n",
      "Large multi-view gaussian model for high-resolution 3d content\n",
      "creation,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[136] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. You,\n",
      "Z. Wang, and A. Kadambi, “Dreamscene360: Unconstrained textto-3d scene generation with panoramic gaussian splatting,” in\n",
      "_Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[137] Z. Li, Y. Chen, L. Zhao, and P. Liu, “Controllable text-to-3d\n",
      "generation via surface-aligned gaussian splatting,” _arXiv preprint_\n",
      "_arXiv:2403.09981_, 2024.\n",
      "\n",
      "[138] Y. Mu, X. Zuo, C. Guo, Y. Wang, J. Lu, X. Wu, S. Xu, P. Dai, Y. Yan,\n",
      "and L. Cheng, “Gsd: View-guided gaussian splatting diffusion\n",
      "for 3d reconstruction,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[139] Y. Jiang, Z. Shen, P. Wang, Z. Su, Y. Hong, Y. Zhang, J. Yu, and\n",
      "L. Xu, “Hifi4g: High-fidelity human performance rendering via\n",
      "compact gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis._\n",
      "_Pattern Recognit._, 2024.\n",
      "\n",
      "[140] Y. Wang, Q. Wu, G. Zhang, and D. Xu, “Gscream: Learning 3d\n",
      "geometry and feature consistent gaussian splatting for object\n",
      "removal,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[141] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. Cai,\n",
      "L. Yang, H. Liu, and G. Lin, “Gaussianeditor: Swift and controllable 3d editing with gaussian splatting,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[142] J. Fang, J. Wang, X. Zhang, L. Xie, and Q. Tian, “Gaussianeditor:\n",
      "Editing 3d gaussians delicately with text instructions,” in _Proc._\n",
      "_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[143] R.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, “Feature splatting:\n",
      "Language-driven physics-based scene synthesis and editing,”\n",
      "_arXiv preprint arXiv:2404.01223_, 2024.\n",
      "\n",
      "[144] Z. Li, Z. Zheng, L. Wang, and Y. Liu, “Animatable gaussians:\n",
      "Learning pose-dependent gaussian maps for high-fidelity human\n",
      "avatar modeling,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recog-_\n",
      "_nit._, 2024.\n",
      "\n",
      "[145] S. Hu and Z. Liu, “Gauhuman: Articulated gaussian splatting\n",
      "from monocular human videos,” in _Proc. IEEE Conf. Comput. Vis._\n",
      "_Pattern Recognit._, 2024.\n",
      "\n",
      "[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, “Gart:\n",
      "Gaussian articulated template models,” in _Proc. IEEE Conf. Com-_\n",
      "_put. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[147] Y. Yuan, X. Li, Y. Huang, S. De Mello, K. Nagano, J. Kautz, and\n",
      "U. Iqbal, “Gavatar: Animatable 3d gaussian avatars with implicit\n",
      "mesh learning,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2024.\n",
      "\n",
      "[148] Z. Zhou, F. Ma, H. Fan, and Y. Yang, “Headstudio: Text to\n",
      "animatable head avatars with 3d gaussian splatting,” in _Proc. Eur._\n",
      "_Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[149] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain,\n",
      "and M. Nießner, “Gaussianavatars: Photorealistic head avatars\n",
      "with rigged 3d gaussians,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[150] H. Dhamo, Y. Nie, A. Moreau, J. Song, R. Shaw, Y. Zhou, and\n",
      "E. P´erez-Pellitero, “Headgas: Real-time animatable head avatars\n",
      "via 3d gaussian splatting,” _arXiv preprint arXiv:2312.02902_, 2023.\n",
      "\n",
      "[151] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. Gu,\n",
      "“Talkinggaussian: Structure-persistent 3d talking head synthesis\n",
      "via gaussian splatting,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[152] Y. Huang, B. Cui, L. Bai, Z. Guo, M. Xu, and H. Ren, “Endo4dgs: Distilling depth ranking for endoscopic monocular scene\n",
      "reconstruction with 4d gaussian splatting,” in _Proc. Int. Conf. Med._\n",
      "_Image Comput. Comput. Assist. Interv._, 2024.\n",
      "\n",
      "[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian\n",
      "splatting for deformable surgical scene reconstruction,” _arXiv_\n",
      "_preprint arXiv:2401.12561_, 2024.\n",
      "\n",
      "[154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, “Deformable endoscopic tissues reconstruction with gaussian splatting,” _arXiv_\n",
      "_preprint arXiv:2401.11535_, 2024.\n",
      "\n",
      "[155] H. Zhao, X. Zhao, L. Zhu, W. Zheng, and Y. Xu, “Hfgs: 4d\n",
      "gaussian splatting with emphasis on spatial and temporal highfrequency components for endoscopic scene reconstruction,”\n",
      "_arXiv preprint arXiv:2405.17872_, 2024.\n",
      "\n",
      "[156] K. Wang, C. Yang, Y. Wang, S. Li, Y. Wang, Q. Dou, X. Yang,\n",
      "and W. Shen, “Endogslam: Real-time dense reconstruction and\n",
      "tracking in endoscopic surgeries using gaussian splatting,” _arXiv_\n",
      "_preprint arXiv:2403.15124_, 2024.\n",
      "\n",
      "[157] S. Bonilla, S. Zhang, D. Psychogyios, D. Stoyanov, F. Vasconcelos,\n",
      "and S. Bano, “Gaussian pancakes: Geometrically-regularized 3d\n",
      "gaussian splatting for realistic endoscopic reconstruction,” _arXiv_\n",
      "_preprint arXiv:2404.06128_, 2024.\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 19\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[158] K. Wu, K. Zhang, Z. Zhang, S. Yuan, M. Tie, J. Wei, Z. Xu, J. Zhao,\n",
      "Z. Gan, and W. Ding, “Hgs-mapping: Online dense mapping\n",
      "using hybrid gaussian representation in urban scenes,” _arXiv_\n",
      "_preprint arXiv:2403.20159_, 2024.\n",
      "\n",
      "[159] C. Wu, Y. Duan, X. Zhang, Y. Sheng, J. Ji, and Y. Zhang, “Mmgaussian: 3d gaussian-based multi-modal fusion for localization\n",
      "and reconstruction in unbounded scenes,” in _Proc. IEEE/RSJ Int._\n",
      "_Conf. Intell. Robot. Syst._, 2024.\n",
      "\n",
      "[160] B. Xiong, Z. Li, and Z. Li, “Gauu-scene: A scene reconstruction\n",
      "benchmark on large scale 3d reconstruction dataset using gaussian splatting,” _arXiv preprint arXiv:2401.14032_, 2024.\n",
      "\n",
      "[161] H. Zhao, H. Weng, D. Lu, A. Li, J. Li, A. Panda, and S. Xie,\n",
      "“On scaling up 3d gaussian splatting training,” _arXiv preprint_\n",
      "_arXiv:2406.18533_, 2024.\n",
      "\n",
      "[162] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and\n",
      "G. Drettakis, “A hierarchical 3d gaussian representation for realtime rendering of very large datasets,” _ACM Trans. Graph._, vol. 44,\n",
      "no. 3, 2024.\n",
      "\n",
      "[163] Y. Liu, H. Guan, C. Luo, L. Fan, J. Peng, and Z. Zhang, “Citygaussian: Real-time high-quality large-scale scene rendering with\n",
      "gaussians,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[164] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, S. Xu,\n",
      "Y. Yan _et al._, “Vastgaussian: Vast 3d gaussians for large scene\n",
      "reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2024.\n",
      "\n",
      "[165] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, “Octreegs: Towards consistent real-time rendering with lod-structured\n",
      "3d gaussians,” _arXiv preprint arXiv:2403.17898_, 2024.\n",
      "\n",
      "[166] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. Jiang,\n",
      "“Physgaussian: Physics-integrated 3d gaussians for generative\n",
      "dynamics,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[167] F. Liu, H. Wang, S. Yao, S. Zhang, J. Zhou, and Y. Duan,\n",
      "“Physics3d: Learning physical properties of 3d gaussians via\n",
      "video diffusion,” _arXiv preprint arXiv:2406.04338_, 2024.\n",
      "\n",
      "[168] P. Borycki, W. Smolak, J. Waczy´nska, M. Mazur, S. Tadeja, and\n",
      "P. Spurek, “Gasp: Gaussian splatting for physic-based simulations,” _arXiv preprint arXiv:2409.05819_, 2024.\n",
      "\n",
      "[169] T. Huang, Y. Zeng, H. Li, W. Zuo, and R. W. Lau, “Dreamphysics:\n",
      "Learning physical properties of dynamic 3d gaussians with video\n",
      "diffusion priors,” in _Proc. AAAI Conf. Artif. Intell._, 2025.\n",
      "\n",
      "[170] T. Zhang, H.-X. Yu, R. Wu, B. Y. Feng, C. Zheng, N. Snavely, J. Wu,\n",
      "and W. T. Freeman, “Physdreamer: Physics-based interaction\n",
      "with 3d objects via video generation,” in _Proc. Eur. Conf. Comput._\n",
      "_Vis._, 2024, pp. 388–406.\n",
      "\n",
      "[171] Y. Feng, X. Feng, Y. Shang, Y. Jiang, C. Yu, Z. Zong, T. Shao,\n",
      "H. Wu, K. Zhou, C. Jiang _et al._, “Gaussian splashing: Dynamic fluid synthesis with gaussian splatting,” _arXiv preprint_\n",
      "_arXiv:2401.15318_, 2024.\n",
      "\n",
      "[172] L. Zhong, H.-X. Yu, J. Wu, and Y. Li, “Reconstruction and simulation of elastic objects with spring-mass 3d gaussians,” in _Proc._\n",
      "_Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[173] Y. Shao, M. Huang, C. C. Loy, and B. Dai, “Gausim: Registering\n",
      "elastic objects into digital world by gaussian simulator,” _arXiv_\n",
      "_preprint arXiv:2412.17804_, 2024.\n",
      "\n",
      "[174] S. Zhang, H. Zhao, Z. Zhou, G. Wu, C. Zheng, X. Wang, and\n",
      "W. Liu, “Togs: Gaussian splatting with temporal opacity offset\n",
      "for real-time 4d dsa rendering,” _arXiv preprint arXiv:2403.19586_,\n",
      "2024.\n",
      "\n",
      "[175] R. Wu, Z. Zhang, Y. Yang, and W. Zuo, “Dual-camera smooth\n",
      "zoom on mobile phones,” _arXiv preprint arXiv:2404.04908_, 2024.\n",
      "\n",
      "[176] H. Li, Y. Gao, D. Zhang, C. Wu, Y. Dai, C. Zhao, H. Feng, E. Ding,\n",
      "J. Wang, and J. Han, “Ggrt: Towards generalizable 3d gaussians\n",
      "without pose priors in real-time,” _arXiv preprint arXiv:2403.10147_,\n",
      "2024.\n",
      "\n",
      "[177] S. Hong, J. He, X. Zheng, H. Wang, H. Fang, K. Liu, C. Zheng, and\n",
      "S. Shen, “Liv-gaussmap: Lidar-inertial-visual fusion for real-time\n",
      "3d radiance field map rendering,” _arXiv preprint arXiv:2401.14857_,\n",
      "2024.\n",
      "\n",
      "[178] S. Sun, M. Mielle, A. J. Lilienthal, and M. Magnusson, “Highfidelity slam using gaussian splatting with rendering-guided\n",
      "densification and regularized optimization,” in _Proc. IEEE/RSJ_\n",
      "_Int. Conf. Intell. Robot. Syst._, 2024.\n",
      "\n",
      "[179] F. Tosi, Y. Zhang, Z. Gong, E. Sandstr¨om, S. Mattoccia, M. R.\n",
      "Oswald, and M. Poggi, “How nerfs and 3d gaussian splatting are\n",
      "reshaping slam: a survey,” _arXiv preprint arXiv:2402.13255_, 2024.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[180] T. Deng, Y. Chen, L. Zhang, J. Yang, S. Yuan, D. Wang, and\n",
      "W. Chen, “Compact 3d gaussian splatting for dense visual slam,”\n",
      "_arXiv preprint arXiv:2403.11247_, 2024.\n",
      "\n",
      "[181] J. Hu, X. Chen, B. Feng, G. Li, L. Yang, H. Bao, G. Zhang,\n",
      "and Z. Cui, “Cg-slam: Efficient dense rgb-d slam in a consistent uncertainty-aware 3d gaussian field,” _arXiv preprint_\n",
      "_arXiv:2403.16095_, 2024.\n",
      "\n",
      "[182] X. Lang, L. Li, H. Zhang, F. Xiong, M. Xu, Y. Liu, X. Zuo, and J. Lv,\n",
      "“Gaussian-lic: Photo-realistic lidar-inertial-camera slam with 3d\n",
      "gaussian splatting,” _arXiv preprint arXiv:2404.06926_, 2024.\n",
      "\n",
      "[183] E. Sandstr¨om, K. Tateno, M. Oechsle, M. Niemeyer, L. Van Gool,\n",
      "M. R. Oswald, and F. Tombari, “Splat-slam: Globally optimized rgb-only slam with 3d gaussians,” _arXiv_ _preprint_\n",
      "_arXiv:2405.16544_, 2024.\n",
      "\n",
      "[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer,\n",
      "“D-nerf: Neural radiance fields for dynamic scenes,” in _Proc. IEEE_\n",
      "_Conf. Comput. Vis. Pattern Recognit._, 2021, pp. 10 318–10 327.\n",
      "\n",
      "[185] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B.\n",
      "Goldman, R. Martin-Brualla, and S. M. Seitz, “Hypernerf: a\n",
      "higher-dimensional representation for topologically varying neural radiance fields,” _ACM Trans. Graph._, vol. 40, no. 6, pp. 1–12,\n",
      "2021.\n",
      "\n",
      "[186] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman,\n",
      "S. M. Seitz, and R. Martin-Brualla, “Nerfies: Deformable neural\n",
      "radiance fields,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2021, pp.\n",
      "5865–5874.\n",
      "\n",
      "[187] X. Guo, J. Sun, Y. Dai, G. Chen, X. Ye, X. Tan, E. Ding, Y. Zhang,\n",
      "and J. Wang, “Forward flow for novel view synthesis of dynamic\n",
      "scenes,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2023, pp. 16 022–\n",
      "16 033.\n",
      "\n",
      "[188] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.-H. Yang,\n",
      "“Drivinggaussian: Composite gaussian splatting for surrounding\n",
      "dynamic autonomous driving scenes,” in _Proc. IEEE Conf. Com-_\n",
      "_put. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[189] Y. Yan, H. Lin, C. Zhou, W. Wang, H. Sun, K. Zhan, X. Lang,\n",
      "X. Zhou, and S. Peng, “Street gaussians for modeling dynamic\n",
      "urban scenes,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[190] H. Zhou, J. Shao, L. Xu, D. Bai, W. Qiu, B. Liu, Y. Wang, A. Geiger,\n",
      "and Y. Liao, “Hugs: Holistic urban 3d scene understanding\n",
      "via gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024, pp. 21 336–21 345.\n",
      "\n",
      "[191] A. Kratimenos, J. Lei, and K. Daniilidis, “Dynmf: Neural motion\n",
      "factorization for real-time dynamic view synthesis with 3d gaussian splatting,” _arXiv preprint arXiv:2312.00112_, 2023.\n",
      "\n",
      "[192] R. Shaw, J. Song, A. Moreau, M. Nazarczuk, S. Catley-Chandar,\n",
      "H. Dhamo, and E. Perez-Pellitero, “Swags: Sampling windows\n",
      "adaptively for dynamic 3d gaussian splatting,” _arXiv preprint_\n",
      "_arXiv:2312.13308_, 2023.\n",
      "\n",
      "[193] Y. Liang, N. Khan, Z. Li, T. Nguyen-Phuoc, D. Lanman,\n",
      "J. Tompkin, and L. Xiao, “Gaufre: Gaussian deformation fields\n",
      "for real-time dynamic novel view synthesis,” _arXiv preprint_\n",
      "_arXiv:2312.11458_, 2023.\n",
      "\n",
      "[194] K. Katsumata, D. M. Vo, and H. Nakayama, “An efficient 3d gaussian representation for monocular/multi-view dynamic scenes,”\n",
      "_arXiv preprint arXiv:2311.12897_, 2023.\n",
      "\n",
      "[195] Z. Guo, W. Zhou, L. Li, M. Wang, and H. Li, “Motion-aware 3d\n",
      "gaussian splatting for efficient dynamic scene reconstruction,”\n",
      "_arXiv preprint arXiv:2403.11447_, 2024.\n",
      "\n",
      "[196] J. Bae, S. Kim, Y. Yun, H. Lee, G. Bang, and Y. Uh, “Per-gaussian\n",
      "embedding-based deformation for deformable 3d gaussian splatting,” _arXiv preprint arXiv:2404.03613_, 2024.\n",
      "\n",
      "[197] J. Lei, Y. Weng, A. Harley, L. Guibas, and K. Daniilidis, “Mosca:\n",
      "Dynamic gaussian fusion from casual videos via 4d motion\n",
      "scaffolds,” _arXiv preprint arXiv:2405.17421_, 2024.\n",
      "\n",
      "[198] Q. Wang, V. Ye, H. Gao, J. Austin, Z. Li, and A. Kanazawa, “Shape\n",
      "of motion: 4d reconstruction from a single video,” _arXiv preprint_\n",
      "_arXiv:2407.13764_, 2024.\n",
      "\n",
      "[199] Y. Duan, F. Wei, Q. Dai, Y. He, W. Chen, and B. Chen, “4drotor gaussian splatting: towards efficient novel view synthesis\n",
      "for dynamic scenes,” in _Proc. ACM Spec. Interest Group Comput._\n",
      "_Graph. Interact. Tech._, 2024, pp. 1–11.\n",
      "\n",
      "[200] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial networks,” _Communications of the ACM_, vol. 63, no. 11, pp.\n",
      "139–144, 2020.\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 20\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[201] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic\n",
      "models,” in _Proc. Adv. Neural Inf. Process. Syst._, 2020, pp. 6840–\n",
      "6851.\n",
      "\n",
      "[202] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n",
      "“High-resolution image synthesis with latent diffusion models,”\n",
      "in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 10 684–\n",
      "10 695.\n",
      "\n",
      "[203] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in _Proc. IEEE Int. Conf._\n",
      "_Comput. Vis._, 2023, pp. 3836–3847.\n",
      "\n",
      "[204] Z. Chen, F. Wang, and H. Liu, “Text-to-3d using gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[205] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, “Luciddreamer: Towards high-fidelity text-to-3d generation via interval\n",
      "score matching,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2024.\n",
      "\n",
      "[206] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu,\n",
      "and Z. Liu, “Humangaussian: Text-driven 3d human generation\n",
      "with gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[207] X. Yang, Y. Chen, C. Chen, C. Zhang, Y. Xu, X. Yang, F. Liu, and\n",
      "G. Lin, “Learn to optimize denoising scores for 3d generation: A\n",
      "unified and improved diffusion prior on nerf and 3d gaussian\n",
      "splatting,” _arXiv preprint arXiv:2312.04820_, 2023.\n",
      "\n",
      "[208] Z.-X. Zou, Z. Yu, Y.-C. Guo, Y. Li, D. Liang, Y.-P. Cao, and S.-H.\n",
      "Zhang, “Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers,” in _Proc._\n",
      "_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[209] H. Ling, S. W. Kim, A. Torralba, S. Fidler, and K. Kreis, “Align\n",
      "your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[210] J. Ren, L. Pan, J. Tang, C. Zhang, A. Cao, G. Zeng, and Z. Liu,\n",
      "“Dreamgaussian4d: Generative 4d gaussian splatting,” _arXiv_\n",
      "_preprint arXiv:2312.17142_, 2023.\n",
      "\n",
      "[211] Y. Yin, D. Xu, Z. Wang, Y. Zhao, and Y. Wei, “4dgen: Grounded\n",
      "4d content generation with spatial-temporal consistency,” _arXiv_\n",
      "_preprint arXiv:2312.17225_, 2023.\n",
      "\n",
      "[212] J. Zhang, Z. Tang, Y. Pang, X. Cheng, P. Jin, Y. Wei, W. Yu,\n",
      "M. Ning, and L. Yuan, “Repaint123: Fast and high-quality one\n",
      "image to 3d generation with progressive controllable 2d repainting,” _arXiv preprint arXiv:2312.13271_, 2023.\n",
      "\n",
      "[213] Z. Pan, Z. Yang, X. Zhu, and L. Zhang, “Fast dynamic 3d\n",
      "object generation from a single-view video,” _arXiv preprint_\n",
      "_arXiv:2401.08742_, 2024.\n",
      "\n",
      "[214] D. Xu, Y. Yuan, M. Mardani, S. Liu, J. Song, Z. Wang, and\n",
      "A. Vahdat, “Agg: Amortized generative 3d gaussians for single\n",
      "image to 3d,” _arXiv preprint arXiv:2401.04099_, 2024.\n",
      "\n",
      "[215] C. Yang, S. Li, J. Fang, R. Liang, L. Xie, X. Zhang, W. Shen,\n",
      "and Q. Tian, “Gaussianobject: Just taking four images to get a\n",
      "high-quality 3d object with gaussian splatting,” _arXiv preprint_\n",
      "_arXiv:2402.10259_, 2024.\n",
      "\n",
      "[216] F. Barthel, A. Beckmann, W. Morgenstern, A. Hilsmann, and\n",
      "P. Eisert, “Gaussian splatting decoder for 3d-aware generative\n",
      "adversarial networks,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit. Worksh._, 2024.\n",
      "\n",
      "[217] L. Jiang and L. Wang, “Brightdreamer: Generic 3d gaussian\n",
      "generative framework for fast text-to-3d synthesis,” _arXiv preprint_\n",
      "_arXiv:2403.11273_, 2024.\n",
      "\n",
      "[218] W. Zhuo, F. Ma, H. Fan, and Y. Yang, “Vividdreamer: Invariant\n",
      "score distillation for hyper-realistic text-to-3d generation,” in\n",
      "_Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[219] Z. Wu, C. Yu, Y. Jiang, C. Cao, F. Wang, and X. Bai, “Sc4d:\n",
      "Sparse-controlled video-to-4d generation and motion transfer,”\n",
      "_arXiv preprint arXiv:2404.03736_, 2024.\n",
      "\n",
      "[220] X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan,\n",
      "W. Ouyang, and T. He, “Gvgen: Text-to-3d generation with\n",
      "volumetric representation,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[221] X. Yang and X. Wang, “Hash3d: Training-free acceleration for 3d\n",
      "generation,” _arXiv preprint arXiv:2404.06091_, 2024.\n",
      "\n",
      "[222] J. Kim, J. Koo, K. Yeo, and M. Sung, “Synctweedies: A general\n",
      "generative framework based on synchronized diffusions,” _arXiv_\n",
      "_preprint arXiv:2403.14370_, 2024.\n",
      "\n",
      "[223] Q. Feng, Z. Xing, Z. Wu, and Y.-G. Jiang, “Fdgaussian: Fast gaussian splatting from single image via geometric-aware diffusion\n",
      "model,” _arXiv preprint arXiv:2403.10242_, 2024.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[224] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L.-h.\n",
      "Lee, and P. Zhou, “Dreamscene: 3d gaussian-based text-to-3d\n",
      "scene generation via formation pattern sampling,” _arXiv preprint_\n",
      "_arXiv:2404.03575_, 2024.\n",
      "\n",
      "[225] L. Melas-Kyriazi, I. Laina, C. Rupprecht, N. Neverova,\n",
      "A. Vedaldi, O. Gafni, and F. Kokkinos, “Im-3d: Iterative multiview diffusion and reconstruction for high-quality 3d generation,” in _Proc. ACM Int. Conf. Mach. Learn._, 2024.\n",
      "\n",
      "[226] B. Zhang, Y. Cheng, J. Yang, C. Wang, F. Zhao, Y. Tang, D. Chen,\n",
      "and B. Guo, “Gaussiancube: Structuring gaussian splatting using\n",
      "optimal transport for 3d generative modeling,” _arXiv preprint_\n",
      "_arXiv:2403.19655_, 2024.\n",
      "\n",
      "[227] Y.-C. Lee, Y.-T. Chen, A. Wang, T.-H. Liao, B. Y. Feng, and\n",
      "J.-B. Huang, “Vividdream: Generating 3d scene with ambient\n",
      "dynamics,” _arXiv preprint arXiv:2405.20334_, 2024.\n",
      "\n",
      "[228] J. Huang and H. Yu, “Point’n move: Interactive scene object manipulation on gaussian splatting radiance fields,” _arXiv preprint_\n",
      "_arXiv:2311.16737_, 2023.\n",
      "\n",
      "[229] K. Lan, H. Li, H. Shi, W. Wu, Y. Liao, L. Wang, and\n",
      "P. Zhou, “2d-guided 3d gaussian segmentation,” _arXiv preprint_\n",
      "_arXiv:2312.16047_, 2023.\n",
      "\n",
      "[230] J. Zhuang, D. Kang, Y.-P. Cao, G. Li, L. Lin, and Y. Shan, “Tipeditor: An accurate 3d editor following both text-prompts and\n",
      "image-prompts,” _arXiv preprint arXiv:2401.14828_, 2024.\n",
      "\n",
      "[231] B. Dou, T. Zhang, Y. Ma, Z. Wang, and Z. Yuan, “Cosseggaussians: Compact and swift scene segmenting 3d gaussians,” _arXiv_\n",
      "_preprint arXiv:2401.05925_, 2024.\n",
      "\n",
      "[232] X. Hu, Y. Wang, L. Fan, J. Fan, J. Peng, Z. Lei, Q. Li, and\n",
      "Z. Zhang, “Semantic anything in 3d gaussians,” _arXiv preprint_\n",
      "_arXiv:2401.17857_, 2024.\n",
      "\n",
      "[233] F. Palandra, A. Sanchietti, D. Baieri, and E. Rodol`a, “Gsedit:\n",
      "Efficient text-guided editing of 3d objects via gaussian splatting,”\n",
      "_arXiv preprint arXiv:2403.05154_, 2024.\n",
      "\n",
      "[234] Q. Gu, Z. Lv, D. Frost, S. Green, J. Straub, and C. Sweeney, “Egolifter: Open-world 3d segmentation for egocentric perception,”\n",
      "_arXiv preprint arXiv:2403.18118_, 2024.\n",
      "\n",
      "[235] W. Lyu, X. Li, A. Kundu, Y.-H. Tsai, and M.-H. Yang, “Gaga:\n",
      "Group any gaussians via 3d-aware memory bank,” _arXiv preprint_\n",
      "_arXiv:2404.07977_, 2024.\n",
      "\n",
      "[236] Z. Liu, H. Ouyang, Q. Wang, K. L. Cheng, J. Xiao, K. Zhu, N. Xue,\n",
      "Y. Liu, Y. Shen, and Y. Cao, “Infusion: Inpainting 3d gaussians via\n",
      "learning depth completion from diffusion prior,” _arXiv preprint_\n",
      "_arXiv:2404.11613_, 2024.\n",
      "\n",
      "[237] D. Zhang, Z. Chen, Y.-J. Yuan, F.-L. Zhang, Z. He, S. Shan,\n",
      "and L. Gao, “Stylizedgs: Controllable stylization for 3d gaussian\n",
      "splatting,” _arXiv preprint arXiv:2404.05220_, 2024.\n",
      "\n",
      "[238] Q. Zhang, Y. Xu, C. Wang, H.-Y. Lee, G. Wetzstein, B. Zhou,\n",
      "and C. Yang, “3ditscene: Editing any scene via language-guided\n",
      "disentangled gaussian splatting,” _arXiv preprint arXiv:2405.18424_,\n",
      "2024.\n",
      "\n",
      "[239] J. Wu, J.-W. Bian, X. Li, G. Wang, I. Reid, P. Torr, and V. A.\n",
      "Prisacariu, “Gaussctrl: Multi-view consistent text-driven 3d gaussian splatting editing,” in _Proc. Eur. Conf. Comput. Vis._, 2024, pp.\n",
      "55–71.\n",
      "\n",
      "[240] Y. Wang, X. Yi, Z. Wu, N. Zhao, L. Chen, and H. Zhang, “Viewconsistent 3d editing with gaussian splatting,” in _Proc. Eur. Conf._\n",
      "_Comput. Vis._, 2024, pp. 404–420.\n",
      "\n",
      "[241] R. Jena, G. S. Iyer, S. Choudhary, B. Smith, P. Chaudhari,\n",
      "and J. Gee, “Splatarmor: Articulated gaussian splatting for animatable humans from monocular rgb videos,” _arXiv preprint_\n",
      "_arXiv:2311.10812_, 2023.\n",
      "\n",
      "[242] K. Ye, T. Shao, and K. Zhou, “Animatable 3d gaussians\n",
      "for high-fidelity synthesis of human motions,” _arXiv preprint_\n",
      "_arXiv:2311.13404_, 2023.\n",
      "\n",
      "[243] A. Moreau, J. Song, H. Dhamo, R. Shaw, Y. Zhou, and E. P´erezPellitero, “Human gaussian splatting: Real-time rendering of\n",
      "animatable avatars,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[244] M. Kocabas, J.-H. R. Chang, J. Gabriel, O. Tuzel, and A. Ranjan,\n",
      "“Hugs: Human gaussian splats,” in _Proc. IEEE Conf. Comput. Vis._\n",
      "_Pattern Recognit._, 2024.\n",
      "\n",
      "[245] R. Abdal, W. Yifan, Z. Shi, Y. Xu, R. Po, Z. Kuang, Q. Chen, D.Y. Yeung, and G. Wetzstein, “Gaussian shell maps for efficient\n",
      "3d human generation,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[246] S. Zheng, B. Zhou, R. Shao, B. Liu, S. Zhang, L. Nie, and Y. Liu,\n",
      "“Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 21\n",
      "\n",
      "\n",
      "\n",
      "for real-time human novel view synthesis,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[247] L. Hu, H. Zhang, Y. Zhang, B. Zhou, B. Liu, S. Zhang, and L. Nie,\n",
      "“Gaussianavatar: Towards realistic human avatar modeling from\n",
      "a single video via animatable 3d gaussians,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[248] H. Pang, H. Zhu, A. Kortylewski, C. Theobalt, and M. Habermann, “Ash: Animatable gaussian splats for efficient and photoreal human rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\n",
      "_Recognit._, 2024.\n",
      "\n",
      "[249] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang, “3dgsavatar: Animatable avatars via deformable 3d gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\n",
      "\n",
      "[250] H. Jung, N. Brasch, J. Song, E. Perez-Pellitero, Y. Zhou, Z. Li,\n",
      "N. Navab, and B. Busam, “Deformable 3d gaussian splatting\n",
      "for animatable human avatars,” _arXiv preprint arXiv:2312.15059_,\n",
      "2023.\n",
      "\n",
      "[251] M. Li, J. Tao, Z. Yang, and Y. Yang, “Human101: Training\n",
      "100+ fps human gaussians in 100s from 1 view,” _arXiv preprint_\n",
      "_arXiv:2312.15258_, 2023.\n",
      "\n",
      "[252] M. Li, S. Yao, Z. Xie, K. Chen, and Y.-G. Jiang, “Gaussianbody:\n",
      "Clothed human reconstruction via 3d gaussian splatting,” _arXiv_\n",
      "_preprint arXiv:2401.09720_, 2024.\n",
      "\n",
      "[253] J. Xiang, X. Gao, Y. Guo, and J. Zhang, “Flashavatar: Highfidelity digital avatar rendering at 300fps,” _arXiv preprint_\n",
      "_arXiv:2312.02214_, 2023.\n",
      "\n",
      "[254] Y. Chen, L. Wang, Q. Li, H. Xiao, S. Zhang, H. Yao, and\n",
      "Y. Liu, “Monogaussianavatar: Monocular gaussian point-based\n",
      "head avatar,” _arXiv preprint arXiv:2312.04558_, 2023.\n",
      "\n",
      "[255] Z. Zhao, Z. Bao, Q. Li, G. Qiu, and K. Liu, “Psavatar: A pointbased morphable shape model for real-time head avatar creation\n",
      "with 3d gaussian splatting,” _arXiv preprint arXiv:2401.12900_, 2024.\n",
      "\n",
      "[256] A. Rivero, S. Athar, Z. Shu, and D. Samaras, “Rig3dgs: Creating controllable portraits from casual monocular videos,” _arXiv_\n",
      "_preprint arXiv:2402.03723_, 2024.\n",
      "\n",
      "[257] H. Luo, M. Ouyang, Z. Zhao, S. Jiang, L. Zhang, Q. Zhang,\n",
      "W. Yang, L. Xu, and J. Yu, “Gaussianhair: Hair modeling and rendering with light-aware gaussians,” _arXiv preprint_\n",
      "_arXiv:2402.10483_, 2024.\n",
      "\n",
      "[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering for\n",
      "stereo 3d reconstruction of deformable tissues in robotic surgery,”\n",
      "in _Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv._, 2022,\n",
      "pp. 431–441.\n",
      "\n",
      "[259] C. Yang, K. Wang, Y. Wang, X. Yang, and W. Shen, “Neural\n",
      "lerplane representations for fast 4d reconstruction of deformable\n",
      "tissues,” in _Proc. Int. Conf. Med. Image Comput. Comput. Assist._\n",
      "_Interv._, 2023, pp. 46–56.\n",
      "\n",
      "[260] R. Zha, X. Cheng, H. Li, M. Harandi, and Z. Ge, “Endosurf:\n",
      "Neural surface reconstruction of deformable tissues with stereo\n",
      "endoscope videos,” in _Proc. Int. Conf. Med. Image Comput. Comput._\n",
      "_Assist. Interv._, 2023, pp. 13–23.\n",
      "\n",
      "[261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma _et al._, “The replica dataset: A\n",
      "digital replica of indoor spaces,” _arXiv preprint arXiv:1906.05797_,\n",
      "2019.\n",
      "\n",
      "[262] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, “imap: Implicit\n",
      "mapping and positioning in real-time,” in _Proc. IEEE Int. Conf._\n",
      "_Comput. Vis._, 2021, pp. 6229–6238.\n",
      "\n",
      "[263] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, “Voxfusion: Dense tracking and mapping with voxel-based neural implicit representation,” in _IEEE International Symposium on Mixed_\n",
      "_and Augmented Reality_, 2022, pp. 499–507.\n",
      "\n",
      "[264] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald,\n",
      "and M. Pollefeys, “Nice-slam: Neural implicit scalable encoding\n",
      "for slam,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2022,\n",
      "pp. 12 786–12 796.\n",
      "\n",
      "[265] M. M. Johari, C. Carta, and F. Fleuret, “Eslam: Efficient dense\n",
      "slam system based on hybrid representation of signed distance\n",
      "fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2023,\n",
      "pp. 17 408–17 419.\n",
      "\n",
      "[266] E. Sandstr¨om, Y. Li, L. Van Gool, and M. R. Oswald, “Point-slam:\n",
      "Dense neural point cloud-based slam,” in _Proc. IEEE Int. Conf._\n",
      "_Comput. Vis._, 2023, pp. 18 433–18 444.\n",
      "\n",
      "[267] H. Wang, J. Wang, and L. Agapito, “Co-slam: Joint coordinate and\n",
      "sparse parametric encodings for neural real-time slam,” in _Proc._\n",
      "_IEEE Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 13 293–13 302.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[268] Y. Hu, Y. Fang, Z. Ge, Z. Qu, Y. Zhu, A. Pradhana, and C. Jiang,\n",
      "“A moving least squares material point method with displacement discontinuity and two-way rigid body coupling,” _ACM_\n",
      "_Trans. Graph._, vol. 37, no. 4, pp. 1–14, 2018.\n",
      "\n",
      "[269] M. M¨uller, B. Heidelberger, M. Hennix, and J. Ratcliff, “Position\n",
      "based dynamics,” _Journal of Visual Communication and Image Rep-_\n",
      "_resentation_, vol. 18, no. 2, pp. 109–118, 2007.\n",
      "\n",
      "[270] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and\n",
      "temples: Benchmarking large-scale scene reconstruction,” _ACM_\n",
      "_Trans. Graph._, vol. 36, no. 4, pp. 1–13, 2017.\n",
      "\n",
      "[271] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, “Stereo\n",
      "magnification: learning view synthesis using multiplane images,”\n",
      "_ACM Trans. Graph._, vol. 37, no. 4, pp. 1–12, 2018.\n",
      "\n",
      "[272] P. Hedman, J. Philip, T. Price, J.-M. Frahm, G. Drettakis, and\n",
      "G. Brostow, “Deep blending for free-viewpoint image-based rendering,” _ACM Trans. Graph._, vol. 37, no. 6, pp. 1–15, 2018.\n",
      "\n",
      "[273] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. Kalantari,\n",
      "R. Ramamoorthi, R. Ng, and A. Kar, “Local light field fusion:\n",
      "Practical view synthesis with prescriptive sampling guidelines,”\n",
      "_ACM Trans. Graph._, vol. 38, no. 4, pp. 1–14, 2019.\n",
      "\n",
      "[274] A. Liu, R. Tucker, V. Jampani, A. Makadia, N. Snavely, and\n",
      "A. Kanazawa, “Infinite nature: Perpetual view generation of\n",
      "natural scenes from a single image,” in _Proc. IEEE Int. Conf._\n",
      "_Comput. Vis._, 2021, pp. 14 458–14 467.\n",
      "\n",
      "[275] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers,\n",
      "“A benchmark for the evaluation of rgb-d slam systems,” in _Proc._\n",
      "_IEEE/RSJ Int. Conf. Intell. Robot. Syst._, 2012, pp. 573–580.\n",
      "\n",
      "[276] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving? the kitti vision benchmark suite,” in _Proc._\n",
      "_IEEE Conf. Comput. Vis. Pattern Recognit._, 2012, pp. 3354–3361.\n",
      "\n",
      "[277] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and\n",
      "M. Nießner, “Scannet: Richly-annotated 3d reconstructions of\n",
      "indoor scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2017, pp. 5828–5839.\n",
      "\n",
      "[278] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik,\n",
      "P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine _et al._, “Scalability in\n",
      "perception for autonomous driving: Waymo open dataset,” in\n",
      "_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2020, pp. 2446–\n",
      "2454.\n",
      "\n",
      "[279] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,\n",
      "A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A\n",
      "multimodal dataset for autonomous driving,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2020, pp. 11 621–11 631.\n",
      "\n",
      "[280] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, “Rlbench:\n",
      "The robot learning benchmark & learning environment,” _IEEE_\n",
      "_Robotics and Automation Letters_, vol. 5, no. 2, pp. 3019–3026, 2020.\n",
      "\n",
      "[281] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. Kulkarni,\n",
      "L. Fei-Fei, S. Savarese, Y. Zhu, and R. Mart´ın-Mart´ın, “What\n",
      "matters in learning from offline human demonstrations for robot\n",
      "manipulation,” in _Proc. Annu. Conf. Robot Learn._, 2022, pp. 1678–\n",
      "1690.\n",
      "\n",
      "[282] Z. Yan, C. Li, and G. H. Lee, “Nerf-ds: Neural radiance fields\n",
      "for dynamic specular objects,” in _Proc. IEEE Conf. Comput. Vis._\n",
      "_Pattern Recognit._, 2023, pp. 8285–8295.\n",
      "\n",
      "[283] K. Kania, K. M. Yi, M. Kowalski, T. Trzci´nski, and A. Tagliasacchi,\n",
      "“Conerf: Controllable neural radiance fields,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2022, pp. 18 623–18 632.\n",
      "\n",
      "[284] A. Mirzaei, T. Aumentado-Armstrong, K. G. Derpanis, J. Kelly,\n",
      "M. A. Brubaker, I. Gilitschenski, and A. Levinshtein, “Spin-nerf:\n",
      "Multiview segmentation and perceptual inpainting with neural\n",
      "radiance fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2023, pp. 20 669–20 679.\n",
      "\n",
      "[285] R. Shao, Z. Zheng, H. Tu, B. Liu, H. Zhang, and Y. Liu, “Tensor4d:\n",
      "Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering,” in _Proc. IEEE Conf. Comput. Vis._\n",
      "_Pattern Recognit._, 2023, pp. 16 632–16 642.\n",
      "\n",
      "[286] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang,\n",
      "J. Wang, C. Qian _et al._, “Omniobject3d: Large-vocabulary 3d\n",
      "object dataset for realistic perception, reconstruction and generation,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2023, pp.\n",
      "803–814.\n",
      "\n",
      "[287] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi,\n",
      "“Objaverse: A universe of annotated 3d objects,” in _Proc. IEEE_\n",
      "_Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 13 142–13 153.\n",
      "\n",
      "[288] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-Moll,\n",
      "\n",
      "\n",
      "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 22\n",
      "\n",
      "\n",
      "\n",
      "“Video based reconstruction of 3d people models,” in _Proc. IEEE_\n",
      "_Conf. Comput. Vis. Pattern Recognit._, 2018, pp. 8387–8397.\n",
      "\n",
      "[289] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. Black,\n",
      "“Capture, learning, and synthesis of 3d speaking styles,” in _Proc._\n",
      "_IEEE Conf. Comput. Vis. Pattern Recognit._, 2019, pp. 10 101–10 111.\n",
      "\n",
      "[290] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu, “Deephuman: 3d\n",
      "human reconstruction from a single image,” in _Proc. IEEE Int._\n",
      "_Conf. Comput. Vis._, 2019, pp. 7739–7749.\n",
      "\n",
      "[291] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, and Y. Liu, “Function4d:\n",
      "Real-time human volumetric capture from very sparse consumer\n",
      "rgbd sensors,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2021, pp. 5746–5756.\n",
      "\n",
      "[292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou,\n",
      "“Neural body: Implicit neural representations with structured\n",
      "latent codes for novel view synthesis of dynamic humans,” in\n",
      "_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2021, pp. 9054–\n",
      "9063.\n",
      "\n",
      "[293] E. Ramon, G. Triginer, J. Escur, A. Pumarola, J. Garcia, X. Giro-i\n",
      "Nieto, and F. Moreno-Noguer, “H3d-net: Few-shot high-fidelity\n",
      "3d head reconstruction,” in _Proc. IEEE Int. Conf. Comput. Vis._,\n",
      "2021, pp. 5620–5629.\n",
      "\n",
      "[294] Z. Su, T. Yu, Y. Wang, and Y. Liu, “Deepcloth: Neural garment\n",
      "representation for shape and style editing,” _IEEE Trans. Pattern_\n",
      "_Anal. Mach. Intell._, vol. 45, no. 2, pp. 1581–1593, 2022.\n",
      "\n",
      "[295] M. Allan, J. Mcleod, C. Wang, J. C. Rosenthal, Z. Hu, N. Gard,\n",
      "P. Eisert, K. X. Fu, T. Zeffiro, W. Xia _et al._, “Stereo correspondence\n",
      "and reconstruction of endoscopic data challenge,” _arXiv preprint_\n",
      "_arXiv:2101.01133_, 2021.\n",
      "\n",
      "[296] Y. Cai, J. Wang, A. Yuille, Z. Zhou, and A. Wang, “Structureaware sparse-view x-ray 3d reconstruction,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2024, pp. 11 174–11 183.\n",
      "\n",
      "[297] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai,\n",
      "and D. Lin, “Bungeenerf: Progressive neural radiance field for\n",
      "extreme multi-scale scene rendering,” in _Proc. Eur. Conf. Comput._\n",
      "_Vis._ Springer, 2022, pp. 106–122.\n",
      "\n",
      "[298] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P.\n",
      "Srinivasan, J. T. Barron, and H. Kretzschmar, “Block-nerf: Scalable\n",
      "large scene neural view synthesis,” in _Proc. IEEE Conf. Comput._\n",
      "_Vis. Pattern Recognit._, 2022, pp. 8248–8258.\n",
      "\n",
      "[299] G. Yang, F. Xue, Q. Zhang, K. Xie, C.-W. Fu, and H. Huang, “Urbanbis: a large-scale benchmark for fine-grained urban building\n",
      "instance segmentation,” in _Proc. ACM Spec. Interest Group Comput._\n",
      "_Graph. Interact. Tech._, 2023, pp. 1–11.\n",
      "\n",
      "[300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image\n",
      "quality assessment: from error visibility to structural similarity,”\n",
      "_IEEE Trans. Image Process._, vol. 13, no. 4, pp. 600–612, 2004.\n",
      "\n",
      "[301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,\n",
      "“The unreasonable effectiveness of deep features as a perceptual\n",
      "metric,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2018,\n",
      "pp. 586–595.\n",
      "\n",
      "[302] J. Fang, T. Yi, X. Wang, L. Xie, X. Zhang, W. Liu, M. Nießner, and\n",
      "Q. Tian, “Fast dynamic radiance fields with time-aware neural\n",
      "voxels,” in _SIGGRAPH Asia_, 2022, pp. 1–9.\n",
      "\n",
      "[303] A. Cao and J. Johnson, “Hexplane: A fast representation for dynamic scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\n",
      "2023, pp. 130–141.\n",
      "\n",
      "[304] F. Wang, Z. Chen, G. Wang, Y. Song, and H. Liu, “Masked spacetime hash encoding for efficient dynamic scene reconstruction,”\n",
      "in _Proc. Adv. Neural Inf. Process. Syst._, 2023.\n",
      "\n",
      "[305] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and\n",
      "I. Kemelmacher-Shlizerman, “Humannerf: Free-viewpoint rendering of moving people from monocular video,” in _Proc. IEEE_\n",
      "_Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 16 210–16 220.\n",
      "\n",
      "[306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, “Learning neural\n",
      "volumetric representations of dynamic humans in minutes,” in\n",
      "_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 8759–\n",
      "8770.\n",
      "\n",
      "[307] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and\n",
      "H. Bao, “Animatable neural radiance fields for modeling dynamic human bodies,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2021,\n",
      "pp. 14 314–14 323.\n",
      "\n",
      "[308] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, “pixelnerf: Neural\n",
      "radiance fields from one or few images,” in _Proc. IEEE Conf._\n",
      "_Comput. Vis. Pattern Recognit._, 2021, pp. 4578–4587.\n",
      "\n",
      "[309] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, “Neural human\n",
      "performer: Learning generalizable radiance fields for human\n",
      "\n",
      "\n",
      "\n",
      "performance rendering,” in _Proc. Adv. Neural Inf. Process. Syst._,\n",
      "2021, pp. 24 741–24 752.\n",
      "\n",
      "[310] J. Wang, Z. Zhang, Q. Zhang, J. Li, J. Sun, M. Sun, J. He, and R. Xu,\n",
      "“Query-based semantic gaussian field for scene representation in\n",
      "reinforcement learning,” _arXiv preprint arXiv:2406.02370_, 2024.\n",
      "\n",
      "[311] Y. Qu, S. Dai, X. Li, J. Lin, L. Cao, S. Zhang, and R. Ji, “Goi: Find\n",
      "3d gaussians of interest with an optimizable open-vocabulary\n",
      "semantic-space hyperplane,” _arXiv preprint arXiv:2405.17596_,\n",
      "2024.\n",
      "\n",
      "[312] Y. Ji, H. Zhu, J. Tang, W. Liu, Z. Zhang, Y. Xie, L. Ma, and\n",
      "X. Tan, “Fastlgs: Speeding up language embedded gaussians with\n",
      "feature grid mapping,” _arXiv preprint arXiv:2406.01916_, 2024.\n",
      "\n",
      "[313] G. Liao, J. Li, Z. Bao, X. Ye, J. Wang, Q. Li, and K. Liu,\n",
      "“Clip-gs: Clip-informed gaussian splatting for real-time and\n",
      "view-consistent 3d semantic understanding,” _arXiv preprint_\n",
      "_arXiv:2404.14249_, 2024.\n",
      "\n",
      "[314] S. Choi, H. Song, J. Kim, T. Kim, and H. Do, “Click-gaussian:\n",
      "Interactive segmentation to any 3d gaussians,” _arXiv preprint_\n",
      "_arXiv:2407.11793_, 2024.\n",
      "\n",
      "[315] S. Ji, G. Wu, J. Fang, J. Cen, T. Yi, W. Liu, Q. Tian, and X. Wang,\n",
      "“Segment any 4d gaussians,” _arXiv preprint arXiv:2407.04504_,\n",
      "2024.\n",
      "\n",
      "[316] A. Gu´edon and V. Lepetit, “Sugar: Surface-aligned gaussian\n",
      "splatting for efficient 3d mesh reconstruction and high-quality\n",
      "mesh rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recog-_\n",
      "_nit._, 2024.\n",
      "\n",
      "[317] T. Liu, G. Wang, S. Hu, L. Shen, X. Ye, Y. Zang, Z. Cao, W. Li,\n",
      "and Z. Liu, “Fast generalizable gaussian splatting reconstruction\n",
      "from multi-view stereo,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[318] Y. Li, X. Fu, S. Zhao, R. Jin, and S. K. Zhou, “Sparse-view\n",
      "ct reconstruction with 3d gaussian volumetric representation,”\n",
      "_arXiv preprint arXiv:2312.15676_, 2023.\n",
      "\n",
      "[319] Y. Cai, Y. Liang, J. Wang, A. Wang, Y. Zhang, X. Yang, Z. Zhou,\n",
      "and A. Yuille, “Radiative gaussian splatting for efficient x-ray\n",
      "novel view synthesis,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\n",
      "\n",
      "[320] J. Chang, Y. Xu, Y. Li, Y. Chen, and X. Han, “Gaussreg: Fast 3d\n",
      "registration with gaussian splatting,” in _Proc. Eur. Conf. Comput._\n",
      "_Vis._, 2024.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "md_text = pymupdf4llm.to_markdown(doc='data/raw_papers/A survey on 3DGS.pdf')\n",
    "\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b88c6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'graphics': [],\n",
      "  'images': [{'bbox': Rect(335.5150146484375, 319.2890319824219, 593.9107666015625, 514.4761352539062),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': True,\n",
      "              'height': 1439,\n",
      "              'number': 10,\n",
      "              'size': 124122,\n",
      "              'transform': (258.3957214355469,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            195.18710327148438,\n",
      "                            335.5150146484375,\n",
      "                            319.2890319824219),\n",
      "              'width': 1905,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(314.4906921386719, 329.46209716796875, 573.0220947265625, 524.5135498046875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': True,\n",
      "              'height': 1438,\n",
      "              'number': 9,\n",
      "              'size': 477351,\n",
      "              'transform': (258.5313720703125,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            195.0514678955078,\n",
      "                            314.4906921386719,\n",
      "                            329.46209716796875),\n",
      "              'width': 1906,\n",
      "              'xres': 96,\n",
      "              'yres': 96}],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 1,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1\\n'\n",
      "          '\\n'\n",
      "          '## A Survey on 3D Gaussian Splatting\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Guikun Chen, and Wenguan Wang, _Senior Member, IEEE_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Abstract** —3D Gaussian splatting (GS) has emerged as a '\n",
      "          'transformative technique in explicit radiance field and computer '\n",
      "          'graphics.\\n'\n",
      "          'This innovative approach, characterized by the use of millions of '\n",
      "          'learnable 3D Gaussians, represents a significant departure from\\n'\n",
      "          'mainstream neural radiance field approaches, which predominantly '\n",
      "          'use implicit, coordinate-based models to map spatial coordinates\\n'\n",
      "          'to pixel values. 3D GS, with its explicit scene representation and '\n",
      "          'differentiable rendering algorithm, not only promises real-time\\n'\n",
      "          'rendering capability but also introduces unprecedented levels of '\n",
      "          'editability. This positions 3D GS as a potential game-changer for '\n",
      "          'the\\n'\n",
      "          'next generation of 3D reconstruction and representation. In the '\n",
      "          'present paper, we provide the first systematic overview of the '\n",
      "          'recent\\n'\n",
      "          'developments and critical contributions in the domain of 3D GS. We '\n",
      "          'begin with a detailed exploration of the underlying principles and\\n'\n",
      "          'the driving forces behind the emergence of 3D GS, laying the '\n",
      "          'groundwork for understanding its significance. A focal point of '\n",
      "          'our\\n'\n",
      "          'discussion is the practical applicability of 3D GS. By enabling '\n",
      "          'unprecedented rendering speed, 3D GS opens up a plethora of\\n'\n",
      "          'applications, ranging from virtual reality to interactive media and '\n",
      "          'beyond. This is complemented by a comparative analysis of leading\\n'\n",
      "          '3D GS models, evaluated across various benchmark tasks to highlight '\n",
      "          'their performance and practical utility. The survey concludes by\\n'\n",
      "          'identifying current challenges and suggesting potential avenues for '\n",
      "          'future research. Through this survey, we aim to provide a valuable\\n'\n",
      "          'resource for both newcomers and seasoned researchers, fostering '\n",
      "          'further exploration and advancement in explicit radiance field.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Index Terms** —3D Gaussian Splatting, Explicit Radiance Field, '\n",
      "          'Real-time Rendering, Scene Understanding\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '✦\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**1** **I** **NTRODUCTION**\\n'\n",
      "          '\\n'\n",
      "          '# T is to convert a collection of views or videos capturing HE '\n",
      "          'objective of image based 3D scene reconstruction\\n'\n",
      "          '\\n'\n",
      "          'a scene into a digital 3D model that can be computationally '\n",
      "          'processed, analyzed, and manipulated. This hard\\n'\n",
      "          'and long-standing problem is fundamental for machines\\n'\n",
      "          'to comprehend the complexity of real-world environments,\\n'\n",
      "          'facilitating a wide array of applications such as 3D modeling\\n'\n",
      "          'and animation, robot navigation, historical preservation,\\n'\n",
      "          'augmented/virtual reality, and autonomous driving.\\n'\n",
      "          'The journey of 3D scene reconstruction began long\\n'\n",
      "          'before the surge of deep learning, with early endeavors\\n'\n",
      "          'focusing on light fields and basic scene reconstruction methods '\n",
      "          '[1]–[3]. These early attempts, however, were limited by\\n'\n",
      "          'their reliance on dense sampling and structured capture,\\n'\n",
      "          'leading to significant challenges in handling complex scenes\\n'\n",
      "          'and lighting conditions. The emergence of structure-frommotion [4] '\n",
      "          'and subsequent advancements in multi-view\\n'\n",
      "          'stereo [5] algorithms provided a more robust framework for\\n'\n",
      "          '3D scene reconstruction. Despite these advancements, such\\n'\n",
      "          'methods struggled with novel-view synthesis and texture\\n'\n",
      "          'loss. NeRF represents a quantum leap in this progression.\\n'\n",
      "          'By leveraging deep neural networks, NeRF enabled the\\n'\n",
      "          'direct mapping of spatial coordinates to color and density.\\n'\n",
      "          'The success of NeRF hinged on its ability to create continuous, '\n",
      "          'volumetric scene functions, producing results with\\n'\n",
      "          'unprecedented fidelity. However, as with any burgeoning\\n'\n",
      "          'technology, this implementation came at a cost: **i** ) '\n",
      "          'Computational Intensity. NeRF based methods are computationally\\n'\n",
      "          'intensive [6]–[9], often requiring extensive training times\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ _G. Chen and W. Wang are with College of Computer Science and_\\n'\n",
      "          '_Technology, Zhejiang University (Email: guikunchen@gmail.com, '\n",
      "          'wen-_\\n'\n",
      "          '_guanwang.ai@gmail.com)_\\n'\n",
      "          '\\n'\n",
      "          '_•_ _Corresponding Author: Wenguan Wang_\\n'\n",
      "          '\\n'\n",
      "          '_•_ _Paper List:_ '\n",
      "          '_[https://github.com/guikunchen/Awesome3DGS](https://github.com/guikunchen/Awesome3DGS)_\\n'\n",
      "          '\\n'\n",
      "          '_•_ _[Benchmarks: '\n",
      "          'https://github.com/guikunchen/3DGS-Benchmarks](https://github.com/guikunchen/3DGS-Benchmarks)_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Fig. 1. The number of published papers and official GitHub stars on '\n",
      "          '3D\\n'\n",
      "          '[GS. The set of statistics is sourced from # Papers and # GitHub '\n",
      "          'Stars.](https://github.com/Awesome3DGS/3D-Gaussian-Splatting-Papers)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'and substantial resources for rendering, especially for '\n",
      "          'highresolution outputs. **ii** ) Editability. Manipulating scenes '\n",
      "          'represented implicitly is challenging, since direct modifications\\n'\n",
      "          'to the neural network’s weights are not intuitively related to\\n'\n",
      "          'changes in geometric or appearance properties of the scene.\\n'\n",
      "          'It is in this context that 3D Gaussian splatting (GS) [10]\\n'\n",
      "          'emerges, not merely as an incremental improvement but as\\n'\n",
      "          'a paradigm-shifting approach that redefines the boundaries\\n'\n",
      "          'of scene representation and rendering. While NeRF excelled\\n'\n",
      "          'in creating photorealistic images, the need for faster, more\\n'\n",
      "          'efficient rendering methods was becoming increasingly apparent, '\n",
      "          'especially for applications ( _e.g_ ., virtual reality and\\n'\n",
      "          'autonomous driving) that are highly sensitive to latency.\\n'\n",
      "          '3D GS addressed this need by introducing an advanced,\\n'\n",
      "          'explicit scene representation that models a scene using\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 2,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'millions of learnable 3D Gaussians in space. Unlike the implicit, '\n",
      "          'coordinate-based models [11], [12], 3D GS employs an\\n'\n",
      "          'explicit representation and highly parallelized workflows,\\n'\n",
      "          'facilitating more efficient computation and rendering. The\\n'\n",
      "          'innovation of 3D GS lies in its unique blend of the benefits of '\n",
      "          'differentiable pipelines and point-based rendering\\n'\n",
      "          'techniques [13]–[17]. By representing scenes with learnable\\n'\n",
      "          '3D Gaussians, it preserves the strong fitting capability of\\n'\n",
      "          'continuous volumetric radiance fields, essential for highquality '\n",
      "          'image synthesis, while simultaneously avoiding\\n'\n",
      "          'the computational overhead associated with NeRF based\\n'\n",
      "          'methods ( _e.g_ ., computationally expensive ray-marching, and\\n'\n",
      "          'unnecessary calculations in empty space).\\n'\n",
      "          'The introduction of 3D GS is not just a technical advancement; it '\n",
      "          'represents a fundamental shift in how we approach\\n'\n",
      "          'scene representation and rendering in computer vision and\\n'\n",
      "          'graphics. By enabling real-time rendering capabilities without '\n",
      "          'compromising on visual quality, 3D GS opens up a\\n'\n",
      "          'plethora of possibilities for applications ranging from virtual '\n",
      "          'reality and augmented reality to real-time cinematic\\n'\n",
      "          'rendering and beyond [18]–[21]. This technology holds the\\n'\n",
      "          'promise of not only enhancing existing applications but\\n'\n",
      "          'also enabling new ones that were previously unfeasible\\n'\n",
      "          'due to computational constraints. Furthermore, 3D GS’s\\n'\n",
      "          'explicit scene representation offers unprecedented flexibility\\n'\n",
      "          'to control the objects and scene dynamics, a crucial factor in\\n'\n",
      "          'complex scenarios involving intricate geometries and varying '\n",
      "          'lighting conditions [22]–[24]. This level of editability,\\n'\n",
      "          'combined with the efficiency of the training and rendering\\n'\n",
      "          'process, positions 3D GS as a transformative force in shaping '\n",
      "          'future developments in relevant fields.\\n'\n",
      "          'In an effort to assist readers in keeping pace with the\\n'\n",
      "          'swift evolution of 3D GS, we provide the first survey on 3D\\n'\n",
      "          'GS, which presents a systematic and timely collection of the\\n'\n",
      "          'most significant literature on the topic. Given that 3D GS\\n'\n",
      "          'is a very recent innovation (Fig. 1), this survey focuses in\\n'\n",
      "          'particular on its principles, and the diverse developments\\n'\n",
      "          'and contributions that have emerged since its introduction.\\n'\n",
      "          'The selected follow-up works are primarily sourced from\\n'\n",
      "          'top-tier conferences, to provide a thorough and up-to-date\\n'\n",
      "          '(Dec. 2024) analysis of the theoretical foundations, remarkable '\n",
      "          'developments, and burgeoning applications of 3D GS.\\n'\n",
      "          'Acknowledging the nascent yet rapidly evolving nature of\\n'\n",
      "          '3D GS, this survey is inevitably a biased view, but we strive\\n'\n",
      "          'to offer a balanced perspective that reflects both the current\\n'\n",
      "          'state and the future potential of this field. Our aim is to\\n'\n",
      "          'encapsulate the primary research trends and serve as a valuable '\n",
      "          'resource for both researchers and practitioners eager to\\n'\n",
      "          'understand and contribute to this rapidly evolving domain.\\n'\n",
      "          'The distinctions of this survey from existing literature [25]–\\n'\n",
      "          '\\n'\n",
      "          '[28] are evident in the following aspects:\\n'\n",
      "          '\\n'\n",
      "          '_•_ We provide the first systematic and comprehensive review\\n'\n",
      "          'that examines 3D GS from a macro-level perspective by\\n'\n",
      "          'establishing clear taxonomies and frameworks. This highlevel '\n",
      "          'systematization helps researchers identify trends and\\n'\n",
      "          'potential directions that might not be apparent from paperspecific '\n",
      "          'reviews. Our organizational structure serves as a\\n'\n",
      "          'roadmap for understanding how different approaches relate\\n'\n",
      "          'to and build upon each other within the 3D GS ecosystem.\\n'\n",
      "          '\\n'\n",
      "          '_•_ This paper is the first and only survey to thoroughly delve\\n'\n",
      "          'into the theoretical background and fundamental principles\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Fig. 2. Structure of the overall review.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'of 3D GS. The comprehensive coverage makes the field more\\n'\n",
      "          'approachable for newcomers while providing valuable insights for '\n",
      "          'experienced researchers.\\n'\n",
      "          '\\n'\n",
      "          '_•_ To ensure our survey remains relevant and offer longterm value '\n",
      "          'in this rapidly evolving field, we maintain two\\n'\n",
      "          '[dynamic GitHub repositories: one that follows our '\n",
      "          'survey’s](https://github.com/guikunchen/Awesome3DGS)\\n'\n",
      "          '[organizational structure and another that includes '\n",
      "          'compre-](https://github.com/guikunchen/3DGS-Benchmarks)\\n'\n",
      "          'hensive performance comparisons with analysis data.\\n'\n",
      "          '\\n'\n",
      "          'A summary of the structure of this article can be found\\n'\n",
      "          'in Fig. 2, which is presented as follows: Sec. 2 provides\\n'\n",
      "          'a brief background on problem formulation, terminology,\\n'\n",
      "          'and related research domains. Sec. 3 introduces the essential\\n'\n",
      "          'insights of 3D GS, encompassing the rendering process with\\n'\n",
      "          'learned 3D Gaussians and the optimization details ( _i.e_ ., how\\n'\n",
      "          'to learn 3D Gaussians) of 3D GS. Sec. 4 presents several\\n'\n",
      "          'fruitful directions that aim to improve the capabilities of\\n'\n",
      "          'the original 3D GS. Sec. 5 unveils the diverse application\\n'\n",
      "          'areas and tasks where 3D GS has made significant impacts,\\n'\n",
      "          'showcasing its versatility. Sec. 6 conducts performance comparison '\n",
      "          'and analysis. Finally, Sec. 7 and 8 highlight the open\\n'\n",
      "          'questions for further research and conclude the survey.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 3,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**2** **B** **ACKGROUND**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'In this section, we first provide a brief formulation of\\n'\n",
      "          'radiance fields (Sec. 2.1), including both implicit and explicit\\n'\n",
      "          'ones. Sec. 2.2 further establishes linkages with relevant rendering '\n",
      "          'algorithms and terminologies. For a comprehensive\\n'\n",
      "          'overview of radiance fields, scene reconstruction and '\n",
      "          'representation, and rendering methods, please see the excellent\\n'\n",
      "          'surveys [29]–[33] for more insights.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**2.1** **Radiance Field**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Implicit Radiance Field.** An implicit radiance field '\n",
      "          'represents light distribution in a scene without explicitly '\n",
      "          'defining\\n'\n",
      "          'the geometry of the scene. In the deep learning era, neural\\n'\n",
      "          'networks are often used to learn a continuous volumetric\\n'\n",
      "          'scene representation [34], [35]. The most prominent example\\n'\n",
      "          'is NeRF [12]. In NeRF (Fig. 3a), one or more MLPs are used\\n'\n",
      "          'to map a set of spatial coordinates ( _x, y, z_ ) and viewing\\n'\n",
      "          'directions ( _θ, ϕ_ ) to color _c_ and volume density _σ_ :\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '( _c, σ_ ) _←_ MLP ( _x, y, z, θ, ϕ_ ) _._ (1)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'This format allows for a differentiable and compact representation '\n",
      "          'of complex scenes, albeit often at the cost of high\\n'\n",
      "          'computational load due to volumetric ray marching. Note\\n'\n",
      "          'that typically, the color _c_ is direction-dependent, whereas\\n'\n",
      "          'the volume density _σ_ is not [12].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Explicit Radiance Field.** An explicit radiance field '\n",
      "          'directly\\n'\n",
      "          'represents the distribution of light in a discrete spatial '\n",
      "          'structure, such as a voxel grid or a set of points [36], [37]. '\n",
      "          'Each\\n'\n",
      "          'element in this structure stores the radiance information\\n'\n",
      "          'for its respective location. This allows for direct and often\\n'\n",
      "          'faster access to radiance data but at the cost of higher\\n'\n",
      "          'memory usage and potentially lower resolution. Similar to\\n'\n",
      "          'the implicit radiance field, the explicit one is written as:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '( _c, σ_ ) _←_ DataStructure ( _x, y, z, θ, ϕ_ ) _,_ (2)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where DataStructure could be in the format of volumes,\\n'\n",
      "          'point clouds, _etc_ . DataStructure encodes directional color\\n'\n",
      "          'in two main ways. One is encoding high-dimensional features that '\n",
      "          'are subsequently decoded by a lightweight MLP.\\n'\n",
      "          'Another one is directly storing coefficients of directional\\n'\n",
      "          'basis functions, such as spherical harmonics or spherical\\n'\n",
      "          'Gaussians, where the final color is computed as a function\\n'\n",
      "          'of these coefficients and the viewing direction.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **3D Gaussian Splatting: Best-of-Both Worlds.** 3D GS [10]\\n'\n",
      "          'is an explicit radiance field with the advantages of implicit\\n'\n",
      "          'radiance fields. Concretely, it leverages the strengths of both\\n'\n",
      "          'paradigms by utilizing _learnable_ 3D Gaussians as the basis\\n'\n",
      "          'elements of DataStructure. Note that 3D GS encodes the\\n'\n",
      "          'opacity _α_ directly for each Gaussian, as opposed to approaches of '\n",
      "          'first establishing density _σ_ and then computing\\n'\n",
      "          'opacity based on that density. As in previous reconstruction\\n'\n",
      "          'work, 3D Gaussians are optimized under the supervision of\\n'\n",
      "          'multi-view images to represent the scene. Such a 3D Gaussian based '\n",
      "          'differentiable pipeline combines the benefits of\\n'\n",
      "          'neural network based optimization and explicit, structured\\n'\n",
      "          'data storage. This hybrid approach aims to achieve realtime, '\n",
      "          'high-quality rendering and requires less training time,\\n'\n",
      "          'particularly for complex scenes and high-resolution outputs.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**2.2** **Context and Terminology**\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Volumetric rendering** aims to transform a 3D volumetric\\n'\n",
      "          'representation into an image by integrating radiance along\\n'\n",
      "          'camera rays. A camera ray _**r**_ ( _t_ ) can be parameterized as:\\n'\n",
      "          '_**r**_ ( _t_ )= _**o**_ + _t_ _**d**_ _, t_ _∈_ [ _t_ near _, t_ '\n",
      "          'far ] _,_ where _**o**_ represents the ray origin\\n'\n",
      "          '(camera center), _**d**_ is the ray direction, and _t_ indicates '\n",
      "          'the\\n'\n",
      "          'distance along the ray between near and far clipping planes.\\n'\n",
      "          'The pixel color _C_ ( _**r**_ ) is computed through a line '\n",
      "          'integral\\n'\n",
      "          'along the ray _**r**_ ( _t_ ), mathematically expressed as [12]:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_t_ far\\n'\n",
      "          '_C_ ( _**r**_ ) = _T_ ( _t_ ) _σ_ ( _**r**_ ( _t_ )) _c_ ( _**r**_ '\n",
      "          '( _t_ ) _,_ _**d**_ ) _dt,_ (3)\\n'\n",
      "          '� _t_ near\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _σ_ ( _**r**_ ( _t_ )) is the volume density at point _**r**_ '\n",
      "          '( _t_ ), _c_ ( _**r**_ ( _t_ ) _,_ _**d**_ )\\n'\n",
      "          'is the color at that point, and _T_ ( _t_ ) is the transmittance. '\n",
      "          'Raymarching directly approximates the volumetric rendering\\n'\n",
      "          'integral by systematically “stepping” along a ray and sampling the '\n",
      "          'scene’s properties at discrete intervals. NeRF [12]\\n'\n",
      "          'shares the same spirit of ray-marching and introduces\\n'\n",
      "          'importance sampling and positional encoding to improve\\n'\n",
      "          'the quality of synthesized images. While providing highquality '\n",
      "          'results, ray-marching is computationally expensive,\\n'\n",
      "          'especially for high-resolution images.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Point-based rendering** represents another class of rendering '\n",
      "          'algorithms, of which 3D GS introduces a notable implementation. Its '\n",
      "          'simplest form [38] rasterizes point clouds\\n'\n",
      "          'with a fixed size, which introduces drawbacks such as holes\\n'\n",
      "          'and rendering artifacts. Seminal works addressed these\\n'\n",
      "          'limitations through various methods, including: **i** ) splatting\\n'\n",
      "          'point primitives with a spatial extent [14], [15], [39], [40],\\n'\n",
      "          'and **ii** ) more recently, embedding neural features directly\\n'\n",
      "          'into points for subsequent network-based rendering [41],\\n'\n",
      "          '\\n'\n",
      "          '[42]. 3D GS uses 3D Gaussian as the point primitive that\\n'\n",
      "          'contains explicit attributes ( _e.g_ ., color and opacity) instead '\n",
      "          'of\\n'\n",
      "          'implicit neural features. The rendering approach, _i.e_ ., '\n",
      "          'pointbased _α_ -blending (exemplified in Eq. 5), shares the same\\n'\n",
      "          'image formation model as NeRF-style volumetric rendering\\n'\n",
      "          '(Eq. 3) [10], but demonstrates substantial speed advantages.\\n'\n",
      "          'This advantage originates from fundamental algorithmic\\n'\n",
      "          'differences. NeRFs approximate a line integral along a ray\\n'\n",
      "          'for each pixel, requiring expensive sampling. Point-based\\n'\n",
      "          'methods render point clouds using rasterization, which inherently '\n",
      "          'benefits from parallel computational strategies [43].\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**3** **3D G** **AUSSIAN** **S** **PLATTING** **: P** '\n",
      "          '**RINCIPLES**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '3D GS offers a breakthrough in real-time, high-resolution\\n'\n",
      "          'image rendering, without relying on deep neural networks.\\n'\n",
      "          'This section aims to provide essential insights of 3D GS. We\\n'\n",
      "          'first elaborate on how 3D GS synthesizes an image given\\n'\n",
      "          'well-constructed 3D Gaussians in Sec. 3.1, _i.e_ ., the forward\\n'\n",
      "          'process of 3D GS. Then, we introduce how to obtain wellconstructed '\n",
      "          '3D Gaussians for a given scene in Sec. 3.2, _i.e_ .,\\n'\n",
      "          'the optimization process of 3D GS.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**3.1** **Rendering with Learned 3D Gaussians**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Consider a scene represented by (millions of) optimized\\n'\n",
      "          '3D Gaussians. The objective is to generate an image from\\n'\n",
      "          'a specified camera pose. Recall that NeRFs approach this\\n'\n",
      "          'task through computationally demanding volumetric raymarching, '\n",
      "          'sampling 3D space points per pixel. Such a\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [{'bbox': Rect(88.35691833496094, 88.7434310913086, 123.12225341796875, 141.17108154296875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'DeviceRGB',\n",
      "              'has-mask': True,\n",
      "              'height': 297,\n",
      "              'number': 2,\n",
      "              'size': 6216,\n",
      "              'transform': (34.76533126831055,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            52.427650451660156,\n",
      "                            88.35691833496094,\n",
      "                            88.7434310913086),\n",
      "              'width': 197,\n",
      "              'xres': 96,\n",
      "              'yres': 96}],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 4,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Rendering by Pixels.** Before delving into the final version\\n'\n",
      "          'of 3D GS which utilizes several techniques to boost parallel\\n'\n",
      "          'computation, we first elaborate on its simpler form to offer\\n'\n",
      "          'insights into its basic working mechanism. Given the position of a '\n",
      "          'pixel _**x**_, its distance to all overlapping Gaussians,\\n'\n",
      "          '_i.e_ ., the depths of these Gaussians, can be computed through\\n'\n",
      "          'the viewing transformation matrix _**W**_, forming a sorted list\\n'\n",
      "          'of Gaussians _N_ . Then, _α_ -blending is adopted to compute\\n'\n",
      "          'the final color of this pixel:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_c_ & _σ_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Image Space\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Splatting\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '(b) 3D GS\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '(a) NeRF\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_n−_ 1\\n'\n",
      "          '�\\n'\n",
      "          '\\n'\n",
      "          '_j_ =1\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Fig. 3. NeRFs _vs_ . 3D GS. (a) NeRF samples along the ray and '\n",
      "          'then\\n'\n",
      "          'queries the MLP to obtain corresponding colors and densities, '\n",
      "          'which\\n'\n",
      "          'can be seen as a _backward_ mapping (ray tracing). (b) In contrast, '\n",
      "          '3D GS\\n'\n",
      "          'projects all 3D Gaussians into the image space ( _i.e_ ., '\n",
      "          'splatting) and then\\n'\n",
      "          'performs parallel rendering, which can be viewed as a _forward_ '\n",
      "          'mapping\\n'\n",
      "          '(rasterization). Best viewed in color.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'paradigm struggles with high-resolution image synthesis,\\n'\n",
      "          'failing to achieve real-time rendering, especially for platforms '\n",
      "          'with limited computing resources [10]. By contrast,\\n'\n",
      "          '3D GS begins by projecting these 3D Gaussians onto a pixelbased '\n",
      "          'image plane, a process termed “splatting” [39], [40]\\n'\n",
      "          '(see Fig. 3b). Afterwards, 3D GS sorts these Gaussians and\\n'\n",
      "          'computes the value for each pixel. As shown in Fig. 3, the\\n'\n",
      "          'rendering of NeRFs and 3D GS can be viewed as an inverse\\n'\n",
      "          'process of each other. In what follows, we begin with the\\n'\n",
      "          'definition of a 3D Gaussian, which is the minimal element\\n'\n",
      "          'of the scene representation in 3D GS. Next, we describe how\\n'\n",
      "          'these 3D Gaussians can be used for differentiable rendering.\\n'\n",
      "          'Finally, we introduce the acceleration technique used in 3D\\n'\n",
      "          'GS, which is the key to fast rendering.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Properties of 3D Gaussian.** A 3D Gaussian is characterized '\n",
      "          'by its center (position) _**µ**_, opacity _α_, 3D covariance\\n'\n",
      "          'matrix **Σ**, and color _c_ . _c_ is represented by spherical '\n",
      "          'harmonics for view-dependent appearance. All the properties\\n'\n",
      "          'are learnable and optimized through back-propagation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Frustum Culling.** Given a specified camera pose, this step\\n'\n",
      "          'determines which 3D Gaussians are outside the camera’s\\n'\n",
      "          'frustum. By doing so, 3D Gaussians outside the given view\\n'\n",
      "          'will not be involved in the subsequent computation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Splatting.** In this step, 3D Gaussians (ellipsoids) in 3D\\n'\n",
      "          'space are projected into 2D image space (ellipses). The projection '\n",
      "          'proceeds through two transformations: first, transforming 3D '\n",
      "          'Gaussians from world coordinates to camera\\n'\n",
      "          'coordinates using the viewing transformation, and subsequently '\n",
      "          'splatting these Gaussians into 2D image space via\\n'\n",
      "          'an approximation of the projective transformation. Mathematically, '\n",
      "          'given the 3D covariance matrix **Σ** describing a 3D\\n'\n",
      "          'Gaussian’s spatial distribution, and the viewing transformation '\n",
      "          'matrix _**W**_, the 2D covariance matrix **Σ** _[′]_ '\n",
      "          'characterizing\\n'\n",
      "          'the projected 2D Gaussian is computed through:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Σ** _[′]_ = _**JW**_ **Σ** _**W**_ _[⊤]_ _**J**_ _[⊤]_ _,_ (4)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _**J**_ is the Jacobian of the affine approximation of the\\n'\n",
      "          'projective transformation [10], [39]. One might wonder why\\n'\n",
      "          'the standard camera intrinsics based projective transformation is '\n",
      "          'not used here. This is because its mappings are not\\n'\n",
      "          'affine and therefore cannot directly project **Σ** . 3D GS adopts\\n'\n",
      "          'an affine one proposed in [39] which approximates the projective '\n",
      "          'transformation using the first two terms (including\\n'\n",
      "          '_**J**_ ) of the Taylor expansion (see Sec. 4.4 in [39]).\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_C_ =\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_|N |_\\n'\n",
      "          '� _c_ _n_ _α_ _n_ _[′]_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_n_ =1\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '� 1 _−_ _α_ _j′_ � _,_ (5)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _c_ _n_ is the learned color. The final opacity _α_ _n_ _[′]_ '\n",
      "          '[is the]\\n'\n",
      "          'multiplication result of the learned opacity _α_ _n_ and the\\n'\n",
      "          'Gaussian, defined as follows:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '1\\n'\n",
      "          '_α_ _n_ _[′]_ [=] _[ α]_ _[n]_ _[×]_ [ exp] � _−_ _n_ [)] _[⊤]_ '\n",
      "          '**[Σ]** _[′−]_ _n_ [1] ( _**x**_ _[′]_ _−_ _**µ**_ _[′]_ _n_ [)] � '\n",
      "          '_,_ (6)\\n'\n",
      "          '\\n'\n",
      "          '2 [(] _**[x]**_ _[′]_ _[ −]_ _**[µ]**_ _[′]_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _**x**_ _[′]_ and _**µ**_ _[′]_ _n_ [are coordinates in the '\n",
      "          'projected space. It]\\n'\n",
      "          'is a reasonable concern that the rendering process described\\n'\n",
      "          'could be slower compared to NeRFs, given that generating\\n'\n",
      "          'the required sorted list is hard to parallelize. Indeed, this\\n'\n",
      "          'concern is justified; rendering speeds can be significantly\\n'\n",
      "          'impacted when utilizing such a simplistic, pixel-by-pixel\\n'\n",
      "          'approach. To achieve real-time rendering, 3D GS makes\\n'\n",
      "          'several concessions to accommodate parallel computation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Tiles (Patches).** To avoid the cost computation of deriving\\n'\n",
      "          'Gaussians for each pixel, 3D GS shifts the precision from\\n'\n",
      "          'pixel-level to patch-level detail, which is inspired by tilebased '\n",
      "          'rasterization [43]. Concretely, 3D GS initially divides\\n'\n",
      "          'the image into multiple non-overlapping patches (tiles).\\n'\n",
      "          'Fig. 4b provides an illustration of tiles. Each tile comprises\\n'\n",
      "          '16 _×_ 16 pixels as suggested in [10]. 3D GS further determines\\n'\n",
      "          'which **tiles** intersect with these projected Gaussians. Given\\n'\n",
      "          'that a projected Gaussian may cover several tiles, a logical\\n'\n",
      "          'method involves replicating the Gaussian, assigning each\\n'\n",
      "          'copy an identifier ( _i.e_ ., a tile ID) for the relevant tile.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Parallel Rendering.** After replication, 3D GS combines\\n'\n",
      "          'the respective tile ID with the depth value obtained from\\n'\n",
      "          'the view transformation for each Gaussian. This results in\\n'\n",
      "          'an unsorted list of bytes where the upper bits represent\\n'\n",
      "          'the tile ID and the lower bits signify depth. By doing so,\\n'\n",
      "          'the sorted list can be directly utilized for rendering ( _i.e_ .,\\n'\n",
      "          'alpha compositing). Fig. 4c and Fig. 4d provide the visual\\n'\n",
      "          'demonstration of such concepts. It’s worth highlighting that\\n'\n",
      "          'rendering each tile and pixel occurs **independently**, making\\n'\n",
      "          'this process highly suitable for parallel computations. An\\n'\n",
      "          'additional benefit is that each tile’s pixels can access a\\n'\n",
      "          'common **shared memory** and maintain an **uniform read**\\n'\n",
      "          '**sequence** (Fig. 5), enabling parallel execution of alpha '\n",
      "          'compositing with increased efficiency. In the official '\n",
      "          'implementation of the original paper [10], the framework regards '\n",
      "          'the\\n'\n",
      "          'processing of tiles and pixels as analogous to the blocks and\\n'\n",
      "          'threads, respectively, in CUDA programming architecture.\\n'\n",
      "          'In a nutshell, 3D GS introduces several approximations\\n'\n",
      "          'during rendering to enhance computational efficiency while\\n'\n",
      "          'maintaining a high standard of image synthesis quality.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**3.2** **Optimization of 3D Gaussian Splatting**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'At the heart of 3D GS lies an optimization procedure devised to '\n",
      "          'construct a copious collection of 3D Gaussians that\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [{'bbox': Rect(85.88441467285156, 120.70323181152344, 143.8795928955078, 178.6984100341797),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'DeviceRGB',\n",
      "              'has-mask': True,\n",
      "              'height': 305,\n",
      "              'number': 38,\n",
      "              'size': 1081,\n",
      "              'transform': (57.99517822265625,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            57.99517822265625,\n",
      "                            85.88441467285156,\n",
      "                            120.70323181152344),\n",
      "              'width': 305,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(71.31451416015625, 48.99148178100586, 109.46174621582031, 111.53778076171875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'DeviceRGB',\n",
      "              'has-mask': True,\n",
      "              'height': 329,\n",
      "              'number': 30,\n",
      "              'size': 51861,\n",
      "              'transform': (38.14723587036133,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            62.54629898071289,\n",
      "                            71.31451416015625,\n",
      "                            48.99148178100586),\n",
      "              'width': 201,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(104.04621887207031, 133.63125610351562, 144.57015991210938, 179.5570068359375),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'DeviceRGB',\n",
      "              'has-mask': True,\n",
      "              'height': 221,\n",
      "              'number': 37,\n",
      "              'size': 16365,\n",
      "              'transform': (16.4140567779541,\n",
      "                            -11.493159294128418,\n",
      "                            24.109874725341797,\n",
      "                            34.432594299316406,\n",
      "                            104.04621887207031,\n",
      "                            145.12442016601562),\n",
      "              'width': 105,\n",
      "              'xres': 96,\n",
      "              'yres': 96}],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 5,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [{'bbox': (3.6166000366210938,\n",
      "                       94.6661605834961,\n",
      "                       559.9910278320312,\n",
      "                       401.82958984375),\n",
      "              'columns': 2,\n",
      "              'rows': 2},\n",
      "             {'bbox': (69.34789276123047,\n",
      "                       260.0520324707031,\n",
      "                       146.24896240234375,\n",
      "                       288.3853454589844),\n",
      "              'columns': 2,\n",
      "              'rows': 2},\n",
      "             {'bbox': (216.38955688476562,\n",
      "                       309.7534484863281,\n",
      "                       294.7477111816406,\n",
      "                       337.3543701171875),\n",
      "              'columns': 2,\n",
      "              'rows': 2},\n",
      "             {'bbox': (260.1175537109375,\n",
      "                       115.7151107788086,\n",
      "                       330.50714111328125,\n",
      "                       172.6157684326172),\n",
      "              'columns': 2,\n",
      "              'rows': 3},\n",
      "             {'bbox': (353.526123046875,\n",
      "                       115.49913787841797,\n",
      "                       423.9157409667969,\n",
      "                       172.6157684326172),\n",
      "              'columns': 2,\n",
      "              'rows': 3}],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '(a)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Splatting\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Image Space 3D Gaussians (c) (d)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Sorted 2D Gaussians\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Tile 1 Depth\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Tile 1 Depth\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Tile1\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_C_ 1 _C_ 2\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_C_ 3 _C_ 4\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Replication\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Tile 1** **Depth**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Tile 2** **Depth**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Tile 2|Depth|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '|Tile 3|Depth|\\n'\n",
      "          '|Tile 4|Depth|\\n'\n",
      "          '|Tile 3|Depth|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Tile 2|Depth|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '|Tile 3|Depth|\\n'\n",
      "          '|Tile 3|Depth|\\n'\n",
      "          '|Tile 4|Depth|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Tile 1|Depth|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '|Tile 1|Depth|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Tile 1|Depth|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '|**Tile 1**|**Depth**|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'through the list of Gaussians just once. The computation for the '\n",
      "          'red\\n'\n",
      "          '\\n'\n",
      "          '|Col1|Parallel Rendering|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '||Parallel Rendering<br>_C_1 =_α′_<br>1_c_1 +_ α′_<br>1_c_2(1_ '\n",
      "          '−α′_<br>1)<br>_C_2 =_α′_<br>2_c_1 +_ α′_<br>2_c_2(1_ '\n",
      "          '−α′_<br>2)<br>_C_3 =_α′_<br>3_c_1 +_ α′_<br>3_c_2(1_ '\n",
      "          '−α′_<br>3)<br>_C_4 =_α′_<br>4_c_1 +_ α′_<br>4_c_2(1_ −α′_<br>4)|\\n'\n",
      "          '|**Depth**<br>**Depth**<br>Tile3<br>Tile4<br>Depth<br>Tile '\n",
      "          '3<br>Depth<br>Tile 4<br>Depth<br>Tile 4<br>Depth<br>Tile 3<br>_C_3 '\n",
      "          '=_α′_<br>3_c_1 +_ α′_<br>3_c_2(<br>_C_4 =_α′_<br>4_c_1 +_ '\n",
      "          'α′_<br>4_c_2(<br>Fig. 4. An illustration of the forward process of '\n",
      "          '3D GS (see Sec. 3.1). (a) The splatting step projects 3D Gaussians '\n",
      "          'into image space.<br>divides the image into multiple '\n",
      "          'non-overlapping patches,_ i.e_., tiles. (c) 3D GS replicates the '\n",
      "          'Gaussians which cover several tiles, assig<br>copy an identifer,_ '\n",
      "          'i.e_., a tile ID. (d) By rendering the sorted Gaussians, we can '\n",
      "          'obtain all pixels within the tile. Note that the '\n",
      "          'computational<br>for pixels and tiles are independent and can be '\n",
      "          'done in parallel. Best viewed in color.<br>**Depth**<br>**Tile '\n",
      "          '1**<br>Sorted 2D Gaussians<br>Tile1<br>Depth<br>Tile '\n",
      "          '1<br>Depth<br>Tile 1<br>Shared<br>……<br>**Depth**<br>**Tile '\n",
      "          '1**<br>Uniform '\n",
      "          'Read<br>Sequence<br>Overlap<br>Non-overlap<br>Parallel '\n",
      "          'Access<br>Pass<br>Eq.<br>Pass<br>Pass<br>5<br>Fig. 5. An '\n",
      "          'illustration of the tile based parallel (at the pixel-level) '\n",
      "          'ren-<br>dering. All the pixels within a tile (Tile1 here) access '\n",
      "          'the same ordered<br>Gaussian list stored in a shared memory for '\n",
      "          'rendering. As the system<br>processes each Gaussian sequentially, '\n",
      "          'every pixel in the tile evaluates<br>the Gaussian’s contribution '\n",
      "          'according to the distance (_i.e_., the exp term<br>in Eq. 6). '\n",
      "          'Therefore, the rendering for a tile can be completed by '\n",
      "          'iterating<br><br>which would not adhere to the physical '\n",
      "          'interpreta<br>ically associated with covariance matrices. To '\n",
      "          'cir<br>this issue, 3D GS chooses to optimize a quaternio<br>a 3D '\n",
      "          'vector**_ s_**. Here**_ q_** and**_ s_** represent rotation '\n",
      "          'a<br>respectively. This approach allows the covariance<br>to be '\n",
      "          'reconstructed as follows:<br>**Σ** =**_ '\n",
      "          'RSS_**_⊤_**_R_**_⊤,_<br>where**_ R_** is the rotation matrix '\n",
      "          'derived from the qu<br>**_q_**, and**_ S_** is the scaling matrix '\n",
      "          'given by diag(**_s_**).<br>there is a complex computational graph '\n",
      "          'to obtain th<br>_α_,_ i.e_.,**_ q_** and**_ s_**_ →_**Σ**,** Σ**_ '\n",
      "          '→_**Σ**_′_, and** Σ**_′ →α_. To avoid<br>of automatic '\n",
      "          'differentiation, 3D GS derives the grad<br>**_q_** and**_ s_** so '\n",
      "          'as to compute them directly during opti|_C_3 =_α′_<br>3_c_1 +_ '\n",
      "          'α′_<br>3_c_2(<br>_C_4 =_α′_<br>4_c_1 +_ α′_<br>4_c_2(|\\n'\n",
      "          '\\n'\n",
      "          'Gaussian follows a similar way and is omitted here for simplicity.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'accurately captures the scene’s essence, thereby facilitating\\n'\n",
      "          'free-viewpoint rendering. On the one hand, the properties\\n'\n",
      "          'of 3D Gaussians should be optimized via differentiable\\n'\n",
      "          'rasterization to fit the textures of a given scene. On the\\n'\n",
      "          'other hand, the number of 3D Gaussians that can represent a\\n'\n",
      "          'given scene well is unknown in advance. We will introduce\\n'\n",
      "          'how to optimize the properties of each Gaussian in Sec. 3.2.1\\n'\n",
      "          'and how to adaptively control the density of the Gaussians\\n'\n",
      "          'in Sec. 3.2.2. The two procedures are interleaved within the\\n'\n",
      "          'optimization workflow. Since there are many manually set\\n'\n",
      "          'hyperparameters in the optimization process, we omit the\\n'\n",
      "          'notations of most hyperparameters for clarity.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_3.2.1_ _Parameter Optimization_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Loss Function.** Once the synthesis of the image is '\n",
      "          'completed, the difference between the rendered image and\\n'\n",
      "          'ground truth can be measured. All the learnable parameters\\n'\n",
      "          'are optimized by stochastic gradient descent using the _ℓ_ 1\\n'\n",
      "          'and D-SSIM loss functions:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_L_ = (1 _−_ _λ_ ) _L_ 1 + _λL_ D-SSIM _,_ (7)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _λ ∈_ [0 _,_ 1] is a weighting factor.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Parameter Update.** Most properties of a 3D Gaussian\\n'\n",
      "          'can be optimized directly through back-propagation. It is\\n'\n",
      "          'essential to note that directly optimizing the covariance\\n'\n",
      "          'matrix **Σ** can result in a non-positive semi-definite matrix,\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_3.2.2_ _Density Control_\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Initialization.** 3D GS starts with the initial set of '\n",
      "          'sparse\\n'\n",
      "          'points from SfM or random initialization. Note that a good\\n'\n",
      "          'initialization is essential to convergence and reconstruction\\n'\n",
      "          'quality [44]. Afterwards, point densification and pruning are\\n'\n",
      "          'adopted to control the density of 3D Gaussians.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Point Densification.** In the point densification phase, 3D\\n'\n",
      "          'GS adaptively increases the density of Gaussians to better\\n'\n",
      "          'capture the details of a scene. This process focuses on areas\\n'\n",
      "          'with missing geometric features or regions where Gaussians\\n'\n",
      "          'are too spread out. The densification procedure will be\\n'\n",
      "          'performed at regular intervals ( _i.e_ ., after a certain number '\n",
      "          'of\\n'\n",
      "          'training iterations), focusing on those Gaussians with large\\n'\n",
      "          'view-space positional gradients ( _i.e_ ., above a specific '\n",
      "          'threshold). It involves either cloning small Gaussians in '\n",
      "          'underreconstructed areas or splitting large Gaussians in '\n",
      "          'overreconstructed regions. For cloning, a copy of the Gaussian\\n'\n",
      "          'is created and moved towards the positional gradient. For\\n'\n",
      "          'splitting, a large Gaussian is replaced with two smaller ones,\\n'\n",
      "          'reducing their scale by a specific factor. This step seeks an\\n'\n",
      "          'optimal distribution and representation of Gaussians in 3D\\n'\n",
      "          'space, enhancing the overall quality of the reconstruction.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Point Pruning.** The point pruning stage involves the\\n'\n",
      "          'removal of superfluous or less impactful Gaussians, which\\n'\n",
      "          'can be viewed as a regularization process. It is executed by\\n'\n",
      "          'eliminating Gaussians that are virtually transparent (with _α_\\n'\n",
      "          'below a specified threshold) and those that are excessively\\n'\n",
      "          'large in either world-space or view-space. In addition, to\\n'\n",
      "          'prevent unjustified increases in Gaussian density near input\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 6,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'cameras, the alpha value of the Gaussians is set close to\\n'\n",
      "          'zero after a certain number of iterations. This allows for a\\n'\n",
      "          'controlled increase in the density of necessary Gaussians\\n'\n",
      "          'while enabling the culling of redundant ones. The process\\n'\n",
      "          'not only helps in conserving computational resources but\\n'\n",
      "          'also ensures that the Gaussians in the model remain precise\\n'\n",
      "          'and effective for the representation of the scene.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4** **3D G** **AUSSIAN** **S** **PLATTING** **: D** '\n",
      "          '**IRECTIONS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Though 3D GS has achieved impressive milestones, significant room '\n",
      "          'for improvement remains, _e.g_ ., data and hardware\\n'\n",
      "          'requirement, rendering and optimization algorithm, and applications '\n",
      "          'in downstream tasks. In the subsequent sections,\\n'\n",
      "          'we seek to elaborate on select extended versions. These are:\\n'\n",
      "          '**i** ) 3D GS for Sparse Input [45]–[55] (Sec. 4.1), **ii** ) '\n",
      "          'Memoryefficient 3D GS [56]–[64] (Sec. 4.2), **iii** ) '\n",
      "          'Photorealistic 3D\\n'\n",
      "          'GS [65]–[80] (Sec. 4.3), **iv** ) Improved Optimization Algorithms '\n",
      "          '[22], [77], [81]–[86] (Sec. 4.4), **v** ) 3D Gaussian with\\n'\n",
      "          'More Properties [87]–[93] (Sec. 4.5), **vi** ) Hybrid '\n",
      "          'Representation [94]–[96] (Sec. 4.6), and **vii** ) New Rendering '\n",
      "          'Algorithm\\n'\n",
      "          '(Sec. 4.7). While we have carefully selected several key\\n'\n",
      "          'directions, we acknowledge that it is inevitably a biased\\n'\n",
      "          '[view. A more comprehensive collection is given in '\n",
      "          'Github.](https://github.com/guikunchen/Awesome3DGS)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.1** **3D GS for Sparse Input**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'A notable issue of 3D GS is the emergence of artifacts in\\n'\n",
      "          'areas with insufficient observational data. This challenge is\\n'\n",
      "          'a prevalent limitation in radiance field rendering, where\\n'\n",
      "          'sparse data often leads to inaccuracies in reconstruction.\\n'\n",
      "          'From a practical perspective, reconstructing scenes from\\n'\n",
      "          'limited viewpoints is of significant interest, particularly for\\n'\n",
      "          'the potential to enhance functionality with minimal input.\\n'\n",
      "          'Existing methods can be categorized into two primary\\n'\n",
      "          'groups. **i) Regularization** based methods introduce additional '\n",
      "          'constraints such as depth information to enhance\\n'\n",
      "          'the detail and global consistency [46], [49], [51], [55]. For\\n'\n",
      "          'example, DNGaussian [49] introduced a depth-regularized\\n'\n",
      "          'approach to address the challenge of geometry degradation\\n'\n",
      "          'in sparse input. FSGS [46] devised a Gaussian Unpooling\\n'\n",
      "          'process for initialization and also introduced depth '\n",
      "          'regularization. MVSplat [51] proposed a cost volume representation '\n",
      "          'so as to provide geometry cues. Unfortunately, when\\n'\n",
      "          'dealing with a limited number of views, or even just one,\\n'\n",
      "          'the efficacy of regularization techniques tends to diminish,\\n'\n",
      "          'which leads to **ii) generalizability** based methods that use\\n'\n",
      "          'learned priors [47], [48], [53], [97]. One approach involves\\n'\n",
      "          'synthesizing additional views through generative models,\\n'\n",
      "          'which can be seamlessly integrated into existing reconstruction '\n",
      "          'pipelines [98]. However, this augmentation strategy is\\n'\n",
      "          'computationally intensive and inherently bounded by the\\n'\n",
      "          'capabilities of the used generative model. Another wellknown '\n",
      "          'paradigm employs feed-forward Gaussian model to\\n'\n",
      "          'directly generates the properties of a set of 3D Gaussians.\\n'\n",
      "          'This paradigm typically requires multiple views for training\\n'\n",
      "          'but can reconstruct 3D scenes with only one input image.\\n'\n",
      "          'For instance, PixelSplat [47] proposed to sample Gaussians\\n'\n",
      "          'from dense probability distributions. Splatter Image [48] '\n",
      "          'introduced a 2D image-to-image network that maps an input\\n'\n",
      "          'image to a 3D Gaussian per pixel. However, as the generated\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'pixel-aligned Gaussians are distributed nearly evenly in the\\n'\n",
      "          'space, they struggle to represent high-frequency details and\\n'\n",
      "          'smoother regions with an appropriate number of Gaussians.\\n'\n",
      "          '\\n'\n",
      "          'The challenge of 3D GS for sparse inputs centers on\\n'\n",
      "          'the modeling of priors, whether through depth information,\\n'\n",
      "          'generative models, or feed-forward Gaussian models. The\\n'\n",
      "          'fundamental trade-off lies between overfitting to available\\n'\n",
      "          'views and using learned priors for generalization. Future\\n'\n",
      "          'research could explore adaptive mechanisms for controlling\\n'\n",
      "          'this trade-off, potentially through learned confidence measures, '\n",
      "          'context-aware prior selection, user preferences, _etc_ . In\\n'\n",
      "          'addition, while current methods focus on static scenes, extending '\n",
      "          'these approaches to dynamic scenarios presents an\\n'\n",
      "          'exciting frontier for investigation, particularly in handling\\n'\n",
      "          'temporal consistency and motion-induced artifacts.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.2** **Memory-efficient 3D GS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'While 3D GS demonstrates remarkable capabilities, its scalability '\n",
      "          'poses significant challenges, particularly when juxtaposed with '\n",
      "          'NeRF-based methods. The latter benefits from\\n'\n",
      "          'the simplicity of storing merely the parameters of a learned\\n'\n",
      "          'MLP. This scalability issue becomes increasingly acute in\\n'\n",
      "          'the context of large-scale scene management, where the\\n'\n",
      "          'computational and memory demands escalate substantially.\\n'\n",
      "          'Consequently, there is an urgent need to optimize memory\\n'\n",
      "          'usage in both model training and storage.\\n'\n",
      "          'Recent research has pursued two primary directions to\\n'\n",
      "          'address memory efficiency. First, several approaches focus\\n'\n",
      "          'on **reducing the number of 3D Gaussians** [58], [62], [63].\\n'\n",
      "          'These methods either employ strategic pruning of lowimpact '\n",
      "          'Gaussians, such as the volume-based masking [58],\\n'\n",
      "          'or represent neighboring Gaussians using the same properties stored '\n",
      "          'within a “local anchor” obtained by clustering [22], hash-grid '\n",
      "          '[62], _etc_ . Second, researchers have developed methods for '\n",
      "          '**compressing Gaussian’s properties** [58],\\n'\n",
      "          '\\n'\n",
      "          '[61], [62]. For instance, Niedermayr _et al_ . [61] compressed\\n'\n",
      "          'color and Gaussian parameters into compact codebooks,\\n'\n",
      "          'using sensitivity measures for effective quantization and\\n'\n",
      "          'fine-tuning. HAC [62] predicted the probability of each\\n'\n",
      "          'quantized attribute using Gaussian distributions and then\\n'\n",
      "          'devise an adaptive quantization module. These directions\\n'\n",
      "          'are not mutually exclusive; instead, one framework might\\n'\n",
      "          'use a hybrid approach combining multiple strategies.\\n'\n",
      "          '\\n'\n",
      "          'While current compression techniques have achieved\\n'\n",
      "          'significant storage reduction ratios (often by factors of 1020 _×_ '\n",
      "          '), several challenges remain. The field particularly needs\\n'\n",
      "          'advances in memory efficiency during the training phase,\\n'\n",
      "          'potentially through quantization-aware training protocols,\\n'\n",
      "          'the development of scene-agnostic, reusable codebooks, _etc_ .\\n'\n",
      "          'Furthermore, optimizing the trade-off between compression\\n'\n",
      "          'efficiency and visual fidelity remains an open problem.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.3** **Photorealistic 3D GS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The current rendering pipeline of 3D GS (Sec. 3.1) is '\n",
      "          'straightforward and involves several drawbacks. For instance, the\\n'\n",
      "          'simple visibility algorithm may lead to a drastic switch\\n'\n",
      "          'in the depth/blending order of Gaussians [10]. The visual\\n'\n",
      "          'fidelity of rendered images, including aspects such as aliasing, '\n",
      "          'reflections, and artifacts, can be further optimized.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 7,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Recent research has focused on addressing three main\\n'\n",
      "          'aspects of visual quality, with aliasing being specific to 3D\\n'\n",
      "          'GS’s rendering algorithm, while reflection and blur handling '\n",
      "          'represent broader challenges in 3D reconstruction. **i)**\\n'\n",
      "          '**Aliasing** . Due to the discrete sampling paradigm (viewing\\n'\n",
      "          'each pixel as a single point instead of an area), 3D GS is\\n'\n",
      "          'susceptible to aliasing when dealing with varying resolutions, '\n",
      "          'which leads to blurring or jagged edges. Solutions\\n'\n",
      "          'emerged at both training and inference stages. Researchers\\n'\n",
      "          'developed training-time improvements from the sampling\\n'\n",
      "          'rate perspective and introduced schemes such as multi-scale\\n'\n",
      "          'Gaussians [67], 2D Mip filter [65], and conditioned logistic\\n'\n",
      "          'function [78]. Inference-time solutions, such as 2D scaleadaptive '\n",
      "          'filtering [80], offer enhanced fidelity that can be\\n'\n",
      "          'integrated into any existing 3D GS frameworks. **ii) Reflec-**\\n'\n",
      "          '**tion** . Achieving realistic rendering of reflective materials '\n",
      "          'is\\n'\n",
      "          'a hard, long-standing problem in 3D scene reconstruction.\\n'\n",
      "          'Recent works have introduced various approaches to model\\n'\n",
      "          'reflective materials [68], [73], [99] and enable relightable\\n'\n",
      "          'Gaussian representation [23], though achieving physically\\n'\n",
      "          'accurate specular effects remains challenging. **iii) Blur** .\\n'\n",
      "          'While 3D GS excels on carefully curated datasets, realworld '\n",
      "          'captures often suffer from blurs such as motion blur\\n'\n",
      "          'and defocus blur. Recent approaches explicitly incorporated\\n'\n",
      "          'blur modeling during training, employing techniques such\\n'\n",
      "          'as coarse-to-fine kernel optimization [74] and photometric\\n'\n",
      "          'bundle adjustment [75] to address this challenge.\\n'\n",
      "          '\\n'\n",
      "          'While the approximations made in 3D GS (Sec. 3.1)\\n'\n",
      "          'contribute to its computational efficiency, they also lead to\\n'\n",
      "          'aliasing, difficulties in illumination estimation, _etc_ . Current\\n'\n",
      "          'solutions, though impressive, typically address individual\\n'\n",
      "          'problems rather than providing a universal solution. A practical '\n",
      "          'intermediate approach involves first detecting specific\\n'\n",
      "          'issues ( _e.g_ ., aliasing, blur) and then applying targeted '\n",
      "          'optimization strategies. The ultimate goal remains developing\\n'\n",
      "          'an advanced reconstruction system that overcomes these\\n'\n",
      "          'limitations, either through fundamental improvements to\\n'\n",
      "          '3D GS or through brand-new architectures.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.4** **Improved Optimization Algorithms**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The optimization of 3D GS presents several challenges that\\n'\n",
      "          'affect the quality of reconstruction. These include issues\\n'\n",
      "          'with convergence speed, visual artifacts from improper\\n'\n",
      "          'Gaussians, and the need for better regularization during\\n'\n",
      "          'optimization. The raw optimization method (Sec. 3.2) might\\n'\n",
      "          'lead to overreconstruction in some regions while underrepresenting '\n",
      "          'others, resulting in blur and visual inconsistencies.\\n'\n",
      "          '\\n'\n",
      "          'Three main directions stand out for improving the optimization of '\n",
      "          '3D GS. **i) Additional Regularization** ( _e.g_ .,\\n'\n",
      "          'frequency [84] and geometry [22], [77]). Geometry-aware\\n'\n",
      "          'approaches have been particularly successful, preserving\\n'\n",
      "          'scene structure through the incorporation of local anchor\\n'\n",
      "          'points [22], depth and surface constraints [100]–[102], Gaussian '\n",
      "          'volumes [103], _etc_ . **ii) Optimization Procedure En-**\\n'\n",
      "          '**hancement** [44], [101], [104]. While the original strategy of\\n'\n",
      "          'density control (Sec. 3.2.2) has proven valuable, considerable room '\n",
      "          'for improvement remains. For example, GaussianPro [44] addresses '\n",
      "          'the challenge of dense initialization\\n'\n",
      "          'in texture-less surfaces and large-scale scenes through an\\n'\n",
      "          'advanced Gaussian densification strategy. **iii) Constraint**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Relaxation** . Reliance on external tools/algorithms can '\n",
      "          'introduce errors and cap the system’s performance potential. For\\n'\n",
      "          'instance, SfM, commonly used in the initialization process,\\n'\n",
      "          'is error-prone and struggle with complex scenes. Recent\\n'\n",
      "          'works have begun exploring COLMAP-free approaches\\n'\n",
      "          'utilizing stream continuity [81], [105], potentially enabling\\n'\n",
      "          'learning from internet-scale unposed video datasets.\\n'\n",
      "          '\\n'\n",
      "          'Though impressive, existing methods primarily concentrate on '\n",
      "          'optimizing Gaussians to accurately reconstruct\\n'\n",
      "          'scenes from scratch, neglecting a challenging yet promising\\n'\n",
      "          'solution which reconstructs scenes in a few-shot manner\\n'\n",
      "          'through established “meta representations”. Such solution\\n'\n",
      "          'could enable adaptive meta-learning strategies that combine\\n'\n",
      "          'scene-specific and general knowledge. See “learning physical priors '\n",
      "          'from large-scale data” in Sec. 7 for further insights.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.5** **3D Gaussian with More Properties**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Despite impressive, the properties of 3D Gaussian (Sec. 3.1)\\n'\n",
      "          'are designed to be used for novel-view synthesis only. By\\n'\n",
      "          'augmenting 3D Gaussian with additional properties, such as\\n'\n",
      "          'linguistic [87]–[89], semantic/instance [90]–[92], and '\n",
      "          'spatialtemporal [93] properties, 3D GS demonstrates its '\n",
      "          'considerable potential to revolutionize various domains.\\n'\n",
      "          'Here we list several interesting applications using 3D\\n'\n",
      "          'Gaussians with specially designed properties. **i** ) **Language**\\n'\n",
      "          '**Embedded Scene Representation** [87]–[89]. Due to the high\\n'\n",
      "          'computational and memory demands of current languageembedded scene '\n",
      "          'representations, Shi _et al_ . [87] proposed\\n'\n",
      "          'a quantization scheme that augments 3D Gaussian with\\n'\n",
      "          'streamlined language embeddings instead of the original '\n",
      "          'high-dimensional embeddings. This method also mitigated semantic '\n",
      "          'ambiguity and enhanced the precision of\\n'\n",
      "          'open-vocabulary querying by smoothing out semantic features across '\n",
      "          'different views, guided by uncertainty values.\\n'\n",
      "          '**ii** ) **Scene Understanding and Editing** [90]–[92]. Feature\\n'\n",
      "          '3DGS [90] integrated 3D GS with feature field distillation from 2D '\n",
      "          'foundation models. By learning a lowerdimensional feature field and '\n",
      "          'applying a lightweight convolutional decoder for upsampling, '\n",
      "          'Feature 3DGS achieved\\n'\n",
      "          'faster training and rendering speeds while enabling highquality '\n",
      "          'feature field distillation, supporting applications\\n'\n",
      "          'like semantic segmentation and language-guided editing.\\n'\n",
      "          '**iii** ) **Spatiotemporal Modeling** [93], [106]. To capture the\\n'\n",
      "          'complex spatial and temporal dynamics of 3D scenes, Yang\\n'\n",
      "          '_et al_ . [93] conceptualized spacetime as a unified entity and\\n'\n",
      "          'approximates the spatiotemporal volume of dynamic scenes\\n'\n",
      "          'using a collection of 4D Gaussians. The proposed 4D Gaussian '\n",
      "          'representation and corresponding rendering pipeline\\n'\n",
      "          'are capable of modeling arbitrary rotations in space and\\n'\n",
      "          'time and allow for end-to-end training.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.6** **Hybrid Representation**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Rather than augmenting 3D Gaussian with additional properties, '\n",
      "          'another promising avenue of adapting to downstream tasks is to '\n",
      "          'introduce structured information ( _e.g_ .,\\n'\n",
      "          'spatial MLPs and grids) tailored for specific applications.\\n'\n",
      "          'Next we showcase various fascinating uses of 3D GS\\n'\n",
      "          'with specially devised structured information. **i** ) **Facial '\n",
      "          'Ex-**\\n'\n",
      "          '**pression Modeling** . Considering the challenge of creating\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 8,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'high-fidelity 3D head avatars under sparse view conditions, '\n",
      "          'Gaussian Head Avatar [96] introduced controllable 3D\\n'\n",
      "          'Gaussians and an MLP-based deformation field. Concretely,\\n'\n",
      "          'it captured detailed facial expressions and dynamics by\\n'\n",
      "          'optimizing neutral 3D Gaussians alongside the deformation field, '\n",
      "          'thus ensuring both detail fidelity and expression\\n'\n",
      "          'accuracy. **ii** ) **Spatiotemporal Modeling** . Yang _et al_ . '\n",
      "          '[94]\\n'\n",
      "          'proposed to reconstruct dynamic scenes with deformable\\n'\n",
      "          '3D Gaussians. The deformable 3D Gaussians are learned in a\\n'\n",
      "          'canonical space, coupled with a deformation field ( _i.e_ ., a '\n",
      "          'spatial MLP) that models the spatial-temporal dynamics. The\\n'\n",
      "          'proposed method also incorporated an annealing smoothing training '\n",
      "          'mechanism to enhance temporal smoothness\\n'\n",
      "          'without additional computational costs. **iii** ) **Style '\n",
      "          'Transfer** .\\n'\n",
      "          'Saroha _et al_ . [107] proposed GS in style, an advanced approach '\n",
      "          'for real-time neural scene stylization. To maintain a\\n'\n",
      "          'cohesive stylized appearance across multiple views without\\n'\n",
      "          'compromising on rendering speed, they used pre-trained 3D\\n'\n",
      "          'Gaussians coupled with a multi-resolution hash grid and a\\n'\n",
      "          'small MLP to produce stylized views. In a nutshell, incorporating '\n",
      "          'structured information can serve as a complementary\\n'\n",
      "          'part for adapting to tasks that are incompatible with the\\n'\n",
      "          'sparsity and disorder of 3D Gaussians.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.7** **New Rendering Algorithm for 3D Gaussians**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'While the rasterization-based pipeline of 3D GS offers impressive '\n",
      "          'real-time performance, it still suffers from the inherent '\n",
      "          'limitations, including inefficient handling of highlydistorted '\n",
      "          'cameras (crucial for robotics), secondary rays (for\\n'\n",
      "          'optical effects like reflections and shadows), and stochastic\\n'\n",
      "          'ray sampling (needed in various existing pipelines). In\\n'\n",
      "          'addition, the assumptions that Gaussians do not overlap\\n'\n",
      "          'and can be sorted accurately using only centers are often\\n'\n",
      "          'violated in practice, leading to temporal artifacts when\\n'\n",
      "          'camera movement changes sorting order.\\n'\n",
      "          '\\n'\n",
      "          'Recent works [108]–[110] explored ray tracing based\\n'\n",
      "          'rendering algorithms as an alternative. For instance, '\n",
      "          'GaussianTracer [108] introduced a new ray tracing implementation '\n",
      "          'for Gaussian primitives, and devised several accelerating '\n",
      "          'strategies according to the uneven density and interleaved nature '\n",
      "          'of Gaussians. EVER [109] deivsed a physically\\n'\n",
      "          'accurate, constant density ellipsoid representation that allows for '\n",
      "          'the exact computation of the volume rendering integral, rather than '\n",
      "          'relying on somewhat satisfactory approximations. This advancement '\n",
      "          'eliminates popping artifacts.\\n'\n",
      "          '\\n'\n",
      "          'Thanks to the fundamental paradigm shift, several exciting '\n",
      "          'possibilities might emerge, including advanced optical\\n'\n",
      "          'effects (reflection, refraction, shadows, global illumination,\\n'\n",
      "          '_etc_ .), support for complex camera models (highly-distorted\\n'\n",
      "          'lenses, rolling shutter effects, _etc_ .), physically accurate '\n",
      "          'rendering with true directional appearance evaluation ( _vs_ . '\n",
      "          'tile\\n'\n",
      "          'based approximation), and more. While these capabilities\\n'\n",
      "          'currently come with additional computational costs, they\\n'\n",
      "          'provide essential building blocks for future research in\\n'\n",
      "          'inverse rendering, physical material modeling, relighting,\\n'\n",
      "          'and complex scene reconstruction.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5** **A** **PPLICATION** **A** **REAS AND** **T** **ASKS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Building on the rapid advancements in 3D GS, a wide range\\n'\n",
      "          'of innovative applications has emerged across multiple do\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'mains (Fig. 6) such as robotics (Sec. 5.1), dynamic scene\\n'\n",
      "          'reconstruction and representation (Sec. 5.2), generation and\\n'\n",
      "          'editing (Sec. 5.3), avatar (Sec. 5.4), medical systems (Sec. 5.5),\\n'\n",
      "          'large-scale scene reconstruction (Sec. 5.6), physics (Sec. 5.7),\\n'\n",
      "          'and even other scientific disciplines [24], [174]–[176]. Here,\\n'\n",
      "          'we highlight key examples that underscore the transformative impact '\n",
      "          'and potential of 3D GS and offer a more\\n'\n",
      "          '[comprehensive collection in '\n",
      "          'Github.](https://github.com/guikunchen/Awesome3DGS)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.1** **Robotics**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The evolution of scene representation in robotics has been\\n'\n",
      "          'profoundly shaped by the emergence of NeRF, which revolutionized '\n",
      "          'dense mapping and environmental interaction\\n'\n",
      "          'through implicit neural models. However, NeRF’s computational cost '\n",
      "          'poses a critical bottleneck for real-time robotic\\n'\n",
      "          'applications. The shift from implicit to explicit representation '\n",
      "          'not only accelerates optimization but also unlocks\\n'\n",
      "          'direct access to spatial and structural scene data, making 3D\\n'\n",
      "          'GS a transformative tool for robotics. Its ability to balance\\n'\n",
      "          'high-fidelity reconstruction with computational efficiency\\n'\n",
      "          'positions 3D GS as a cornerstone for advancing robotic\\n'\n",
      "          'perception, manipulation, and navigation in dynamic, realworld '\n",
      "          'environments.\\n'\n",
      "          '\\n'\n",
      "          'The integration of GS into robotic systems has yielded\\n'\n",
      "          'significant advancements across three core domains. In\\n'\n",
      "          '**SLAM**, GS-based methods [111]–[117], [123], [124], [177]–\\n'\n",
      "          '\\n'\n",
      "          '[182] excel in real-time dense mapping but face inherent\\n'\n",
      "          'trade-offs. Visual SLAM frameworks, particularly RGBD variants '\n",
      "          '[112], [114], [178], leverage depth supervision\\n'\n",
      "          'for geometric fidelity but falter in low-texture or motiondegraded '\n",
      "          'environments. RGB-only approaches [113], [115],\\n'\n",
      "          '\\n'\n",
      "          '[183] circumvent depth sensors but grapple with scale ambiguity and '\n",
      "          'drift. Multi-sensor fusion strategies, such as\\n'\n",
      "          'LiDAR integration [159], [177], [182], enhance robustness in\\n'\n",
      "          'unstructured settings at the cost of calibration complexity.\\n'\n",
      "          'Semantic SLAM [116], [117], [123] extends scene understanding '\n",
      "          'through object-level semantics but struggles with\\n'\n",
      "          'scalability due to lighting sensitivity in color-based methods\\n'\n",
      "          'or computational overhead in feature-based methods. 3D\\n'\n",
      "          'GS based **manipulation** [118]–[122] bypasses the need for\\n'\n",
      "          'auxiliary pose estimation in NeRF-based methods, enabling\\n'\n",
      "          'rapid single-stage tasks like grasping in static environments\\n'\n",
      "          'via geometric and semantic attributes encoded in Gaussian '\n",
      "          'properties. Multi-stage manipulation [118], [120], where\\n'\n",
      "          'environmental dynamics demand real-time map updates,\\n'\n",
      "          'requires explicit modeling of dynamic adjustments ( _e.g_ .,\\n'\n",
      "          'object motions and interactions), material compliance, _etc_ .\\n'\n",
      "          '\\n'\n",
      "          'The advancement of 3D GS in robotics faces three pivotal\\n'\n",
      "          'challenges. First, adaptability in dynamic and unstructured\\n'\n",
      "          'environments remains critical: real-world scenes are rarely\\n'\n",
      "          'static, requiring systems to continuously update representations '\n",
      "          'amid motion, occlusions, and sensor noise without sacrificing '\n",
      "          'accuracy. Second, current semantic mapping methods rely on costly, '\n",
      "          'scene-specific optimization processes,\\n'\n",
      "          'limiting generalizability and scalability for real-world '\n",
      "          'deployment. Third, unlike NeRF based systems which can\\n'\n",
      "          'use MLP parameters as input features for downstream\\n'\n",
      "          'decision-making, 3D Gaussians’ inherent lack of spatial\\n'\n",
      "          'order complicates feature aggregation, with no standardized\\n'\n",
      "          'framework yet established. Bridging the gap between high\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [{'bbox': Rect(332.0238952636719, 240.26486206054688, 489.68603515625, 324.9046630859375),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 306,\n",
      "              'number': 5,\n",
      "              'size': 61842,\n",
      "              'transform': (157.66213989257812,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            84.63980102539062,\n",
      "                            332.0238952636719,\n",
      "                            240.26486206054688),\n",
      "              'width': 570,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(119.4323501586914, 80.66507720947266, 224.21754455566406, 171.0806427001953),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 1120,\n",
      "              'number': 17,\n",
      "              'size': 297450,\n",
      "              'transform': (104.78519439697266,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            90.41556549072266,\n",
      "                            119.4323501586914,\n",
      "                            80.66507720947266),\n",
      "              'width': 1298,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(420.6471862792969, 80.72288513183594, 559.964599609375, 125.10945129394531),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 173,\n",
      "              'number': 21,\n",
      "              'size': 31227,\n",
      "              'transform': (139.3174285888672,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            44.386566162109375,\n",
      "                            420.6471862792969,\n",
      "                            80.72288513183594),\n",
      "              'width': 543,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(425.5146789550781, 244.84066772460938, 503.2488708496094, 322.5748596191406),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 796,\n",
      "              'number': 1,\n",
      "              'size': 80355,\n",
      "              'transform': (77.73419189453125,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            77.73419189453125,\n",
      "                            425.5146789550781,\n",
      "                            244.84066772460938),\n",
      "              'width': 796,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(275.435302734375, 257.9797058105469, 324.48870849609375, 323.8699035644531),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 407,\n",
      "              'number': 28,\n",
      "              'size': 67806,\n",
      "              'transform': (49.05339050292969,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            65.89019775390625,\n",
      "                            275.435302734375,\n",
      "                            257.9797058105469),\n",
      "              'width': 303,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(507.0722351074219, 255.87501525878906, 556.5585327148438, 316.983154296875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 326,\n",
      "              'number': 40,\n",
      "              'size': 25278,\n",
      "              'transform': (49.48631286621094,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            61.10813522338867,\n",
      "                            507.0722351074219,\n",
      "                            255.87501525878906),\n",
      "              'width': 264,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(51.070430755615234, 288.0701599121094, 106.21868896484375, 327.8208923339844),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 253,\n",
      "              'number': 3,\n",
      "              'size': 21399,\n",
      "              'transform': (55.14825439453125,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            39.75072479248047,\n",
      "                            51.070430755615234,\n",
      "                            288.0701599121094),\n",
      "              'width': 351,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(231.565673828125, 91.22856140136719, 263.7178649902344, 153.20867919921875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 160,\n",
      "              'number': 11,\n",
      "              'size': 6598,\n",
      "              'transform': (32.152183532714844,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            61.98012161254883,\n",
      "                            231.565673828125,\n",
      "                            91.22856140136719),\n",
      "              'width': 83,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(124.30836486816406, 256.5342712402344, 159.76148986816406, 300.11566162109375),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 252,\n",
      "              'number': 2,\n",
      "              'size': 10848,\n",
      "              'transform': (35.453121185302734,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            43.58139419555664,\n",
      "                            124.30836486816406,\n",
      "                            256.5342712402344),\n",
      "              'width': 205,\n",
      "              'xres': 96,\n",
      "              'yres': 96}],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 9,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Fig. 6. Typical applications benefited from GS (Sec. 5). Some '\n",
      "          'images are borrowed from [132], [135], [146], [154], [160], [166] '\n",
      "          'and redrawn.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'fidelity reconstruction and actionable semantic/physical '\n",
      "          'understanding will define the next frontier for 3D GS, moving\\n'\n",
      "          'beyond passive mapping towards embodied intelligence.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.2** **Dynamic Scene Reconstruction**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Dynamic scene reconstruction refers to the process of capturing and '\n",
      "          'representing the three-dimensional structure and\\n'\n",
      "          'appearance of a scene that changes over time [184]–[187].\\n'\n",
      "          'This involves creating a digital model that accurately reflects\\n'\n",
      "          'the geometry, motion, and visual aspects of the objects in\\n'\n",
      "          'the scene as they evolve. Dynamic scene reconstruction is\\n'\n",
      "          'crucial in various applications, _e.g_ ., VR/AR, 3D animation,\\n'\n",
      "          'and autonomous driving [188]–[190].\\n'\n",
      "          'The key to adapt 3D GS to dynamic scenes is the\\n'\n",
      "          'modeling of temporal dimension which allows for the\\n'\n",
      "          'representation of scenes that change over time. 3D GS\\n'\n",
      "          'based methods [93]–[95], [106], [125]–[130], [191]–[199] for\\n'\n",
      "          'dynamic scene reconstruction can generally be divided into\\n'\n",
      "          'two main categories as discussed in Sec. 4.5 and Sec. 4.6. The\\n'\n",
      "          'first category utilizes **additional fields** like spatial MLPs or\\n'\n",
      "          'grids to **model deformation** (Sec. 4.6). For example, Yang _et_\\n'\n",
      "          '_al_ . [94] first proposed deformable 3D Gaussians tailored for\\n'\n",
      "          'dynamic scenes. These 3D Gaussians are learned in a canonical space '\n",
      "          'and can be used to model spatial-temporal deformation with an '\n",
      "          'implicit deformation field (implemented\\n'\n",
      "          'as an MLP). GaGS [132] devised the voxelization of a set\\n'\n",
      "          'of Gaussian distributions, followed by the use of sparse\\n'\n",
      "          'convolutions to extract geometry-aware features, which are\\n'\n",
      "          'then utilized for deformation learning. On the other hand,\\n'\n",
      "          'the second category is based on the idea that scene changes\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'can be **encoded into the 3D Gaussian representation** with a\\n'\n",
      "          'specially designed rendering process (Sec. 4.5). For instance,\\n'\n",
      "          'Luiten _et al_ . [125] introduced dynamic 3D Gaussians to\\n'\n",
      "          'model dynamic scenes by keeping the properties of 3D\\n'\n",
      "          'Gaussians unchanged over time while allowing their positions and '\n",
      "          'orientations to change. Yang _et al_ . [93] designed a\\n'\n",
      "          '4D Gaussian representation, where additional properties are\\n'\n",
      "          'used to represent 4D rotations and spherindrical harmonics,\\n'\n",
      "          'to approximate the spatial-temporal volume of scenes.\\n'\n",
      "          '\\n'\n",
      "          'While 3D GS advances dynamic scene reconstruction by\\n'\n",
      "          'modeling per-Gaussian deformations, its reliance on finegrained '\n",
      "          'primitives limits scalability and robustness. Current\\n'\n",
      "          'methods struggle to balance computational efficiency and\\n'\n",
      "          'precision: small-scale reconstructions unify dynamic and\\n'\n",
      "          'static elements but become intractable in large environments, often '\n",
      "          'requiring manual priors to segment regions —–\\n'\n",
      "          'a barrier in unstructured settings. Furthermore, the absence\\n'\n",
      "          'of object-level motion reasoning leads to artifacts and poor\\n'\n",
      "          'generalization over long sequences. Future work might prioritize '\n",
      "          'object-centric frameworks that hierarchically group\\n'\n",
      "          'Gaussians into persistent entities, enabling efficient largescale '\n",
      "          'reconstruction through inherent motion disentanglement (dynamic '\n",
      "          '_vs_ . static).\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.3** **Generation and Editing**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Content generation and editing represent two fundamental\\n'\n",
      "          'and inherently interconnected capabilities in modern AI\\n'\n",
      "          'systems. While generation enables the synthesis of novel\\n'\n",
      "          'digital content from scratch or conditional inputs [200]–\\n'\n",
      "          '\\n'\n",
      "          '[202], editing provides the crucial ability to refine, adapt,\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 10,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'and manipulate existing content with precise control [203].\\n'\n",
      "          'Together, these capabilities revolutionize creative workflows\\n'\n",
      "          'by combining initial content creation with iterative refinement, '\n",
      "          'enabling applications from professional content production to '\n",
      "          'interactive consumer tools.\\n'\n",
      "          '\\n'\n",
      "          'Recent advances in generation [133]–[138], [204]–[227]\\n'\n",
      "          'have led to the emergence of three main approaches. **Op-**\\n'\n",
      "          '**timization** based methods [133], [134], [204] distill diffusion\\n'\n",
      "          'priors (gradients) to guide 3D model updates with the score\\n'\n",
      "          'functions. While these methods demonstrate impressive fidelity, '\n",
      "          'they face significant computational overhead due to\\n'\n",
      "          'the necessity of comparing multiple viewpoints during the\\n'\n",
      "          'optimization process. **Reconstruction** based methods [135],\\n'\n",
      "          '\\n'\n",
      "          '[225], [227] reframe the generation problem as a multiview '\n",
      "          'reconstruction task utilizing pre-trained multi-view\\n'\n",
      "          'diffusion models. Although this approach offers an intuitive\\n'\n",
      "          'and straightforward solution, it grapples with fundamental\\n'\n",
      "          'limitations in maintaining view consistency. The lack of\\n'\n",
      "          'strict geometric constraints across different viewpoints often\\n'\n",
      "          'results in inconsistent surface geometry and degraded texture '\n",
      "          'quality, particularly in regions with complex visual features. '\n",
      "          '**Direct 3D generation** methods train diffusion models\\n'\n",
      "          'on 3D representations [138], [220], [226]. While the learned\\n'\n",
      "          '3D diffusion models facilitate multi-view consistency, the\\n'\n",
      "          'demanding computational costs impede the expansion of\\n'\n",
      "          'training scales necessary for improved generative diversity.\\n'\n",
      "          '\\n'\n",
      "          'Current editing works [90]–[92], [126]–[128], [140]–[143],\\n'\n",
      "          '\\n'\n",
      "          '[228]–[239] fall into two primary classes. The first class '\n",
      "          'leverages **2D image-editing** models ( _e.g_ ., diffusion-based '\n",
      "          'editors)\\n'\n",
      "          'to iteratively refine 3D Gaussians. Early efforts [141], [142],\\n'\n",
      "          '\\n'\n",
      "          '[233] adopt optimization- or reconstruction-based strategies\\n'\n",
      "          'akin to methods in generation, but introduce task-specific\\n'\n",
      "          'control signals. However, naively applying 2D edits independently '\n",
      "          'across views often introduces multi-view inconsistencies. '\n",
      "          'Subsequent works [140], [238]–[240] mitigate this\\n'\n",
      "          'through iterative refinement or cross-view attention, albeit\\n'\n",
      "          'at increased computational costs for alignment. A notable\\n'\n",
      "          'challenge is unintended object deformations, attributed to\\n'\n",
      "          'the weak 3D geometric priors in 2D editing models and\\n'\n",
      "          'the difficulty of reconciling 2D edits with underlying 3D\\n'\n",
      "          'structures. The second class exploits the explicit nature of\\n'\n",
      "          '3D GS to enable **direct manipulation** based on embedded\\n'\n",
      "          'properties such as semantics [91], [92], [143], [232] and key\\n'\n",
      "          'points [128]. However, this class remains underexplored\\n'\n",
      "          'due to essentail challenges: the lack of inherent ordering\\n'\n",
      "          'of Gaussians complicates the design of efficient indexing\\n'\n",
      "          'schemes, while editing attributes ( _e.g_ ., texture and geometry) '\n",
      "          'requires careful regularization and alignment to preserve '\n",
      "          'plausibility.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.4** **Avatar**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Avatars, the digital representations of users in virtual\\n'\n",
      "          'spaces, bridge physical and digital realms, enabling immersive '\n",
      "          'interaction, identity expression, and remote collaboration. '\n",
      "          'Spanning entertainment (gaming, virtual influencers),\\n'\n",
      "          'enterprise (AI agents, virtual meetings), healthcare, and '\n",
      "          'education, they underpin metaverse economies. Advances in\\n'\n",
      "          'AR and VR amplify their role in redefining social, industrial,\\n'\n",
      "          'and creative landscapes.\\n'\n",
      "          '\\n'\n",
      "          '3D GS has emerged as a powerful tool for human avatar\\n'\n",
      "          'reconstruction, primarily advancing along two directions:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'full-body modeling and head-centric modeling. For **full-**\\n'\n",
      "          '**body avatars** [139], [144]–[147], [241]–[252], the current\\n'\n",
      "          'methods typically anchor 3D Gaussians in a canonical space\\n'\n",
      "          'and deform them via parametric body models ( _e.g_ ., SMPL)\\n'\n",
      "          'or cage-based rigging to model dynamic motions. These\\n'\n",
      "          'approaches adopt a hybrid deformation strategy: linear\\n'\n",
      "          'blend skinning handles rigid skeletal transformations such\\n'\n",
      "          'as joint rotations, while pose-conditioned deformation fields\\n'\n",
      "          'account for secondary non-rigid effects like muscle jiggles.\\n'\n",
      "          'For **head avatars** [23], [148]–[151], [253]–[256], the emphasis\\n'\n",
      "          'shifts to modeling intricate facial expressions, fine-grained\\n'\n",
      "          'geometry (e.g., wrinkles, hair [257]), and dynamic speechdriven '\n",
      "          'animations. Techniques mainly combine parametric\\n'\n",
      "          'morphable face models ( _e.g_ ., FLAME) with deformable 3D\\n'\n",
      "          'Gaussians, employing diffusion strategies and expressionaware '\n",
      "          'deformation fields to disentangle rigid head poses\\n'\n",
      "          'from non-rigid facial movements. Both directions exploit\\n'\n",
      "          'the speed advantage and editability of 3D GS to enable\\n'\n",
      "          'efficient training, real-time rendering, and precise control\\n'\n",
      "          'over deformations, while addressing challenges in crossframe '\n",
      "          'correspondence, topology flexibility, and multi-view\\n'\n",
      "          'consistency.\\n'\n",
      "          '\\n'\n",
      "          'Reconstruction in challenging scenes ( _e.g_ ., occlusions,\\n'\n",
      "          'sparse single-view inputs, or loose clothing) and enhancing\\n'\n",
      "          'avatar interactivity represent critical challenges and '\n",
      "          'opportunities. Parametric model-free methods, which bypass '\n",
      "          'predefined priors by learning skinning weights directly from\\n'\n",
      "          'data, show promise for such scenarios. Complementary to\\n'\n",
      "          'this, generative models can mitigate ambiguities inherent\\n'\n",
      "          'in underconstrained settings. Further integrating physicsbased '\n",
      "          'constraints might bridge the gap between static reconstructions and '\n",
      "          'responsive, lifelike interactions, unlocking\\n'\n",
      "          'applications in AR, embodied AI, _etc_ .\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.5** **Endoscopic Scene Reconstruction**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Surgical 3D reconstruction represents a fundamental task\\n'\n",
      "          'in robot-assisted minimally invasive surgery, aimed at enhancing '\n",
      "          'intraoperative navigation, preoperative planning,\\n'\n",
      "          'and educational simulations through precise modeling of\\n'\n",
      "          'dynamic surgical scenes. Pioneering the integration of dynamic '\n",
      "          'radiance fields into this domain, recent advancements\\n'\n",
      "          'have focused on surmounting the inherent challenges of\\n'\n",
      "          'single-viewpoint video reconstructions such as occlusions\\n'\n",
      "          'by surgical instruments and sparse viewpoint diversity\\n'\n",
      "          'within the confined spaces of endoscopic exploration [258]–\\n'\n",
      "          '\\n'\n",
      "          '[260]. Despite the progress, the call for high fidelity in tissue\\n'\n",
      "          'deformability and topological variation remains, coupled\\n'\n",
      "          'with the pressing demand for faster rendering to bridge\\n'\n",
      "          'the utility in applications sensitive to latency [152]–[154].\\n'\n",
      "          'This synthesis of immediacy and precision in reconstructing\\n'\n",
      "          'deformable tissues from endoscopic videos is essential in\\n'\n",
      "          'propelling robotic surgery towards reduced patient trauma\\n'\n",
      "          'and AR/VR applications, ultimately fostering a more intuitive '\n",
      "          'surgical environment and nurturing the future of\\n'\n",
      "          'surgical automation and robotic proficiency.\\n'\n",
      "          'Endoscopic scene reconstruction introduces distinct\\n'\n",
      "          'challenges compared to general dynamic scenes, including\\n'\n",
      "          'sparse training data from limited camera mobility in narrow\\n'\n",
      "          'cavities, frequent tool occlusions obscuring critical regions,\\n'\n",
      "          'and single-view geometry ambiguities. Existing approaches\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 11,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'mainly used additional depth guidance to infer the geometry of '\n",
      "          'tissues [152]–[154]. For instance, EndoGS [154]\\n'\n",
      "          'integrated depth-guided supervision with spatial-temporal\\n'\n",
      "          'weight masks and surface-aligned regularization terms to\\n'\n",
      "          'enhance the quality and speed of 3D tissue rendering\\n'\n",
      "          'while addressing tool occlusion. EndoGaussian [153] introduced two '\n",
      "          'new strategies: holistic Gaussian initialization for\\n'\n",
      "          'dense initialization and spatiotemporal Gaussian tracking\\n'\n",
      "          'for modeling surface dynamics. Zhao _et al_ . [155] argued that\\n'\n",
      "          'these methods suffer from under-reconstruction and proposed to '\n",
      "          'alleviate this problem from frequency perspectives.\\n'\n",
      "          'In addition, EndoGSLAM [156] and Gaussian Pancake [157]\\n'\n",
      "          'devised SLAM systems for endoscopic scenes and showed\\n'\n",
      "          'significant speed advantages.\\n'\n",
      "          '\\n'\n",
      "          'Advancing endoscopic 3D reconstruction requires targeted efforts in '\n",
      "          'both data and dynamics modeling. Data limitations arise from '\n",
      "          'single-viewpoint videos, which produce\\n'\n",
      "          'ill-posed reconstruction problems due to instrument occlusions and '\n",
      "          'constrained camera mobility, leaving critical tissue\\n'\n",
      "          'regions unobserved. While depth estimators provide temporary '\n",
      "          'workarounds, integrating multi-view camera systems\\n'\n",
      "          'addresses the root cause. In addition, existing datasets often\\n'\n",
      "          'feature truncated sequences ( _e.g_ ., 4 _∼_ 8 _s_ in EndoNeRF '\n",
      "          '[258]),\\n'\n",
      "          'which fail to capture prolonged tissue deformation dynamics or '\n",
      "          'complex surgical workflows. Extending temporal coverage to include '\n",
      "          'longer, clinically representative sequences\\n'\n",
      "          'would benefit downstream applications as aforementioned.\\n'\n",
      "          'Modeling limitations persist in current methods, which often '\n",
      "          'represent tissue dynamics at the Gaussian level rather\\n'\n",
      "          'than object- or 3D region-level. This reduces their capacity\\n'\n",
      "          'to encode semantically meaningful anatomical interactions\\n'\n",
      "          'and deserves further explorations.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.6** **Large-scale Scene Reconstruction**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Large-scale scene reconstruction is a critical component in\\n'\n",
      "          'fields such as autonomous driving, aerial surveying, and\\n'\n",
      "          'AR/VR, demanding both photorealistic visual quality and\\n'\n",
      "          'real-time rendering capabilities. Before the emergence of\\n'\n",
      "          '3D GS, the task has been approached using NeRF based\\n'\n",
      "          'methods, which, while effective for smaller scenes, often fall\\n'\n",
      "          'short in detail and rendering speed when scaled to larger\\n'\n",
      "          'areas ( _e.g_ ., over 1.5 _km_ [2] ). Though 3D GS has '\n",
      "          'demonstrated\\n'\n",
      "          'considerable advantages over NeRFs, the direct application\\n'\n",
      "          'of 3D GS to large-scale environments introduces significant '\n",
      "          'challenges. 3D GS requires an immense number of\\n'\n",
      "          'Gaussians to maintain visual quality over extensive areas,\\n'\n",
      "          'leading to prohibitive GPU memory demands and considerable '\n",
      "          'computational burdens during rendering. For instance,\\n'\n",
      "          'a scene spanning 2.7 _km_ [2] may require over 20 million\\n'\n",
      "          'Gaussians, pushing the limits of even the most advanced\\n'\n",
      "          'hardware ( _e.g_ ., NVIDIA A100 with 40GB memory) [163].\\n'\n",
      "          'To address the highlighted challenges, researchers have\\n'\n",
      "          'made significant strides in two key areas: **i** ) For '\n",
      "          '**training**, a\\n'\n",
      "          'divide-and-conquer strategy [162]–[165] has been adopted,\\n'\n",
      "          'which segments a large scene into multiple, independent\\n'\n",
      "          'cells. This facilitates parallel optimization for expansive\\n'\n",
      "          'environments. With the same spirit, Zhao _et al_ . [161] proposed a '\n",
      "          'distributed implementation of 3D GS training. An\\n'\n",
      "          'additional challenge lies in maintaining visual quality, as\\n'\n",
      "          'large-scale scenes often feature texture-less surfaces that can\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'hamper the effectiveness of optimization such as Gaussian\\n'\n",
      "          'initialization and density control (Sec. 3.2). Enhancing the\\n'\n",
      "          'optimization algorithm presents a viable solution to mitigate this '\n",
      "          'issue [44], [164]. **ii** ) Regarding **rendering**, the\\n'\n",
      "          'adoption of the Level of Details (LoD) technique from\\n'\n",
      "          'computer graphics has proven instrumental. LoD adjusts\\n'\n",
      "          'the complexity of 3D scenes to balance visual quality with\\n'\n",
      "          'computational efficiency. Current implementations involve\\n'\n",
      "          'feeding only the essential Gaussians to the rasterizer [164],\\n'\n",
      "          'or designing explicit LoD structures like the Octree [165]\\n'\n",
      "          'and hierarchy [162]. Furthermore, integrating extra input\\n'\n",
      "          'modalities like LiDAR can further enhanced the reconstruction '\n",
      "          'process [158]–[160].\\n'\n",
      "          'One prominent challenge in large-scale scene reconstruction lies in '\n",
      "          'handling sparse or incomplete capture\\n'\n",
      "          'data, which can be mitigated through few-shot adaptation\\n'\n",
      "          'schemes (see Sec. 4.1) or generalizable priors (see “learning\\n'\n",
      "          'physical priors from large-scale data” in Sec. 7). Meanwhile,\\n'\n",
      "          'memory and computational bottlenecks can be addressed\\n'\n",
      "          'via distributed learning strategies [161], such as parameter\\n'\n",
      "          'partitioning across GPU clusters and parallel batched multiview '\n",
      "          'optimization.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.7** **Physics**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The simulation of complex real-world dynamics, such as\\n'\n",
      "          'seed dispersal or fluid motion, is pivotal for applications\\n'\n",
      "          'spanning virtual reality, animation, and scientific modeling,\\n'\n",
      "          'where realism hinges on accurate physical behavior. Advances in '\n",
      "          'video diffusion models have driven progress in\\n'\n",
      "          '4D content generation, yet these methods might produce\\n'\n",
      "          'visually plausible results that violate fundamental physical\\n'\n",
      "          'laws. 3D GS emerges as a promising solution by embedding\\n'\n",
      "          'physical constraints and properties into scene representations, '\n",
      "          'enabling both visually convincing and physically\\n'\n",
      "          'coherent simulations.\\n'\n",
      "          '\\n'\n",
      "          'Existing methods differ in how they formulate and integrate '\n",
      "          'physics-based priors into their frameworks. The most\\n'\n",
      "          'common approach is employing physics simulation engines\\n'\n",
      "          '( _e.g_ ., MLS-MPM [268]) to guide the dynamics generation.\\n'\n",
      "          'The material point method [268] and position based dynamics [269] — '\n",
      "          'numerical methods used in computer graphics\\n'\n",
      "          'for simulating deformations in materials like fluids, granular '\n",
      "          'media, and fracturing solids — have been extensively\\n'\n",
      "          'explored by the community through various customizations [21], '\n",
      "          '[143], [166]–[171]. Analytical material models,\\n'\n",
      "          'such as mass-spring systems, have also demonstrated success in '\n",
      "          'approximating deformations by explicitly encoding\\n'\n",
      "          'material properties into 3D Gaussians [172]. Across these\\n'\n",
      "          'methods, 3D Gaussians are treated as discrete particles (with\\n'\n",
      "          'one exception [173] using a continuous representation) and\\n'\n",
      "          'serve as computational units within the chosen simulator.\\n'\n",
      "          'Unknown material properties or physical parameters are\\n'\n",
      "          'typically learned through video-based supervision from\\n'\n",
      "          'conditional generative models.\\n'\n",
      "          '\\n'\n",
      "          'Despite advancements in physics based 3D GS frameworks, critical '\n",
      "          'limitations persist. Current systems struggle\\n'\n",
      "          'to unify diverse physical behaviors ( _e.g_ ., rigid, elastic, or\\n'\n",
      "          'soft-body dynamics) into cohesive simulations, handle complex '\n",
      "          'multi-object interactions without manual intervention,\\n'\n",
      "          'and model scene-level interactions such as environmental\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 12,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [{'bbox': (55.273075103759766,\n",
      "                       75.0974349975586,\n",
      "                       557.890380859375,\n",
      "                       183.87701416015625),\n",
      "              'columns': 4,\n",
      "              'rows': 2},\n",
      "             {'bbox': (55.273075103759766,\n",
      "                       232.72474670410156,\n",
      "                       298.72357177734375,\n",
      "                       574.1319580078125),\n",
      "              'columns': 3,\n",
      "              'rows': 8}],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 1\\n'\n",
      "          'Comparison of **localization** methods (§6.1) on Replica [261] '\n",
      "          '(static scenes), in terms of absolute trajectory error (ATE, cm). '\n",
      "          '(The three best\\n'\n",
      "          'scores are marked in **red**, **blue**, and **green**, '\n",
      "          'respectively. These notes also apply to the other tables.)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|Room0 Room1 Room2 Office0 Office1 Office2 Offci e3 '\n",
      "          'Office4|Avarage|\\n'\n",
      "          '|---|---|---|---|\\n'\n",
      "          '|iMAP [262]<br>[ICCV21]<br>Vox-Fusion '\n",
      "          '[263]<br>[ISMAR22]<br>NICE-SLAM [264]<br>[CVPR22]<br>ESLAM '\n",
      "          '[265]<br>[CVPR23]<br>Point-SLAM [266]<br>[ICCV23]<br>Co-SLAM '\n",
      "          '[267]<br>[CVPR23]<br>Gaussian-SLAM [114]<br>[arXiv]<br>GSSLAM '\n",
      "          '[113]<br>[CVPR24]<br>GS-SLAM [111]<br>[CVPR24]<br>SplaTAM '\n",
      "          '[112]<br>[CVPR24]|✓<br>✓<br>✓<br>✓|3.12<br>2.54<br>2.31<br>1.69<br>1.03<br>3.99<br>4.05<br>1.93<br>1.37<br>4.70<br>1.47<br>8.48<br>2.04<br>2.58<br>1.11<br>2.94<br>0.97<br>1.31<br>1.07<br>0.88<br>1.00<br>1.06<br>1.10<br>1.13<br>0.71<br>0.70<br>0.52<br>0.57<br>0.55<br>0.58<br>0.72<br>0.63<br>0.61<br>0.41<br>0.37<br>0.38<br>0.48<br>0.54<br>0.69<br>0.72<br>0.70<br>0.95<br>1.35<br>0.59<br>0.55<br>2.03<br>1.56<br>0.72<br>3.35<br>8.74<br>3.13<br>1.11<br>0.81<br>0.78<br>1.08<br>7.21<br>0.47<br>0.43<br>0.31<br>0.70<br>0.57<br>0.31<br>0.31<br>3.20<br>0.48<br>0.53<br>0.33<br>0.52<br>0.41<br>0.59<br>0.46<br>0.70<br>0.31<br>0.40<br>0.29<br>0.47<br>0.27<br>0.29<br>0.32<br>0.55|2.58<br>3.09<br>1.06<br>0.63<br>0.52<br>1.00<br>3.27<br>0.79<br>0.50<br>0.36|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 2\\n'\n",
      "          'Collection of representative datasets for 3D GS. Here PC '\n",
      "          'represents\\n'\n",
      "          'point clouds.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Name|Type # Sample|Task|\\n'\n",
      "          '|---|---|---|\\n'\n",
      "          '|Tanks&Temples [270][TOG17]<br>RealEstate10K '\n",
      "          '[271][TOG18]<br>DeepBlending [272][TOG18]<br>LLFF '\n",
      "          '[273][TOG19]<br>NeRF [12][ECCV20]<br>ACID [274][ICCV21]<br>Mip-NeRF '\n",
      "          '360 '\n",
      "          '[8][CVPR22]|RGB<br>14<br>RGB<br>1,000<br>RGB<br>19<br>RGB<br>8<br>RGB<br>8<br>RGB<br>700+<br>RGB<br>9|Novel '\n",
      "          'View<br>Synthesis|\\n'\n",
      "          '|TUM RGB-D [275][IROS12]<br>KITTI [276][CVPR12]<br>ScanNet '\n",
      "          '[277][CVPR17]<br>Replica [261][arXiv19]<br>Waymo '\n",
      "          '[278][CVPR20]<br>nuScenes [279][CVPR20]<br>RLBench '\n",
      "          '[280][RA-L20]<br>Robomimic '\n",
      "          '[281][CoRL22]|RGB-D<br>39<br>RGB-D&PC<br>11<br>RGB-D<br>1,513<br>RGB-D<br>18<br>RGB-D&PC<br>1,150<br>RGB-D&PC<br>1,000<br>RGB<br>100<br>RGB<br>800|Robotics|\\n'\n",
      "          '|D-NeRF [184][CVPR21]<br>HyperNeRF [185][TOG21]<br>NeRF-DS '\n",
      "          '[282][CVPR23]|RGB<br>8<br>RGB<br>6<br>RGB<br>8|Dynamic '\n",
      "          'Scene<br>Reconstruction|\\n'\n",
      "          '|CoNeRF [283][CVPR22]<br>SPIn-NeRF [284][CVPR23]<br>Tensor4D '\n",
      "          '[285][CVPR23]<br>OmniObject3D [286][CVPR23]<br>Objaverse '\n",
      "          '[287][CVPR23]|RGB<br>7<br>RGB<br>10<br>RGB<br>4<br>3D '\n",
      "          'Object<br>6,000<br>3D Object<br>800K+|Generation<br>and Editing|\\n'\n",
      "          '|People-Snapshot [288][CVPR18]<br>VOCASET [289][CVPR19]<br>THUman '\n",
      "          '[290][ICCV19]<br>THUman2.0 [291][CVPR21]<br>ZJU-Mocap '\n",
      "          '[292][CVPR21]<br>H3DS [293][ICCV21]<br>THUman3.0 '\n",
      "          '[294][TPAMI22]|RGB<br>24<br>RGB<br>12<br>RGB<br>200<br>RGB-D<br>500<br>RGB<br>9<br>RGB<br>23<br>3D '\n",
      "          'Scan<br>20|Avatar|\\n'\n",
      "          '|SCARED [295][MICCAI19]<br>EndoNeRF [258][MICCAI22]<br>X3D '\n",
      "          '[296][CVPR24]|RGB-D<br>9<br>RGB<br>2<br>X-ray<br>15|Medical|\\n'\n",
      "          '|CityNeRF [297][ECCV22]<br>Waymo Block-NeRF '\n",
      "          '[298][CVPR22]<br>UrbanBIS [299][SIGGR23]<br>GauU-Scene '\n",
      "          '[160][arXiv24]|RGB<br>12<br>RGB&PC<br>1<br>RGB&PC<br>6<br>RGB&PC<br>1|Large-scale<br>Reconstrction|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'feedback and dynamic lighting changes. Integrating adaptive physics '\n",
      "          'engines capable of multi-object and multimaterial interactions, '\n",
      "          'developing new simulation architectures that are compatible with '\n",
      "          'priors learned from largescale data, and expanding datasets to '\n",
      "          'encompass diverse\\n'\n",
      "          'materials and dynamic scenarios are equally vital.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6** **P** **ERFORMANCE** **C** **OMPARISON**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'In this section, we provide more empirical evidence by\\n'\n",
      "          'presenting the performance of several 3D GS algorithms\\n'\n",
      "          'that we previously discussed. The diverse applications of\\n'\n",
      "          '3D GS across numerous tasks, coupled with the customtailored '\n",
      "          'algorithmic designs for each task, render a uniform\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'comparison of all 3D GS algorithms across a single task or\\n'\n",
      "          'dataset impracticable. For comprehensiveness, we provide a\\n'\n",
      "          'collection of representative datasets in Table 2 according to\\n'\n",
      "          'our analysis in Sec. 5. Due to the limited space, we have chosen '\n",
      "          'several representative tasks for an in-depth performance\\n'\n",
      "          'evaluation. The performance scores are primarily sourced\\n'\n",
      "          'from the original papers, except where indicated otherwise.\\n'\n",
      "          '[We also maintain a Github repository for this '\n",
      "          'section.](https://github.com/guikunchen/3DGS-Benchmarks)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.1** **Performance Benchmarking: Localization**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The localization task in SLAM involves determining the\\n'\n",
      "          'precise position and orientation of a robot or device within\\n'\n",
      "          'an environment, typically using sensor data.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** Replica [261] dataset is a collection of 18 '\n",
      "          'highly\\n'\n",
      "          'detailed 3D indoor scenes. These scenes are not only visually\\n'\n",
      "          'realistic but also offer comprehensive data including dense\\n'\n",
      "          'meshes, high-quality HDR textures, and detailed semantic\\n'\n",
      "          'information for each element. Following [262], three sequences '\n",
      "          'about rooms and five sequences about offices are\\n'\n",
      "          'used for the evaluation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve four recent 3D GS based algorithms [111]–[114]\\n'\n",
      "          'and six typical SLAM methods [262]–[267].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** The root mean square error (RMSE)\\n'\n",
      "          'of the absolute trajectory error (ATE) is a commonly used\\n'\n",
      "          'metric in evaluating SLAM systems [275], which measures\\n'\n",
      "          'the root mean square of the Euclidean distances between the\\n'\n",
      "          'estimated and true positions over the entire trajectory.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** As shown in Table 1, the recent 3D Gaussians\\n'\n",
      "          'based localization algorithms have a clear advantage over\\n'\n",
      "          'existing NeRF based dense visual SLAM. For example,\\n'\n",
      "          'SplaTAM [112] achieves a trajectory error improvement of\\n'\n",
      "          '_∼_ **50** %, decreasing it from 0.52cm to **0.36cm** compared to\\n'\n",
      "          'the previous state-of-the-art (SOTA) [266]. We attribute this\\n'\n",
      "          'to the dense and accurate 3D Gaussians reconstructed for\\n'\n",
      "          '\\n'\n",
      "          'scenes, which can handle the noise of real sensors. This\\n'\n",
      "          'reveals that effective scene representations can improve the\\n'\n",
      "          'accuracy of localization tasks.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.2** **Performance Benchmarking: Static Scenes**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Rendering focuses on transforming computer-readable information ( '\n",
      "          '_e.g_ ., 3D objects in the scene) to pixel-based\\n'\n",
      "          'images. This section focuses on evaluating the quality of\\n'\n",
      "          'rendering results in static scenes.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** The same dataset as in Sec. 6.1, _i.e_ ., Replica '\n",
      "          '[261],\\n'\n",
      "          'is used for comparison. The testing views are the same as\\n'\n",
      "          'those collected by [262].\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 13,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [{'bbox': (63.6470832824707,\n",
      "                       75.12527465820312,\n",
      "                       550.2225341796875,\n",
      "                       281.9548645019531),\n",
      "              'columns': 6,\n",
      "              'rows': 8},\n",
      "             {'bbox': (318.8070068359375,\n",
      "                       339.8204650878906,\n",
      "                       562.5723266601562,\n",
      "                       466.28314208984375),\n",
      "              'columns': 3,\n",
      "              'rows': 2}],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 13\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 3\\n'\n",
      "          'Comparison of **mapping** methods (§6.2) on Replica [261] (static '\n",
      "          'scenes), in terms of PSNR, SSIM, and LPIPS. The results for FPS are '\n",
      "          'taken\\n'\n",
      "          'from [113] using one 4090 GPU.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|Metric|Room0 Room1 Room2 Office0 Office1 Office2 Office3 '\n",
      "          'Office4|Avarage|FPS|\\n'\n",
      "          '|---|---|---|---|---|---|\\n'\n",
      "          '|NICE-SLAM [264] '\n",
      "          '[CVPR22]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|22.12<br>22.47<br>24.52<br>29.07<br>30.34<br>19.66<br>22.23<br>24.94<br>0.69<br>0.76<br>0.81<br>0.87<br>0.89<br>0.80<br>0.80<br>0.86<br>0.33<br>0.27<br>0.21<br>0.23<br>0.18<br>0.23<br>0.21<br>0.20|24.42<br>0.81<br>0.23|0.54|\\n'\n",
      "          '|Vox-Fusion [263] '\n",
      "          '[ISMAR22]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|22.39<br>22.36<br>23.92<br>27.79<br>29.83<br>20.33<br>23.47<br>25.21<br>0.68<br>0.75<br>0.80<br>0.86<br>0.88<br>0.79<br>0.80<br>0.85<br>0.30<br>0.27<br>0.23<br>0.24<br>0.18<br>0.24<br>0.21<br>0.20|24.41<br>0.80<br>0.24|2.17|\\n'\n",
      "          '|Point-SLAM [266] '\n",
      "          '[ICCV23]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|32.40<br>34.08<br>35.50<br>38.26<br>39.16<br>33.99<br>33.48<br>33.49<br>0.97<br>0.98<br>0.98<br>0.98<br>0.99<br>0.96<br>0.96<br>0.98<br>0.11<br>0.12<br>0.11<br>0.10<br>0.12<br>0.16<br>0.13<br>0.14|35.17<br>0.97<br>0.12|1.33|\\n'\n",
      "          '|SplaTAM [112] '\n",
      "          '[CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|32.86<br>33.89<br>35.25<br>38.26<br>39.17<br>31.97<br>29.70<br>31.81<br>0.98<br>0.97<br>0.98<br>0.98<br>0.98<br>0.97<br>0.95<br>0.95<br>0.07<br>0.10<br>0.08<br>0.09<br>0.09<br>0.10<br>0.12<br>0.15|34.11<br>0.97<br>0.10|-|\\n'\n",
      "          '|GS-SLAM [111] '\n",
      "          '[CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|31.56<br>32.86<br>32.59<br>38.70<br>41.17<br>32.36<br>32.03<br>32.92<br>0.97<br>0.97<br>0.97<br>0.99<br>0.99<br>0.98<br>0.97<br>0.97<br>0.09<br>0.07<br>0.09<br>0.05<br>0.03<br>0.09<br>0.11<br>0.11|34.27<br>0.97<br>0.08|-|\\n'\n",
      "          '|GSSLAM [113] '\n",
      "          '[CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|34.83<br>36.43<br>37.49<br>39.95<br>42.09<br>36.24<br>36.70<br>36.07<br>0.95<br>0.96<br>0.96<br>0.97<br>0.98<br>0.96<br>0.96<br>0.96<br>0.07<br>0.08<br>0.07<br>0.07<br>0.06<br>0.08<br>0.07<br>0.10|37.50<br>0.96<br>0.07|769|\\n'\n",
      "          '|Gaussian-SLAM '\n",
      "          '[114][arXiv]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|34.31<br>37.28<br>38.18<br>43.97<br>43.56<br>37.39<br>36.48<br>40.19<br>0.99<br>0.99<br>0.99<br>1.00<br>0.99<br>0.99<br>0.99<br>1.00<br>0.08<br>0.07<br>0.07<br>0.04<br>0.07<br>0.08<br>0.08<br>0.07|38.90<br>0.99<br>0.07|-|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve four recent papers which introduce 3D Gaussians into '\n",
      "          'their systems [111]–[114], as well as three dense\\n'\n",
      "          'SLAM methods [263], [264], [266].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** Peak signal-to-noise ratio (PSNR),\\n'\n",
      "          'structural similarity (SSIM) [300], and learned perceptual\\n'\n",
      "          'image patch similarity (LPIPS) [301] are used for measuring\\n'\n",
      "          'RGB rendering performance.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** Table 3 shows that 3D Gaussians based systems\\n'\n",
      "          'generally outperform the three dense SLAM competitors.\\n'\n",
      "          'For example, Gaussian-SLAM [114] establishes new SOTA\\n'\n",
      "          'and outperforms previous methods by a large margin.\\n'\n",
      "          'Compared to Point-SLAM [266], GSSLAM [113] is about\\n'\n",
      "          '**578** times faster in achieving very competitive accuracy.\\n'\n",
      "          'In contrast to previous method [266] that relies on depth\\n'\n",
      "          'information, such as depth-guided ray sampling, for synthesizing '\n",
      "          'novel views, 3D GS based system eliminates this\\n'\n",
      "          'need, allowing for high-fidelity rendering for any views.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.3** **Performance Benchmarking: Dynamic Scenes**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'This section focuses on evaluating the rendering quality in\\n'\n",
      "          'dynamic scenes.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** D-NeRF [184] dataset includes videos with 50\\n'\n",
      "          'to 200 frames each, captured from unique viewpoints. It\\n'\n",
      "          'features synthetic, animated objects in complex scenes, with\\n'\n",
      "          'non-Lambertian materials. The dataset provides 50 to 200\\n'\n",
      "          'training images and 20 test images per scene, designed for\\n'\n",
      "          'evaluating models in the monocular setting. The testing\\n'\n",
      "          'views are the same as the original paper [184].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve five recent papers that model dynamic scenes\\n'\n",
      "          'with 3D GS [93]–[95], [126], [132], as well as six NeRF based\\n'\n",
      "          'approaches [37], [184], [187], [302]–[304].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** The same metrics as in Sec. 6.2, _i.e_ '\n",
      "          '.,\\n'\n",
      "          'PSNR, SSIM [300], and LPIPS [301], are used for evaluation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** From Table 4 we can observe that 3D GS based\\n'\n",
      "          'methods outperform existing SOTAs by a clear margin. The\\n'\n",
      "          'static version of 3D GS [10] fails to reconstruct dynamic\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 4\\n'\n",
      "          'Comparison of **reconstruction** methods (§6.3) on D-NeRF [184]\\n'\n",
      "          '(dynamic scenes), in terms of PSNR, SSIM, and LPIPS. _[∗]_ denotes\\n'\n",
      "          'results reported in [95].\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|PSNR↑ SSIM↑ LPIPS↓|\\n'\n",
      "          '|---|---|---|\\n'\n",
      "          '|D-NeRF [184]<br>[CVPR21]<br>TiNeuVox-B [302]<br>[SGA22]<br>KPlanes '\n",
      "          '[37]<br>[CVPR23]<br>HexPlane-Slim [303]<br>[CVPR23]<br>FFDNeRF '\n",
      "          '[187]<br>[ICCV23]<br>MSTH [304]<br>[NeurIPS23]<br>3D '\n",
      "          'GS_∗_[10]<br>[TOG23]<br>4DGS [93]<br>[ICLR24]<br>4D-GS '\n",
      "          '[95]<br>[CVPR24]<br>GaGS [132]<br>[CVPR24]<br>CoGS '\n",
      "          '[126]<br>[CVPR24]<br>D-3DGS '\n",
      "          '[94]<br>[CVPR24]|✓<br>✓<br>✓<br>✓<br>✓<br>✓|30.50<br>0.95<br>0.07<br>32.67<br>0.97<br>0.04<br>31.61<br>0.97<br>-<br>32.68<br>0.97<br>0.02<br>32.68<br>0.97<br>0.02<br>31.34<br>0.98<br>0.02<br>23.19<br>0.93<br>0.08<br>34.09<br>0.98<br>-<br>34.05<br>0.98<br>0.02<br>37.36<br>0.99<br>0.01<br>37.90<br>0.98<br>0.02<br>39.51<br>0.99<br>0.01|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'scenes, resulting in a sharp drop in performance. By modeling the '\n",
      "          'dynamics, D-3DGS [94] outperforms the SOTA\\n'\n",
      "          'method, FFDNeRF [187], by **6.83** dB in terms of PSNR. These\\n'\n",
      "          'results indicate the effectiveness of introducing additional\\n'\n",
      "          'properties or structured information to model the deformation of '\n",
      "          'Gaussians so as to model the scene dynamics.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.4** **Performance Benchmarking: Human Avatar**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Human avatar modeling aims to create the model of human\\n'\n",
      "          'avatars from a given multi-view video.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** ZJU-MoCap [292] is a prevalent benchmark in human '\n",
      "          'modeling from videos, captured with 23 synchronized\\n'\n",
      "          'cameras at a 1024 _×_ 1024 resolution. Six subjects ( _i.e_ ., '\n",
      "          '377,\\n'\n",
      "          '386, 387, 392, 393, and 394) are used for evaluation [305].\\n'\n",
      "          'The same testing views following [306] are adopted.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve three recent papers which model human avatar\\n'\n",
      "          'with 3D GS [145], [146], [249], as well as six human rendering '\n",
      "          'approaches [292], [305]–[309].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** PSNR, SSIM [300], and LPIPS* [301]\\n'\n",
      "          'are used for measuring RGB rendering performance. Here\\n'\n",
      "          'LPIPS* equals to LPIPS _×_ 1000.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 14,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [{'bbox': (55.227783203125,\n",
      "                       83.96910858154297,\n",
      "                       294.1467590332031,\n",
      "                       189.20849609375),\n",
      "              'columns': 3,\n",
      "              'rows': 2},\n",
      "             {'bbox': (55.227783203125,\n",
      "                       247.83876037597656,\n",
      "                       298.76348876953125,\n",
      "                       320.5862121582031),\n",
      "              'columns': 4,\n",
      "              'rows': 2}],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 5\\n'\n",
      "          'Comparison of **reconstruction** methods (§6.4) on ZJU-MoCap [292]\\n'\n",
      "          '(avatar), in terms of PSNR, SSIM, and LPIPS*. The results for '\n",
      "          'non-GS\\n'\n",
      "          'methods are taken from [146].\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|PSNR↑ SSIM↑ LPIPS*↓|\\n'\n",
      "          '|---|---|---|\\n'\n",
      "          '|NeuralBody [292] [CVPR21]<br>AnimNeRF [307] [ICCV21]<br>PixelNeRF '\n",
      "          '[308] [ICCV21]<br>NHP [309] [NeurIPS21]<br>HumanNeRF [305] '\n",
      "          '[CVPR22]<br>Instant-NVR [306] [CVPR23]<br>GauHuman [145] '\n",
      "          '[CVPR24]<br>3DGS-Avatar [249] [CVPR24]<br>GART [146] '\n",
      "          '[CVPR24]|✓<br>✓<br>✓|29.03<br>0.96<br>42.47<br>29.77<br>0.96<br>46.89<br>24.71<br>0.89<br>121.86<br>28.25<br>0.95<br>64.77<br>30.66<br>0.97<br>33.38<br>31.01<br>0.97<br>38.45<br>31.34<br>0.97<br>30.51<br>30.61<br>0.97<br>29.58<br>32.22<br>0.98<br>29.21|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 6\\n'\n",
      "          'Comparison of **reconstruction** methods (§6.5) on EndoNeRF [258]\\n'\n",
      "          '(surgical scenes), in terms of PSNR, SSIM, and LPIPS. The results '\n",
      "          'for\\n'\n",
      "          'non-GS methods are taken from [153]. FPS and GPU usage for '\n",
      "          'training\\n'\n",
      "          '(Mem.) are measured using one 4090 GPU [153].\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|PSNR↑ SSIM↑ LPIPS↓|FPS↑ Mem.↓|\\n'\n",
      "          '|---|---|---|---|\\n'\n",
      "          '|EndoNeRF [258][MICCAI22]<br>EndoSurf '\n",
      "          '[260][MICCAI23]<br>LerPlane-9k [259][MICCAI23]<br>LerPlane-32k '\n",
      "          '[259][MICCAI23]<br>Endo-4DGS [152][MICCAI24]<br>EndoGaussian '\n",
      "          '[153][arXiv]<br>HFGS '\n",
      "          '[155][BMVC24]|✓<br>✓<br>✓|36.06<br>0.93<br>0.09<br>36.53<br>0.95<br>0.07<br>35.00<br>0.93<br>0.08<br>37.38<br>0.95<br>0.05<br>37.00<br>0.96<br>0.05<br>37.85<br>0.96<br>0.05<br>38.14<br>0.97<br>0.03|0.04<br>19GB<br>0.04<br>17GB<br>0.91<br>20GB<br>0.87<br>20GB<br>-<br>4GB<br>195.09<br>2GB<br>-<br>-|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** Table 5 presents the numerical results of '\n",
      "          'topleading solutions in human avatar modeling. We observe\\n'\n",
      "          'that introducing 3D GS into the framework leads to consistent '\n",
      "          'performance improvements in both rendering quality\\n'\n",
      "          'and speed. For instance, GART [146] outperforms current\\n'\n",
      "          'SOTA, Instant-NVR [306], by **1.21** dB in terms of PSNR.\\n'\n",
      "          'Considering the enhanced fidelity, inference speed and editability, '\n",
      "          '3D GS based avatar modeling may revolutionize\\n'\n",
      "          'the field of 3D animation, interactive gaming, _etc_ .\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.5** **Performance Benchmarking: Surgical Scenes**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '3D reconstruction from endoscopic video is critical to\\n'\n",
      "          'robotic-assisted minimally invasive surgery, enabling preoperative '\n",
      "          'planning, training through AR/VR simulations,\\n'\n",
      "          'and intraoperative guidance.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** EndoNeRF [258] dataset presents a specialized\\n'\n",
      "          'collection of stereo camera captures, comprising two samples of '\n",
      "          'in-vivo prostatectomy. It is tailored to represent realworld '\n",
      "          'surgical complexities, including challenging scenes\\n'\n",
      "          'with tool occlusion and pronounced non-rigid deformation.\\n'\n",
      "          'The same testing views as in [260] are used.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve three recent papers which reconstruct dynamic\\n'\n",
      "          '3D endoscopic scenes with GS [152], [153], [155], as well as\\n'\n",
      "          'three NeRF-based surgical reconstruction approaches [258]–\\n'\n",
      "          '\\n'\n",
      "          '[260].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** PSNR, SSIM [300], and LPIPS [301]\\n'\n",
      "          'are adopted for evaluation. In addition, the requirement for\\n'\n",
      "          'GPU memory is also reported.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** Table 6 shows that introducing the explicit '\n",
      "          'representation of 3D Gaussians leads to several significant '\n",
      "          'improvements. For instance, EndoGaussian [153] outperforms\\n'\n",
      "          'a strong baseline, LerPlane-32k [259], among all metrics. In\\n'\n",
      "          'particular, EndoGaussian demonstrates an approximate 224fold '\n",
      "          'increase in speed while consumes just 10% of the GPU\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'resources. These impressive results attest to the efficiency\\n'\n",
      "          'of GS-based methods, which not only expedite processing\\n'\n",
      "          'but also minimize GPU load, thus easing the demands on\\n'\n",
      "          'hardware. Such attributes are vitally significant for realworld '\n",
      "          'surgical application deployment, where optimized\\n'\n",
      "          'resource usage can be a key determinant of practical utility.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**7** **F** **UTURE** **R** **ESEARCH** **D** **IRECTIONS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'As impressive as those follow-up works on 3D GS are, and\\n'\n",
      "          'as much as those fields have been or might be revolutionized by 3D '\n",
      "          'GS, there is a general agreement that 3D GS still\\n'\n",
      "          'has considerable room for improvement.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Physics- and Semantics-aware Scene Representation.** As\\n'\n",
      "          'a new, explicit scene representation technique, 3D Gaussian\\n'\n",
      "          'offers transformative potential beyond merely enhancing\\n'\n",
      "          'novel-view synthesis. It has the potential to pave the way\\n'\n",
      "          'for simultaneous advancements in scene reconstruction and\\n'\n",
      "          'understanding by devising physics- and semantics-aware\\n'\n",
      "          '3D GS systems. While significant progress has been made\\n'\n",
      "          'in physics (Sec. 5.7) and semantics [310]–[315] individually, there '\n",
      "          'remains considerable untapped potential in their\\n'\n",
      "          'synergistic integration. This is poised to revolutionize a\\n'\n",
      "          'range of fields and downstream applications. For instance,\\n'\n",
      "          'incorporating prior knowledge such as the general shape\\n'\n",
      "          'of objects can reduce the need for extensive training viewpoints '\n",
      "          '[47], [48] while improving geometry/surface reconstruction [77], '\n",
      "          '[316]. A critical metric for assessing scene\\n'\n",
      "          'representation is the quality of its generated scenes, which\\n'\n",
      "          'encompasses challenges in geometry, texture, and lighting\\n'\n",
      "          'fidelity [66], [128], [141]. By merging physical principles\\n'\n",
      "          'and semantic information within the 3D GS framework,\\n'\n",
      "          'one can expect that the quality will be enhanced, thereby\\n'\n",
      "          'facilitating dynamics modeling [21], [166], editing [90], [92],\\n'\n",
      "          'generation [133], [134], and beyond. In a nutshell, pursuing\\n'\n",
      "          'this advanced and versatile scene representation opens up\\n'\n",
      "          'new possibilities for innovation in computational creativity\\n'\n",
      "          'and practical applications across diverse domains.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Learning Physical Priors from Large-scale Data.** As we\\n'\n",
      "          'explore the potential of physics- and semantics-aware scene\\n'\n",
      "          'representations, leveraging large-scale datasets to learn '\n",
      "          'generalizable, physical priors emerges as a promising direction.\\n'\n",
      "          'The goal is to model the inherent physical properties and\\n'\n",
      "          'dynamics embedded within real-world data, transforming\\n'\n",
      "          'them into actionable insights that can be applied across various '\n",
      "          'domains such as robotics and visual effects. Establishing\\n'\n",
      "          'a learning framework for extracting these generalizable\\n'\n",
      "          'priors enables the application of these insights to specific\\n'\n",
      "          'tasks in a few-shot manner. For instance, it allows for rapid\\n'\n",
      "          'adaptation to new objects and environments with minimal\\n'\n",
      "          'data input. Furthermore, integrating physical priors can enhance '\n",
      "          'not only the accuracy and quality of generated scenes\\n'\n",
      "          'but also their interactive and dynamic qualities. This is\\n'\n",
      "          'particularly valuable in AR/VR environments, where users\\n'\n",
      "          'interact with virtual objects that behave in ways consistent\\n'\n",
      "          'with their real-world counterparts. However, the existing\\n'\n",
      "          'body of work on capturing and distilling physics-based\\n'\n",
      "          'knowledge from extensive 2D and 3D datasets remains\\n'\n",
      "          'sparse. Notable efforts in related area include the continuum\\n'\n",
      "          'mechanics based GS systems (Sec. 5.7), and the generalizable\\n'\n",
      "          'Gaussian representation based on multi-view stereo [317].\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 15,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 15\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Further exploration on real2sim and sim2real might offer\\n'\n",
      "          'viable routes for advancements in this field.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Modeling Internal Structures of Objects with 3D GS.**\\n'\n",
      "          'Despite the ability of 3D GS to produce highly photorealistic\\n'\n",
      "          'renderings, modeling internal structures of objects ( _e.g_ ., for '\n",
      "          'a\\n'\n",
      "          'scanned object in computed tomography) within the current\\n'\n",
      "          'GS framework presents a notable challenge. Due to the\\n'\n",
      "          'splatting and density control process, the current representation '\n",
      "          'of 3D Gaussian is unorganized and cannot align\\n'\n",
      "          'well with the object’s actual internal structures. Moreover,\\n'\n",
      "          'there is a strong preference in various applications to depict\\n'\n",
      "          'objects as volumes ( _e.g_ ., computed tomography). However,\\n'\n",
      "          'the disordered nature of 3D GS makes volume modeling\\n'\n",
      "          'particularly difficult. Li _et al_ . [318] used 3D Gaussians with\\n'\n",
      "          'density control as the basis for the volumetric representation\\n'\n",
      "          'and did not involve the splatting process. X-Gaussian [319]\\n'\n",
      "          'involves the splatting process for fast training and inference but '\n",
      "          'cannot generate volumetric representation. Using\\n'\n",
      "          '3D GS to model the internal structures of objects remains\\n'\n",
      "          'unanswered and deserves further exploration.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **3D GS for Simulation in Autonomous Driving and be-**\\n'\n",
      "          '**yond.** Collecting real-world datasets for autonomous driving is '\n",
      "          'both expensive and logistically challenging, yet crucial\\n'\n",
      "          'for training effective image-based perception systems. To\\n'\n",
      "          'mitigate these issues, simulation emerges as a cost-effective\\n'\n",
      "          'alternative, enabling the generation of synthetic datasets\\n'\n",
      "          'across diverse environments. However, the development of\\n'\n",
      "          'simulators capable of producing photorealistic and diverse\\n'\n",
      "          'synthetic data is fraught with challenges. These include\\n'\n",
      "          'achieving a high level of quality, accommodating various\\n'\n",
      "          'control methods, and accurately simulating a range of lighting '\n",
      "          'conditions. While early efforts [188]–[190] in reconstructing '\n",
      "          'urban/street scenes with 3D GS have been encouraging,\\n'\n",
      "          'they are just the tip of the iceberg in terms of the full '\n",
      "          'capabilities. There remain numerous critical aspects to be '\n",
      "          'explored,\\n'\n",
      "          'such as the integration of user-defined object models, the\\n'\n",
      "          'modeling of physics-aware scene changes ( _e.g_ ., the rotation\\n'\n",
      "          'of vehicle wheels), and the enhancement of controllability\\n'\n",
      "          'and quality ( _e.g_ ., in varying lighting conditions). Mastery\\n'\n",
      "          'of these capabilities would not only advance autonomous\\n'\n",
      "          'systems but also redefine computational understanding of\\n'\n",
      "          'physical spaces — a leap with implications for world models, '\n",
      "          'spatial intelligence, embodied AI, and beyond.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Empowering 3D GS with More Possibilities.** Despite the\\n'\n",
      "          'significant potential of 3D GS, the full scope of applications\\n'\n",
      "          'for 3D GS remains largely untapped. A promising avenue\\n'\n",
      "          'for exploration involves augmenting 3D Gaussians with additional '\n",
      "          'attributes ( _e.g_ ., linguistic and spatiotemporal properties as '\n",
      "          'mentioned in Sec. 4.5) and introducing structured\\n'\n",
      "          'information (e.g., spatial MLPs and grids as mentioned in\\n'\n",
      "          'Sec. 4.6), tailored for specific applications. Moreover, recent\\n'\n",
      "          'studies have begun to unveil the capability of 3D GS in\\n'\n",
      "          'several domains, _e.g_ ., point cloud registration [320], image '\n",
      "          'representation and compression [60], and fluid synthesis [171]. '\n",
      "          'These findings highlight a significant opportunity\\n'\n",
      "          'for interdisciplinary scholars to explore 3D GS further.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**8** **C** **ONCLUSIONS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'To the best of our knowledge, this survey presents the\\n'\n",
      "          'first comprehensive overview of 3D GS, a groundbreaking\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'technique revolutionizing explicit radiance fields, computer\\n'\n",
      "          'graphics, and computer vision. It delineates the paradigm\\n'\n",
      "          'shift from traditional NeRF based methods, spotlighting the\\n'\n",
      "          'advantages of 3D GS in real-time rendering and enhanced\\n'\n",
      "          'editability. Our in-depth analysis and extensive quantitative\\n'\n",
      "          'studies demonstrate the superiority of 3D GS in practical\\n'\n",
      "          'applications, particularly those highly sensitive to latency.\\n'\n",
      "          'We offer insights into principles, prospective research directions, '\n",
      "          'and the unresolved challenges within this domain. Overall, 3D GS '\n",
      "          'stands as a transformative technology,\\n'\n",
      "          'poised to significantly influence future advancements in 3D\\n'\n",
      "          'reconstruction and representation. This survey is intended\\n'\n",
      "          'to serve as a foundational resource, propelling further exploration '\n",
      "          'and progress in this rapidly evolving field.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**R** **EFERENCES**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[1] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen, '\n",
      "          '“The\\n'\n",
      "          'lumigraph,” in _Seminal Graphics Papers: Pushing the Boundaries,_\\n'\n",
      "          '_Volume 2_, 2023, pp. 453–464.\\n'\n",
      "          '\\n'\n",
      "          '[2] M. Levoy and P. Hanrahan, “Light field rendering,” in '\n",
      "          '_Seminal_\\n'\n",
      "          '_Graphics Papers: Pushing the Boundaries, Volume 2_, 2023, pp. '\n",
      "          '441–\\n'\n",
      "          '452.\\n'\n",
      "          '\\n'\n",
      "          '[3] C. Buehler, M. Bosse, L. McMillan, S. Gortler, and M. Cohen,\\n'\n",
      "          '“Unstructured lumigraph rendering,” in _Seminal Graphics Papers:_\\n'\n",
      "          '_Pushing the Boundaries, Volume 2_, 2023, pp. 497–504.\\n'\n",
      "          '\\n'\n",
      "          '[4] N. Snavely, S. M. Seitz, and R. Szeliski, “Photo tourism: '\n",
      "          'exploring\\n'\n",
      "          'photo collections in 3d,” in _ACM Trans. Graph._, 2006, pp. '\n",
      "          '835–846.\\n'\n",
      "          '\\n'\n",
      "          '[5] M. Goesele, N. Snavely, B. Curless, H. Hoppe, and S. M. Seitz,\\n'\n",
      "          '“Multi-view stereo for community photo collections,” in _Proc._\\n'\n",
      "          '_IEEE Int. Conf. Comput. Vis._, 2007, pp. 1–8.\\n'\n",
      "          '\\n'\n",
      "          '[6] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. '\n",
      "          'Valentin,\\n'\n",
      "          '“Fastnerf: High-fidelity neural rendering at 200fps,” in _Proc. '\n",
      "          'IEEE_\\n'\n",
      "          '_Int. Conf. Comput. Vis._, 2021, pp. 14 346–14 355.\\n'\n",
      "          '\\n'\n",
      "          '[7] C. Reiser, S. Peng, Y. Liao, and A. Geiger, “Kilonerf: Speeding '\n",
      "          'up\\n'\n",
      "          'neural radiance fields with thousands of tiny mlps,” in _Proc. '\n",
      "          'IEEE_\\n'\n",
      "          '_Int. Conf. Comput. Vis._, 2021, pp. 14 335–14 345.\\n'\n",
      "          '\\n'\n",
      "          '[8] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and '\n",
      "          'P. Hedman, “Mip-nerf 360: Unbounded anti-aliased neural radiance\\n'\n",
      "          'fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2022,\\n'\n",
      "          'pp. 5470–5479.\\n'\n",
      "          '\\n'\n",
      "          '[9] T. M¨uller, A. Evans, C. Schied, and A. Keller, “Instant '\n",
      "          'neural\\n'\n",
      "          'graphics primitives with a multiresolution hash encoding,” _ACM_\\n'\n",
      "          '_Trans. Graph._, vol. 41, no. 4, pp. 1–15, 2022.\\n'\n",
      "          '\\n'\n",
      "          '[10] B. Kerbl, G. Kopanas, T. Leimk¨uhler, and G. Drettakis, “3d\\n'\n",
      "          'gaussian splatting for real-time radiance field rendering,” _ACM_\\n'\n",
      "          '_Trans. Graph._, vol. 42, no. 4, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[11] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, '\n",
      "          'and\\n'\n",
      "          'M. Zollhofer, “Deepvoxels: Learning persistent 3d feature '\n",
      "          'embeddings,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2019,\\n'\n",
      "          'pp. 2437–2446.\\n'\n",
      "          '\\n'\n",
      "          '[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. '\n",
      "          'Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural\\n'\n",
      "          'radiance fields for view synthesis,” in _Proc. Eur. Conf. Comput._\\n'\n",
      "          '_Vis._, 2020, pp. 405–421.\\n'\n",
      "          '\\n'\n",
      "          '[13] H. Pfister, M. Zwicker, J. Van Baar, and M. Gross, “Surfels: '\n",
      "          'Surface\\n'\n",
      "          'elements as rendering primitives,” in _Proceedings of the 27th '\n",
      "          'annual_\\n'\n",
      "          '_conference on Computer graphics and interactive techniques_, 2000, '\n",
      "          'pp.\\n'\n",
      "          '335–342.\\n'\n",
      "          '\\n'\n",
      "          '[14] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Surface\\n'\n",
      "          'splatting,” in _Proceedings of the 28th annual conference on '\n",
      "          'Computer_\\n'\n",
      "          '_graphics and interactive techniques_, 2001, pp. 371–378.\\n'\n",
      "          '\\n'\n",
      "          '[15] L. Ren, H. Pfister, and M. Zwicker, “Object space ewa surface\\n'\n",
      "          'splatting: A hardware accelerated approach to high quality point\\n'\n",
      "          'rendering,” in _Comput. Graph. Forum_, no. 3, 2002, pp. 461–470.\\n'\n",
      "          '\\n'\n",
      "          '[16] W. Yifan, F. Serena, S. Wu, C. Oztireli, and O. '\n",
      "          'Sorkine-Hornung, [¨]\\n'\n",
      "          '“Differentiable surface splatting for point-based geometry '\n",
      "          'processing,” _ACM Trans. Graph._, vol. 38, no. 6, pp. 1–14, 2019.\\n'\n",
      "          '\\n'\n",
      "          '[17] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, “Synsin: '\n",
      "          'Endto-end view synthesis from a single image,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2020, pp. 7467–7477.\\n'\n",
      "          '\\n'\n",
      "          '[18] D. Kalkofen, E. Mendez, and D. Schmalstieg, “Comprehensible\\n'\n",
      "          'visualization for augmented reality,” _IEEE Trans. Vis. Comput._\\n'\n",
      "          '_Graph._, vol. 15, no. 2, pp. 193–204, 2008.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 16,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 16\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[19] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. '\n",
      "          'Benty,\\n'\n",
      "          'D. Luebke, and A. Lefohn, “Towards foveated rendering for '\n",
      "          'gazetracked virtual reality,” _ACM Trans. Graph._, vol. 35, no. 6, '\n",
      "          'pp. 1–12,\\n'\n",
      "          '2016.\\n'\n",
      "          '\\n'\n",
      "          '[20] R. Albert, A. Patney, D. Luebke, and J. Kim, “Latency '\n",
      "          'requirements for foveated rendering in virtual reality,” _ACM '\n",
      "          'Transactions_\\n'\n",
      "          '_on Applied Perception_, vol. 14, no. 4, pp. 1–13, 2017.\\n'\n",
      "          '\\n'\n",
      "          '[21] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. '\n",
      "          'Lau,\\n'\n",
      "          'F. Gao, Y. Yang _et al._, “Vr-gs: A physical dynamics-aware '\n",
      "          'interactive gaussian splatting system in virtual reality,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2401.16663_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai,\\n'\n",
      "          '“Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” '\n",
      "          'in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[23] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam, '\n",
      "          '“Relightable\\n'\n",
      "          'gaussian codec avatars,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[24] T. Zhang, K. Huang, W. Zhi, and M. Johnson-Roberson, “Darkgs:\\n'\n",
      "          'Learning neural illumination and 3d gaussians relighting for\\n'\n",
      "          'robotic exploration in the dark,” in _Proc. IEEE/RSJ Int. Conf. '\n",
      "          'Intell._\\n'\n",
      "          '_Robot. Syst._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[25] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He, “3d '\n",
      "          'gaussian\\n'\n",
      "          'splatting as new era: A survey,” _IEEE Trans. Vis. Comput. '\n",
      "          'Graph._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[26] A. Dalal, D. Hagen, K. G. Robbersmyr, and K. M. Knausg˚ard,\\n'\n",
      "          '“Gaussian splatting: 3d reconstruction and novel view synthesis,\\n'\n",
      "          'a review,” _IEEE Access_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[27] Y. Bao, T. Ding, J. Huo, Y. Liu, Y. Li, W. Li, Y. Gao, and J. '\n",
      "          'Luo,\\n'\n",
      "          '“3d gaussian splatting: Survey, technologies, challenges, and\\n'\n",
      "          'opportunities,” _arXiv preprint arXiv:2407.17418_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[28] T. Wu, Y.-J. Yuan, L.-X. Zhang, J. Yang, Y.-P. Cao, L.-Q. Yan, '\n",
      "          'and\\n'\n",
      "          'L. Gao, “Recent advances in 3d gaussian splatting,” _Comput. Vis._\\n'\n",
      "          '_Media_, pp. 1–30, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[29] L. Kobbelt and M. Botsch, “A survey of point-based techniques '\n",
      "          'in\\n'\n",
      "          'computer graphics,” _Comput. Graph._, vol. 28, no. 6, pp. 801–814,\\n'\n",
      "          '2004.\\n'\n",
      "          '\\n'\n",
      "          '[30] Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan,\\n'\n",
      "          'F. Tombari, J. Tompkin, V. Sitzmann, and S. Sridhar, “Neural\\n'\n",
      "          'fields in visual computing and beyond,” in _Comput. Graph. Forum_,\\n'\n",
      "          'no. 2, 2022, pp. 641–676.\\n'\n",
      "          '\\n'\n",
      "          '[31] W. Wang, Y. Yang, and Y. Pan, “Visual knowledge in\\n'\n",
      "          'the big model era: Retrospect and prospect,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.04308_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[32] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. '\n",
      "          'Tretschk,\\n'\n",
      "          'W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi\\n'\n",
      "          '_et al._, “Advances in neural rendering,” in _Comput. Graph. '\n",
      "          'Forum_,\\n'\n",
      "          'no. 2, 2022, pp. 703–735.\\n'\n",
      "          '\\n'\n",
      "          '[33] X.-F. Han, H. Laga, and M. Bennamoun, “Image-based 3d object\\n'\n",
      "          'reconstruction: State-of-the-art and trends in the deep learning\\n'\n",
      "          'era,” _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 43, no. 5, '\n",
      "          'pp.\\n'\n",
      "          '1578–1604, 2019.\\n'\n",
      "          '\\n'\n",
      "          '[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and\\n'\n",
      "          'A. Geiger, “Occupancy networks: Learning 3d reconstruction in\\n'\n",
      "          'function space,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2019, pp. 4460–4470.\\n'\n",
      "          '\\n'\n",
      "          '[35] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. '\n",
      "          'Lovegrove,\\n'\n",
      "          '“Deepsdf: Learning continuous signed distance functions for\\n'\n",
      "          'shape representation,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2019, pp. 165–174.\\n'\n",
      "          '\\n'\n",
      "          '[36] C. Sun, M. Sun, and H.-T. Chen, “Direct voxel grid '\n",
      "          'optimization:\\n'\n",
      "          'Super-fast convergence for radiance fields reconstruction,” in\\n'\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 5459–\\n'\n",
      "          '5469.\\n'\n",
      "          '\\n'\n",
      "          '[37] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and\\n'\n",
      "          'A. Kanazawa, “K-planes: Explicit radiance fields in space, time,\\n'\n",
      "          'and appearance,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recog-_\\n'\n",
      "          '_nit._, 2023, pp. 12 479–12 488.\\n'\n",
      "          '\\n'\n",
      "          '[38] J. P. Grossman and W. J. Dally, “Point sample rendering,” in\\n'\n",
      "          '_Render. Tech._, 1998, pp. 181–192.\\n'\n",
      "          '\\n'\n",
      "          '[39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Ewa '\n",
      "          'volume\\n'\n",
      "          'splatting,” in _Proceedings Visualization, 2001. VIS’01._, 2001, '\n",
      "          'pp. 29–\\n'\n",
      "          '538.\\n'\n",
      "          '\\n'\n",
      "          '[40] ——, “Ewa splatting,” _IEEE Trans. Vis. Comput. Graph._, vol. '\n",
      "          '8,\\n'\n",
      "          'no. 3, pp. 223–238, 2002.\\n'\n",
      "          '\\n'\n",
      "          '[41] K.-A. Aliev, A. Sevastopolsky, M. Kolos, D. Ulyanov, and V. '\n",
      "          'Lempitsky, “Neural point-based graphics,” in _Proc. Eur. Conf. '\n",
      "          'Comput._\\n'\n",
      "          '_Vis._, 2020, pp. 696–712.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[42] D. R¨uckert, L. Franke, and M. Stamminger, “Adop: Approximate\\n'\n",
      "          'differentiable one-pixel point rendering,” _ACM Trans. Graph._,\\n'\n",
      "          'vol. 41, no. 4, pp. 1–14, 2022.\\n'\n",
      "          '\\n'\n",
      "          '[43] C. Lassner and M. Zollhofer, “Pulsar: Efficient sphere-based '\n",
      "          'neural rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2021, pp. 1440–1449.\\n'\n",
      "          '\\n'\n",
      "          '[44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, '\n",
      "          'and\\n'\n",
      "          'X. Chen, “Gaussianpro: 3d gaussian splatting with progressive\\n'\n",
      "          'propagation,” in _Proc. ACM Int. Conf. Mach. Learn._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[45] H. Xiong, S. Muttukuru, R. Upadhyay, P. Chari, and A. '\n",
      "          'Kadambi,\\n'\n",
      "          '“Sparsegs: Real-time 360 _{\\\\_ deg _}_ sparse view synthesis using\\n'\n",
      "          'gaussian splatting,” _arXiv preprint arXiv:2312.00206_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[46] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, “Fsgs: Real-time '\n",
      "          'fewshot view synthesis using gaussian splatting,” in _Proc. Eur. '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, '\n",
      "          '“pixelsplat:\\n'\n",
      "          '3d gaussian splats from image pairs for scalable generalizable 3d\\n'\n",
      "          'reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, “Splatter '\n",
      "          'image:\\n'\n",
      "          'Ultra-fast single-view 3d reconstruction,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[49] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. '\n",
      "          'Gu,\\n'\n",
      "          '“Dngaussian: Optimizing sparse-view 3d gaussian radiance\\n'\n",
      "          'fields with global-local depth normalization,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[50] A. Swann, M. Strong, W. K. Do, G. S. Camps, M. Schwager, and\\n'\n",
      "          'M. Kennedy III, “Touch-gs: Visual-tactile supervised 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2403.09875_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[51] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger,\\n'\n",
      "          'T.-J. Cham, and J. Cai, “Mvsplat: Efficient 3d gaussian splatting\\n'\n",
      "          'from sparse multi-view images,” in _Proc. Eur. Conf. Comput. '\n",
      "          'Vis._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[52] C. Wewer, K. Raj, E. Ilg, B. Schiele, and J. E. Lenssen, '\n",
      "          '“latentsplat:\\n'\n",
      "          'Autoencoding variational gaussians for fast generalizable 3d\\n'\n",
      "          'reconstruction,” _arXiv preprint arXiv:2403.16292_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[53] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen,\\n'\n",
      "          'and G. Wetzstein, “Grm: Large gaussian reconstruction model\\n'\n",
      "          'for efficient 3d reconstruction and generation,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.14621_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[54] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. '\n",
      "          'Wang,\\n'\n",
      "          '“Gamba: Marry gaussian splatting with mamba for single view\\n'\n",
      "          '3d reconstruction,” _arXiv preprint arXiv:2403.18795_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[55] J. Zhang, J. Li, X. Yu, L. Huang, L. Gu, J. Zheng, and X. Bai, '\n",
      "          '“Corgs: Sparse-view 3d gaussian splatting via co-regularization,” '\n",
      "          'in\\n'\n",
      "          '_Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[56] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. Xu, and Z. Wang, '\n",
      "          '“Lightgaussian: Unbounded 3d gaussian compression with 15x '\n",
      "          'reduction and 200+ fps,” _arXiv preprint arXiv:2311.17245_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[57] K. Navaneet, K. P. Meibodi, S. A. Koohpayegani, and\\n'\n",
      "          'H. Pirsiavash, “Compact3d: Compressing gaussian splat radiance '\n",
      "          'field models with vector quantization,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2311.18159_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, “Compact 3d\\n'\n",
      "          'gaussian representation for radiance field,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[59] W. Morgenstern, F. Barthel, A. Hilsmann, and P. Eisert, '\n",
      "          '“Compact\\n'\n",
      "          '3d scene representation via self-organizing gaussian grids,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2312.13299_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[60] X. Zhang, X. Ge, T. Xu, D. He, Y. Wang, H. Qin, G. Lu, J. '\n",
      "          'Geng,\\n'\n",
      "          'and J. Zhang, “Gaussianimage: 1000 fps image representation\\n'\n",
      "          'and compression by 2d gaussian splatting,” in _Proc. Eur. Conf._\\n'\n",
      "          '_Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[61] S. Niedermayr, J. Stumpfegger, and R. Westermann, “Compressed '\n",
      "          '3d gaussian splatting for accelerated novel view synthesis,” in '\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: '\n",
      "          'Hash-grid\\n'\n",
      "          'assisted context for 3d gaussian splatting compression,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[63] P. Papantonakis, G. Kopanas, B. Kerbl, A. Lanvin, and G. '\n",
      "          'Drettakis, “Reducing the memory footprint of 3d gaussian '\n",
      "          'splatting,”\\n'\n",
      "          'in _I3D_, 2024, pp. 1–17.\\n'\n",
      "          '\\n'\n",
      "          '[64] G. Fang and B. Wang, “Mini-splatting: Representing scenes\\n'\n",
      "          'with a constrained number of gaussians,” _arXiv_ _preprint_\\n'\n",
      "          '_arXiv:2403.14166_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 17,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 17\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[65] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, '\n",
      "          '“Mipsplatting: Alias-free 3d gaussian splatting,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024, pp. 19 447–19 456.\\n'\n",
      "          '\\n'\n",
      "          '[66] J. Gao, C. Gu, Y. Lin, H. Zhu, X. Cao, L. Zhang, and Y. Yao, '\n",
      "          '“Relightable 3d gaussian: Real-time point cloud relighting with '\n",
      "          'brdf\\n'\n",
      "          'decomposition and ray tracing,” _arXiv preprint arXiv:2311.16043_,\\n'\n",
      "          '2023.\\n'\n",
      "          '\\n'\n",
      "          '[67] Z. Yan, W. F. Low, Y. Chen, and G. H. Lee, “Multi-scale 3d\\n'\n",
      "          'gaussian splatting for anti-aliased rendering,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[68] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma,\\n'\n",
      "          '“Gaussianshader: 3d gaussian splatting with shading functions\\n'\n",
      "          'for reflective surfaces,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[69] B. Lee, H. Lee, X. Sun, U. Ali, and E. Park, “Deblurring 3d\\n'\n",
      "          'gaussian splatting,” _arXiv preprint arXiv:2401.00834_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[70] D. Malarz, W. Smolak, J. Tabor, S. Tadeja, and P. Spurek, '\n",
      "          '“Gaussian splitting algorithm with color and opacity depended on\\n'\n",
      "          'viewing direction,” _arXiv preprint arXiv:2312.13729_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[71] L. Bolanos, S.-Y. Su, and H. Rhodin, “Gaussian shadow casting\\n'\n",
      "          'for neural characters,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[72] L. Radl, M. Steiner, M. Parger, A. Weinrauch, B. Kerbl, and\\n'\n",
      "          'M. Steinberger, “Stopthepop: Sorted gaussian splatting for '\n",
      "          'viewconsistent real-time rendering,” _ACM Trans. Graph._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[73] Z. Yang, X. Gao, Y. Sun, Y. Huang, X. Lyu, W. Zhou, S. Jiao, '\n",
      "          'X. Qi,\\n'\n",
      "          'and X. Jin, “Spec-gaussian: Anisotropic view-dependent appearance '\n",
      "          'for 3d gaussian splatting,” _arXiv preprint arXiv:2402.15870_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[74] C. Peng, Y. Tang, Y. Zhou, N. Wang, X. Liu, D. Li, and R. '\n",
      "          'Chellappa, “Bags: Blur agnostic gaussian splatting through '\n",
      "          'multiscale kernel modeling,” _arXiv preprint arXiv:2403.04926_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[75] L. Zhao, P. Wang, and P. Liu, “Bad-gaussians: Bundle adjusted\\n'\n",
      "          'deblur gaussian splatting,” _arXiv preprint arXiv:2403.11831_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[76] H. Dahmani, M. Bennehar, N. Piasco, L. Roldao, and D. '\n",
      "          'Tsishkou,\\n'\n",
      "          '“Swag: Splatting in the wild images with appearanceconditioned '\n",
      "          'gaussians,” _arXiv preprint arXiv:2403.10427_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[77] Y. Li, C. Lyu, Y. Di, G. Zhai, G. H. Lee, and F. Tombari, '\n",
      "          '“Geogaussian: Geometry-aware gaussian splatting for scene '\n",
      "          'rendering,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.11324_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[78] Z. Liang, Q. Zhang, W. Hu, Y. Feng, L. Zhu, and K. Jia, '\n",
      "          '“Analyticsplatting: Anti-aliased 3d gaussian splatting via analytic '\n",
      "          'integration,” _arXiv preprint arXiv:2403.11056_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[79] O. Seiskari, J. Ylilammi, V. Kaatrasalo, P. Rantalankila, M. '\n",
      "          'Turkulainen, J. Kannala, E. Rahtu, and A. Solin, “Gaussian '\n",
      "          'splatting\\n'\n",
      "          'on the move: Blur and rolling shutter compensation for natural\\n'\n",
      "          'camera motion,” _arXiv preprint arXiv:2403.13327_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[80] X. Song, J. Zheng, S. Yuan, H.-a. Gao, J. Zhao, X. He, W. Gu, '\n",
      "          'and\\n'\n",
      "          'H. Zhao, “Sa-gs: Scale-adaptive gaussian splatting for trainingfree '\n",
      "          'anti-aliasing,” _arXiv preprint arXiv:2403.19615_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[81] Y. Fu, S. Liu, A. Kulkarni, J. Kautz, A. A. Efros, and X. '\n",
      "          'Wang,\\n'\n",
      "          '“Colmap-free 3d gaussian splatting,” in _Proc. IEEE Conf. Comput._\\n'\n",
      "          '_Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[82] J. Jung, J. Han, H. An, J. Kang, S. Park, and S. Kim, '\n",
      "          '“Relaxing\\n'\n",
      "          'accurate initialization constraint for 3d gaussian splatting,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2403.09413_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[83] M. Yu, T. Lu, L. Xu, L. Jiang, Y. Xiangli, and B. Dai, “Gsdf: '\n",
      "          '3dgs\\n'\n",
      "          'meets sdf for improved rendering and reconstruction,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2403.16964_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[84] J. Zhang, F. Zhan, M. Xu, S. Lu, and E. Xing, “Fregs: 3d '\n",
      "          'gaussian\\n'\n",
      "          'splatting with progressive frequency regularization,” in _Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[85] L. Huang, J. Bai, J. Guo, and Y. Guo, “Gs++: Error analyzing '\n",
      "          'and\\n'\n",
      "          'optimal gaussian splatting,” _arXiv preprint arXiv:2402.00752_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[86] J. Li, L. Cheng, Z. Wang, T. Mu, and J. He, “Loopgaussian:\\n'\n",
      "          'Creating 3d cinemagraph with multi-view images via eulerian\\n'\n",
      "          'motion field,” _arXiv preprint arXiv:2404.08966_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[87] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language '\n",
      "          'embedded 3d gaussians for open-vocabulary scene understanding,”\\n'\n",
      "          'in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[88] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, “Langsplat: '\n",
      "          '3d\\n'\n",
      "          'language gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[89] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, “Fmgs:\\n'\n",
      "          'Foundation model embedded 3d gaussian splatting for holistic\\n'\n",
      "          '3d scene understanding,” _arXiv preprint arXiv:2401.01970_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari,\\n'\n",
      "          'S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging\\n'\n",
      "          '3d gaussian splatting to enable distilled feature fields,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping:\\n'\n",
      "          'Segment and edit anything in 3d scenes,” in _Proc. Eur. Conf._\\n'\n",
      "          '_Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. '\n",
      "          'Tian,\\n'\n",
      "          '“Segment any 3d gaussians,” _arXiv preprint arXiv:2312.00860_,\\n'\n",
      "          '2023.\\n'\n",
      "          '\\n'\n",
      "          '[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time\\n'\n",
      "          'photorealistic dynamic scene representation and rendering with\\n'\n",
      "          '4d gaussian splatting,” in _Proc. Int. Conf. Learn. Represent._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, '\n",
      "          '“Deformable 3d gaussians for high-fidelity monocular dynamic scene\\n'\n",
      "          'reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. '\n",
      "          'Tian,\\n'\n",
      "          'and X. Wang, “4d gaussian splatting for real-time dynamic scene\\n'\n",
      "          'rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[96] Y. Xu, B. Chen, Z. Li, H. Zhang, L. Wang, Z. Zheng, and\\n'\n",
      "          'Y. Liu, “Gaussian head avatar: Ultra high-fidelity head avatar\\n'\n",
      "          'via dynamic gaussians,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[97] S. Szymanowicz, E. Insafutdinov, C. Zheng, D. Campbell, J. F.\\n'\n",
      "          'Henriques, C. Rupprecht, and A. Vedaldi, “Flash3d: Feedforward '\n",
      "          'generalisable 3d scene reconstruction from a single\\n'\n",
      "          'image,” _arXiv preprint arXiv:2406.04343_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[98] K. Sargent, Z. Li, T. Shah, C. Herrmann, H.-X. Yu, Y. Zhang, '\n",
      "          'E. R.\\n'\n",
      "          'Chan, D. Lagun, L. Fei-Fei, D. Sun _et al._, “Zeronvs: Zero-shot '\n",
      "          '360degree view synthesis from a single image,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024, pp. 9420–9429.\\n'\n",
      "          '\\n'\n",
      "          '[99] J. Meng, H. Li, Y. Wu, Q. Gao, S. Yang, J. Zhang, and S. Ma,\\n'\n",
      "          '“Mirror-3dgs: Incorporating mirror reflections into 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2404.01168_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[100] H. Chen, C. Li, and G. H. Lee, “Neusg: Neural implicit '\n",
      "          'surface reconstruction with 3d gaussian splatting guidance,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2312.00846_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[101] Z. Yu, T. Sattler, and A. Geiger, “Gaussian opacity fields: '\n",
      "          'Efficient\\n'\n",
      "          'and compact surface reconstruction in unbounded scenes,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2404.10772_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[102] B. Zhang, C. Fang, R. Shrestha, Y. Liang, X. Long, and P. '\n",
      "          'Tan,\\n'\n",
      "          '“Rade-gs: Rasterizing depth in gaussian splatting,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2406.01467_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[103] A. Chen, H. Xu, S. Esposito, S. Tang, and A. Geiger,\\n'\n",
      "          '“Lara: Efficient large-baseline radiance fields,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2407.04699_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[104] E. Ververas, R. A. Potamias, J. Song, J. Deng, and S. '\n",
      "          'Zafeiriou,\\n'\n",
      "          '“Sags: Structure-aware 3d gaussian splatting,” in _Proc. Eur. '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis._, 2024, pp. 221–238.\\n'\n",
      "          '\\n'\n",
      "          '[105] C. Smith, D. Charatan, A. Tewari, and V. Sitzmann, “Flowmap:\\n'\n",
      "          'High-quality camera poses, intrinsics, and depth via gradient\\n'\n",
      "          'descent,” _arXiv preprint arXiv:2404.15259_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[106] Y. Lin, Z. Dai, S. Zhu, and Y. Yao, “Gaussian-flow: 4d '\n",
      "          'reconstruction with dynamic 3d gaussian particle,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[107] A. Saroha, M. Gladkova, C. Curreli, T. Yenamandra, and\\n'\n",
      "          'D. Cremers, “Gaussian splatting in style,” _arXiv_ _preprint_\\n'\n",
      "          '_arXiv:2403.08498_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[108] N. Moenne-Loccoz, A. Mirzaei, O. Perel, R. de Lutio, J. '\n",
      "          'Martinez Esturo, G. State, S. Fidler, N. Sharp, and Z. Gojcic, “3d\\n'\n",
      "          'gaussian ray tracing: Fast tracing of particle scenes,” _ACM '\n",
      "          'Trans._\\n'\n",
      "          '_Graph._, vol. 43, no. 6, pp. 1–19, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[109] A. Mai, P. Hedman, G. Kopanas, D. Verbin, D. Futschik, Q. '\n",
      "          'Xu,\\n'\n",
      "          'F. Kuester, J. T. Barron, and Y. Zhang, “Ever: Exact volumetric\\n'\n",
      "          'ellipsoid rendering for real-time view synthesis,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2410.01804_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[110] J. Condor, S. Speierer, L. Bode, A. Bozic, S. Green, P. '\n",
      "          'Didyk, and\\n'\n",
      "          'A. Jarabo, “Don’t splat your gaussians: Volumetric ray-traced\\n'\n",
      "          'primitives for modeling and rendering scattering and emissive\\n'\n",
      "          'media,” _ACM Trans. Graph._, 2025.\\n'\n",
      "          '\\n'\n",
      "          '[111] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li,\\n'\n",
      "          '“Gs-slam: Dense visual slam with 3d gaussian splatting,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. '\n",
      "          'Scherer,\\n'\n",
      "          'D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 18,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 18\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'gaussians for dense rgb-d slam,” in _Proc. IEEE Conf. Comput. '\n",
      "          'Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, '\n",
      "          '“Gaussian\\n'\n",
      "          'splatting slam,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, '\n",
      "          '“Gaussian-slam:\\n'\n",
      "          'Photo-realistic dense slam with gaussian splatting,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2312.10070_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[115] H. Huang, L. Li, H. Cheng, and S.-K. Yeung, “Photo-slam: '\n",
      "          'Realtime simultaneous localization and photorealistic mapping for\\n'\n",
      "          'monocular, stereo, and rgb-d cameras,” in _Proc. IEEE Conf. Com-_\\n'\n",
      "          '_put. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[116] M. Li, S. Liu, and H. Zhou, “Sgs-slam: Semantic gaussian '\n",
      "          'splatting for neural dense slam,” in _Proc. Eur. Conf. Comput. '\n",
      "          'Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[117] Y. Ji, Y. Liu, G. Xie, B. Ma, and Z. Xie, “Neds-slam: A '\n",
      "          'novel\\n'\n",
      "          'neural explicit dense semantic slam framework using 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2403.11679_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[118] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, '\n",
      "          '“Manigaussian: Dynamic gaussian splatting for multi-task robotic '\n",
      "          'manipulation,” _arXiv preprint arXiv:2403.08321_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[119] J. Abou-Chakra, K. Rana, F. Dayoub, and N. S¨underhauf, '\n",
      "          '“Physically embodied gaussian splatting: A realtime correctable '\n",
      "          'world\\n'\n",
      "          'model for robotics,” in _Proc. Annu. Conf. Robot Learn._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[120] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. '\n",
      "          'Firoozi,\\n'\n",
      "          'M. D. Kennedy, and M. Schwager, “Splat-mover: Multi-stage,\\n'\n",
      "          'open-vocabulary robotic manipulation via editable gaussian\\n'\n",
      "          'splatting,” in _Proc. Annu. Conf. Robot Learn._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[121] M. Ji, R.-Z. Qiu, X. Zou, and X. Wang, “Graspsplats: '\n",
      "          'Efficient manipulation with 3d feature splatting,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2409.02084_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[122] Y. Zheng, X. Chen, Y. Zheng, S. Gu, R. Yang, B. Jin, P. Li, '\n",
      "          'C. Zhong,\\n'\n",
      "          'Z. Wang, L. Liu _et al._, “Gaussiangrasper: 3d language gaussian\\n'\n",
      "          'splatting for open-vocabulary robotic grasping,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.09637_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[123] S. Zhu, R. Qin, G. Wang, J. Liu, and H. Wang, “Semgaussslam: '\n",
      "          'Dense semantic gaussian splatting slam,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.07494_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[124] Z. Peng, T. Shao, Y. Liu, J. Zhou, Y. Yang, J. Wang, and K. '\n",
      "          'Zhou,\\n'\n",
      "          '“Rtg-slam: Real-time 3d reconstruction at scale using gaussian\\n'\n",
      "          'splatting,” _ACM Trans. Graph._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[125] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, “Dynamic 3d\\n'\n",
      "          'gaussians: Tracking by persistent dynamic view synthesis,” in\\n'\n",
      "          '_Proc. Int. Conf. 3D Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[126] H. Yu, J. Julin, Z. A. Milacski, K. Niinuma, and L. A. Jeni, '\n",
      "          '“Cogs: [´]\\n'\n",
      "          'Controllable gaussian splatting,” in _Proc. IEEE Conf. Comput. '\n",
      "          'Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[127] R. Shao, J. Sun, C. Peng, Z. Zheng, B. Zhou, H. Zhang, and Y. '\n",
      "          'Liu,\\n'\n",
      "          '“Control4d: Efficient 4d portrait editing with text,” in _Proc. '\n",
      "          'IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[128] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. '\n",
      "          'Qi,\\n'\n",
      "          '“Sc-gs: Sparse-controlled gaussian splatting for editable dynamic\\n'\n",
      "          'scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[129] D. Das, C. Wewer, R. Yunus, E. Ilg, and J. E. Lenssen, '\n",
      "          '“Neural\\n'\n",
      "          'parametric gaussians for monocular non-rigid object '\n",
      "          'reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[130] Z. Li, Z. Chen, Z. Li, and Y. Xu, “Spacetime gaussian '\n",
      "          'feature\\n'\n",
      "          'splatting for real-time dynamic view synthesis,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[131] J. Sun, H. Jiao, G. Li, Z. Zhang, L. Zhao, and W. Xing, '\n",
      "          '“3dgstream:\\n'\n",
      "          'On-the-fly training of 3d gaussians for efficient streaming of\\n'\n",
      "          'photo-realistic free-viewpoint videos,” in _Proc. IEEE Conf. Com-_\\n'\n",
      "          '_put. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[132] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, '\n",
      "          'and\\n'\n",
      "          'Y. Dai, “3d geometry-aware deformable gaussian splatting for\\n'\n",
      "          'dynamic view synthesis,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[133] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, '\n",
      "          '“Dreamgaussian:\\n'\n",
      "          'Generative gaussian splatting for efficient 3d content creation,”\\n'\n",
      "          'in _Proc. Int. Conf. Learn. Represent._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[134] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. '\n",
      "          'Tian,\\n'\n",
      "          'and X. Wang, “Gaussiandreamer: Fast generation from text to 3d\\n'\n",
      "          'gaussians by bridging 2d and 3d diffusion models,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[135] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, '\n",
      "          '“Lgm:\\n'\n",
      "          'Large multi-view gaussian model for high-resolution 3d content\\n'\n",
      "          'creation,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[136] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. '\n",
      "          'You,\\n'\n",
      "          'Z. Wang, and A. Kadambi, “Dreamscene360: Unconstrained textto-3d '\n",
      "          'scene generation with panoramic gaussian splatting,” in\\n'\n",
      "          '_Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[137] Z. Li, Y. Chen, L. Zhao, and P. Liu, “Controllable '\n",
      "          'text-to-3d\\n'\n",
      "          'generation via surface-aligned gaussian splatting,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2403.09981_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[138] Y. Mu, X. Zuo, C. Guo, Y. Wang, J. Lu, X. Wu, S. Xu, P. Dai, '\n",
      "          'Y. Yan,\\n'\n",
      "          'and L. Cheng, “Gsd: View-guided gaussian splatting diffusion\\n'\n",
      "          'for 3d reconstruction,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[139] Y. Jiang, Z. Shen, P. Wang, Z. Su, Y. Hong, Y. Zhang, J. Yu, '\n",
      "          'and\\n'\n",
      "          'L. Xu, “Hifi4g: High-fidelity human performance rendering via\\n'\n",
      "          'compact gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[140] Y. Wang, Q. Wu, G. Zhang, and D. Xu, “Gscream: Learning 3d\\n'\n",
      "          'geometry and feature consistent gaussian splatting for object\\n'\n",
      "          'removal,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[141] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. '\n",
      "          'Cai,\\n'\n",
      "          'L. Yang, H. Liu, and G. Lin, “Gaussianeditor: Swift and '\n",
      "          'controllable 3d editing with gaussian splatting,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[142] J. Fang, J. Wang, X. Zhang, L. Xie, and Q. Tian, '\n",
      "          '“Gaussianeditor:\\n'\n",
      "          'Editing 3d gaussians delicately with text instructions,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[143] R.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, “Feature '\n",
      "          'splatting:\\n'\n",
      "          'Language-driven physics-based scene synthesis and editing,”\\n'\n",
      "          '_arXiv preprint arXiv:2404.01223_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[144] Z. Li, Z. Zheng, L. Wang, and Y. Liu, “Animatable gaussians:\\n'\n",
      "          'Learning pose-dependent gaussian maps for high-fidelity human\\n'\n",
      "          'avatar modeling,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recog-_\\n'\n",
      "          '_nit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[145] S. Hu and Z. Liu, “Gauhuman: Articulated gaussian splatting\\n'\n",
      "          'from monocular human videos,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, '\n",
      "          '“Gart:\\n'\n",
      "          'Gaussian articulated template models,” in _Proc. IEEE Conf. Com-_\\n'\n",
      "          '_put. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[147] Y. Yuan, X. Li, Y. Huang, S. De Mello, K. Nagano, J. Kautz, '\n",
      "          'and\\n'\n",
      "          'U. Iqbal, “Gavatar: Animatable 3d gaussian avatars with implicit\\n'\n",
      "          'mesh learning,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[148] Z. Zhou, F. Ma, H. Fan, and Y. Yang, “Headstudio: Text to\\n'\n",
      "          'animatable head avatars with 3d gaussian splatting,” in _Proc. '\n",
      "          'Eur._\\n'\n",
      "          '_Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[149] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. '\n",
      "          'Giebenhain,\\n'\n",
      "          'and M. Nießner, “Gaussianavatars: Photorealistic head avatars\\n'\n",
      "          'with rigged 3d gaussians,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[150] H. Dhamo, Y. Nie, A. Moreau, J. Song, R. Shaw, Y. Zhou, and\\n'\n",
      "          'E. P´erez-Pellitero, “Headgas: Real-time animatable head avatars\\n'\n",
      "          'via 3d gaussian splatting,” _arXiv preprint arXiv:2312.02902_, '\n",
      "          '2023.\\n'\n",
      "          '\\n'\n",
      "          '[151] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. '\n",
      "          'Gu,\\n'\n",
      "          '“Talkinggaussian: Structure-persistent 3d talking head synthesis\\n'\n",
      "          'via gaussian splatting,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[152] Y. Huang, B. Cui, L. Bai, Z. Guo, M. Xu, and H. Ren, '\n",
      "          '“Endo4dgs: Distilling depth ranking for endoscopic monocular scene\\n'\n",
      "          'reconstruction with 4d gaussian splatting,” in _Proc. Int. Conf. '\n",
      "          'Med._\\n'\n",
      "          '_Image Comput. Comput. Assist. Interv._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian\\n'\n",
      "          'splatting for deformable surgical scene reconstruction,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2401.12561_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, “Deformable '\n",
      "          'endoscopic tissues reconstruction with gaussian splatting,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2401.11535_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[155] H. Zhao, X. Zhao, L. Zhu, W. Zheng, and Y. Xu, “Hfgs: 4d\\n'\n",
      "          'gaussian splatting with emphasis on spatial and temporal '\n",
      "          'highfrequency components for endoscopic scene reconstruction,”\\n'\n",
      "          '_arXiv preprint arXiv:2405.17872_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[156] K. Wang, C. Yang, Y. Wang, S. Li, Y. Wang, Q. Dou, X. Yang,\\n'\n",
      "          'and W. Shen, “Endogslam: Real-time dense reconstruction and\\n'\n",
      "          'tracking in endoscopic surgeries using gaussian splatting,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2403.15124_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[157] S. Bonilla, S. Zhang, D. Psychogyios, D. Stoyanov, F. '\n",
      "          'Vasconcelos,\\n'\n",
      "          'and S. Bano, “Gaussian pancakes: Geometrically-regularized 3d\\n'\n",
      "          'gaussian splatting for realistic endoscopic reconstruction,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2404.06128_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 19,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 19\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[158] K. Wu, K. Zhang, Z. Zhang, S. Yuan, M. Tie, J. Wei, Z. Xu, J. '\n",
      "          'Zhao,\\n'\n",
      "          'Z. Gan, and W. Ding, “Hgs-mapping: Online dense mapping\\n'\n",
      "          'using hybrid gaussian representation in urban scenes,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2403.20159_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[159] C. Wu, Y. Duan, X. Zhang, Y. Sheng, J. Ji, and Y. Zhang, '\n",
      "          '“Mmgaussian: 3d gaussian-based multi-modal fusion for localization\\n'\n",
      "          'and reconstruction in unbounded scenes,” in _Proc. IEEE/RSJ Int._\\n'\n",
      "          '_Conf. Intell. Robot. Syst._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[160] B. Xiong, Z. Li, and Z. Li, “Gauu-scene: A scene '\n",
      "          'reconstruction\\n'\n",
      "          'benchmark on large scale 3d reconstruction dataset using gaussian '\n",
      "          'splatting,” _arXiv preprint arXiv:2401.14032_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[161] H. Zhao, H. Weng, D. Lu, A. Li, J. Li, A. Panda, and S. Xie,\\n'\n",
      "          '“On scaling up 3d gaussian splatting training,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2406.18533_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[162] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and\\n'\n",
      "          'G. Drettakis, “A hierarchical 3d gaussian representation for '\n",
      "          'realtime rendering of very large datasets,” _ACM Trans. Graph._, '\n",
      "          'vol. 44,\\n'\n",
      "          'no. 3, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[163] Y. Liu, H. Guan, C. Luo, L. Fan, J. Peng, and Z. Zhang, '\n",
      "          '“Citygaussian: Real-time high-quality large-scale scene rendering '\n",
      "          'with\\n'\n",
      "          'gaussians,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[164] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, '\n",
      "          'S. Xu,\\n'\n",
      "          'Y. Yan _et al._, “Vastgaussian: Vast 3d gaussians for large scene\\n'\n",
      "          'reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[165] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, '\n",
      "          '“Octreegs: Towards consistent real-time rendering with '\n",
      "          'lod-structured\\n'\n",
      "          '3d gaussians,” _arXiv preprint arXiv:2403.17898_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[166] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. '\n",
      "          'Jiang,\\n'\n",
      "          '“Physgaussian: Physics-integrated 3d gaussians for generative\\n'\n",
      "          'dynamics,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[167] F. Liu, H. Wang, S. Yao, S. Zhang, J. Zhou, and Y. Duan,\\n'\n",
      "          '“Physics3d: Learning physical properties of 3d gaussians via\\n'\n",
      "          'video diffusion,” _arXiv preprint arXiv:2406.04338_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[168] P. Borycki, W. Smolak, J. Waczy´nska, M. Mazur, S. Tadeja, '\n",
      "          'and\\n'\n",
      "          'P. Spurek, “Gasp: Gaussian splatting for physic-based simulations,” '\n",
      "          '_arXiv preprint arXiv:2409.05819_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[169] T. Huang, Y. Zeng, H. Li, W. Zuo, and R. W. Lau, '\n",
      "          '“Dreamphysics:\\n'\n",
      "          'Learning physical properties of dynamic 3d gaussians with video\\n'\n",
      "          'diffusion priors,” in _Proc. AAAI Conf. Artif. Intell._, 2025.\\n'\n",
      "          '\\n'\n",
      "          '[170] T. Zhang, H.-X. Yu, R. Wu, B. Y. Feng, C. Zheng, N. Snavely, '\n",
      "          'J. Wu,\\n'\n",
      "          'and W. T. Freeman, “Physdreamer: Physics-based interaction\\n'\n",
      "          'with 3d objects via video generation,” in _Proc. Eur. Conf. '\n",
      "          'Comput._\\n'\n",
      "          '_Vis._, 2024, pp. 388–406.\\n'\n",
      "          '\\n'\n",
      "          '[171] Y. Feng, X. Feng, Y. Shang, Y. Jiang, C. Yu, Z. Zong, T. '\n",
      "          'Shao,\\n'\n",
      "          'H. Wu, K. Zhou, C. Jiang _et al._, “Gaussian splashing: Dynamic '\n",
      "          'fluid synthesis with gaussian splatting,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2401.15318_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[172] L. Zhong, H.-X. Yu, J. Wu, and Y. Li, “Reconstruction and '\n",
      "          'simulation of elastic objects with spring-mass 3d gaussians,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[173] Y. Shao, M. Huang, C. C. Loy, and B. Dai, “Gausim: '\n",
      "          'Registering\\n'\n",
      "          'elastic objects into digital world by gaussian simulator,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2412.17804_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[174] S. Zhang, H. Zhao, Z. Zhou, G. Wu, C. Zheng, X. Wang, and\\n'\n",
      "          'W. Liu, “Togs: Gaussian splatting with temporal opacity offset\\n'\n",
      "          'for real-time 4d dsa rendering,” _arXiv preprint '\n",
      "          'arXiv:2403.19586_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[175] R. Wu, Z. Zhang, Y. Yang, and W. Zuo, “Dual-camera smooth\\n'\n",
      "          'zoom on mobile phones,” _arXiv preprint arXiv:2404.04908_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[176] H. Li, Y. Gao, D. Zhang, C. Wu, Y. Dai, C. Zhao, H. Feng, E. '\n",
      "          'Ding,\\n'\n",
      "          'J. Wang, and J. Han, “Ggrt: Towards generalizable 3d gaussians\\n'\n",
      "          'without pose priors in real-time,” _arXiv preprint '\n",
      "          'arXiv:2403.10147_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[177] S. Hong, J. He, X. Zheng, H. Wang, H. Fang, K. Liu, C. Zheng, '\n",
      "          'and\\n'\n",
      "          'S. Shen, “Liv-gaussmap: Lidar-inertial-visual fusion for real-time\\n'\n",
      "          '3d radiance field map rendering,” _arXiv preprint '\n",
      "          'arXiv:2401.14857_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[178] S. Sun, M. Mielle, A. J. Lilienthal, and M. Magnusson, '\n",
      "          '“Highfidelity slam using gaussian splatting with rendering-guided\\n'\n",
      "          'densification and regularized optimization,” in _Proc. IEEE/RSJ_\\n'\n",
      "          '_Int. Conf. Intell. Robot. Syst._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[179] F. Tosi, Y. Zhang, Z. Gong, E. Sandstr¨om, S. Mattoccia, M. '\n",
      "          'R.\\n'\n",
      "          'Oswald, and M. Poggi, “How nerfs and 3d gaussian splatting are\\n'\n",
      "          'reshaping slam: a survey,” _arXiv preprint arXiv:2402.13255_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[180] T. Deng, Y. Chen, L. Zhang, J. Yang, S. Yuan, D. Wang, and\\n'\n",
      "          'W. Chen, “Compact 3d gaussian splatting for dense visual slam,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.11247_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[181] J. Hu, X. Chen, B. Feng, G. Li, L. Yang, H. Bao, G. Zhang,\\n'\n",
      "          'and Z. Cui, “Cg-slam: Efficient dense rgb-d slam in a consistent '\n",
      "          'uncertainty-aware 3d gaussian field,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.16095_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[182] X. Lang, L. Li, H. Zhang, F. Xiong, M. Xu, Y. Liu, X. Zuo, '\n",
      "          'and J. Lv,\\n'\n",
      "          '“Gaussian-lic: Photo-realistic lidar-inertial-camera slam with 3d\\n'\n",
      "          'gaussian splatting,” _arXiv preprint arXiv:2404.06926_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[183] E. Sandstr¨om, K. Tateno, M. Oechsle, M. Niemeyer, L. Van '\n",
      "          'Gool,\\n'\n",
      "          'M. R. Oswald, and F. Tombari, “Splat-slam: Globally optimized '\n",
      "          'rgb-only slam with 3d gaussians,” _arXiv_ _preprint_\\n'\n",
      "          '_arXiv:2405.16544_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer,\\n'\n",
      "          '“D-nerf: Neural radiance fields for dynamic scenes,” in _Proc. '\n",
      "          'IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2021, pp. 10 318–10 327.\\n'\n",
      "          '\\n'\n",
      "          '[185] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. '\n",
      "          'B.\\n'\n",
      "          'Goldman, R. Martin-Brualla, and S. M. Seitz, “Hypernerf: a\\n'\n",
      "          'higher-dimensional representation for topologically varying neural '\n",
      "          'radiance fields,” _ACM Trans. Graph._, vol. 40, no. 6, pp. 1–12,\\n'\n",
      "          '2021.\\n'\n",
      "          '\\n'\n",
      "          '[186] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman,\\n'\n",
      "          'S. M. Seitz, and R. Martin-Brualla, “Nerfies: Deformable neural\\n'\n",
      "          'radiance fields,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2021, '\n",
      "          'pp.\\n'\n",
      "          '5865–5874.\\n'\n",
      "          '\\n'\n",
      "          '[187] X. Guo, J. Sun, Y. Dai, G. Chen, X. Ye, X. Tan, E. Ding, Y. '\n",
      "          'Zhang,\\n'\n",
      "          'and J. Wang, “Forward flow for novel view synthesis of dynamic\\n'\n",
      "          'scenes,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2023, pp. 16 '\n",
      "          '022–\\n'\n",
      "          '16 033.\\n'\n",
      "          '\\n'\n",
      "          '[188] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.-H. Yang,\\n'\n",
      "          '“Drivinggaussian: Composite gaussian splatting for surrounding\\n'\n",
      "          'dynamic autonomous driving scenes,” in _Proc. IEEE Conf. Com-_\\n'\n",
      "          '_put. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[189] Y. Yan, H. Lin, C. Zhou, W. Wang, H. Sun, K. Zhan, X. Lang,\\n'\n",
      "          'X. Zhou, and S. Peng, “Street gaussians for modeling dynamic\\n'\n",
      "          'urban scenes,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[190] H. Zhou, J. Shao, L. Xu, D. Bai, W. Qiu, B. Liu, Y. Wang, A. '\n",
      "          'Geiger,\\n'\n",
      "          'and Y. Liao, “Hugs: Holistic urban 3d scene understanding\\n'\n",
      "          'via gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024, pp. 21 336–21 345.\\n'\n",
      "          '\\n'\n",
      "          '[191] A. Kratimenos, J. Lei, and K. Daniilidis, “Dynmf: Neural '\n",
      "          'motion\\n'\n",
      "          'factorization for real-time dynamic view synthesis with 3d gaussian '\n",
      "          'splatting,” _arXiv preprint arXiv:2312.00112_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[192] R. Shaw, J. Song, A. Moreau, M. Nazarczuk, S. '\n",
      "          'Catley-Chandar,\\n'\n",
      "          'H. Dhamo, and E. Perez-Pellitero, “Swags: Sampling windows\\n'\n",
      "          'adaptively for dynamic 3d gaussian splatting,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.13308_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[193] Y. Liang, N. Khan, Z. Li, T. Nguyen-Phuoc, D. Lanman,\\n'\n",
      "          'J. Tompkin, and L. Xiao, “Gaufre: Gaussian deformation fields\\n'\n",
      "          'for real-time dynamic novel view synthesis,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.11458_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[194] K. Katsumata, D. M. Vo, and H. Nakayama, “An efficient 3d '\n",
      "          'gaussian representation for monocular/multi-view dynamic scenes,”\\n'\n",
      "          '_arXiv preprint arXiv:2311.12897_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[195] Z. Guo, W. Zhou, L. Li, M. Wang, and H. Li, “Motion-aware 3d\\n'\n",
      "          'gaussian splatting for efficient dynamic scene reconstruction,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.11447_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[196] J. Bae, S. Kim, Y. Yun, H. Lee, G. Bang, and Y. Uh, '\n",
      "          '“Per-gaussian\\n'\n",
      "          'embedding-based deformation for deformable 3d gaussian splatting,” '\n",
      "          '_arXiv preprint arXiv:2404.03613_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[197] J. Lei, Y. Weng, A. Harley, L. Guibas, and K. Daniilidis, '\n",
      "          '“Mosca:\\n'\n",
      "          'Dynamic gaussian fusion from casual videos via 4d motion\\n'\n",
      "          'scaffolds,” _arXiv preprint arXiv:2405.17421_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[198] Q. Wang, V. Ye, H. Gao, J. Austin, Z. Li, and A. Kanazawa, '\n",
      "          '“Shape\\n'\n",
      "          'of motion: 4d reconstruction from a single video,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2407.13764_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[199] Y. Duan, F. Wei, Q. Dai, Y. He, W. Chen, and B. Chen, '\n",
      "          '“4drotor gaussian splatting: towards efficient novel view '\n",
      "          'synthesis\\n'\n",
      "          'for dynamic scenes,” in _Proc. ACM Spec. Interest Group Comput._\\n'\n",
      "          '_Graph. Interact. Tech._, 2024, pp. 1–11.\\n'\n",
      "          '\\n'\n",
      "          '[200] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. '\n",
      "          'WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative '\n",
      "          'adversarial networks,” _Communications of the ACM_, vol. 63, no. '\n",
      "          '11, pp.\\n'\n",
      "          '139–144, 2020.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 20,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 20\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[201] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion '\n",
      "          'probabilistic\\n'\n",
      "          'models,” in _Proc. Adv. Neural Inf. Process. Syst._, 2020, pp. '\n",
      "          '6840–\\n'\n",
      "          '6851.\\n'\n",
      "          '\\n'\n",
      "          '[202] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\\n'\n",
      "          '“High-resolution image synthesis with latent diffusion models,”\\n'\n",
      "          'in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 10 '\n",
      "          '684–\\n'\n",
      "          '10 695.\\n'\n",
      "          '\\n'\n",
      "          '[203] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional '\n",
      "          'control to text-to-image diffusion models,” in _Proc. IEEE Int. '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis._, 2023, pp. 3836–3847.\\n'\n",
      "          '\\n'\n",
      "          '[204] Z. Chen, F. Wang, and H. Liu, “Text-to-3d using gaussian '\n",
      "          'splatting,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[205] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, '\n",
      "          '“Luciddreamer: Towards high-fidelity text-to-3d generation via '\n",
      "          'interval\\n'\n",
      "          'score matching,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[206] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu,\\n'\n",
      "          'and Z. Liu, “Humangaussian: Text-driven 3d human generation\\n'\n",
      "          'with gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[207] X. Yang, Y. Chen, C. Chen, C. Zhang, Y. Xu, X. Yang, F. Liu, '\n",
      "          'and\\n'\n",
      "          'G. Lin, “Learn to optimize denoising scores for 3d generation: A\\n'\n",
      "          'unified and improved diffusion prior on nerf and 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2312.04820_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[208] Z.-X. Zou, Z. Yu, Y.-C. Guo, Y. Li, D. Liang, Y.-P. Cao, and '\n",
      "          'S.-H.\\n'\n",
      "          'Zhang, “Triplane meets gaussian splatting: Fast and generalizable '\n",
      "          'single-view 3d reconstruction with transformers,” in _Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[209] H. Ling, S. W. Kim, A. Torralba, S. Fidler, and K. Kreis, '\n",
      "          '“Align\\n'\n",
      "          'your gaussians: Text-to-4d with dynamic 3d gaussians and composed '\n",
      "          'diffusion models,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[210] J. Ren, L. Pan, J. Tang, C. Zhang, A. Cao, G. Zeng, and Z. '\n",
      "          'Liu,\\n'\n",
      "          '“Dreamgaussian4d: Generative 4d gaussian splatting,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2312.17142_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[211] Y. Yin, D. Xu, Z. Wang, Y. Zhao, and Y. Wei, “4dgen: '\n",
      "          'Grounded\\n'\n",
      "          '4d content generation with spatial-temporal consistency,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2312.17225_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[212] J. Zhang, Z. Tang, Y. Pang, X. Cheng, P. Jin, Y. Wei, W. Yu,\\n'\n",
      "          'M. Ning, and L. Yuan, “Repaint123: Fast and high-quality one\\n'\n",
      "          'image to 3d generation with progressive controllable 2d '\n",
      "          'repainting,” _arXiv preprint arXiv:2312.13271_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[213] Z. Pan, Z. Yang, X. Zhu, and L. Zhang, “Fast dynamic 3d\\n'\n",
      "          'object generation from a single-view video,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2401.08742_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[214] D. Xu, Y. Yuan, M. Mardani, S. Liu, J. Song, Z. Wang, and\\n'\n",
      "          'A. Vahdat, “Agg: Amortized generative 3d gaussians for single\\n'\n",
      "          'image to 3d,” _arXiv preprint arXiv:2401.04099_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[215] C. Yang, S. Li, J. Fang, R. Liang, L. Xie, X. Zhang, W. '\n",
      "          'Shen,\\n'\n",
      "          'and Q. Tian, “Gaussianobject: Just taking four images to get a\\n'\n",
      "          'high-quality 3d object with gaussian splatting,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2402.10259_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[216] F. Barthel, A. Beckmann, W. Morgenstern, A. Hilsmann, and\\n'\n",
      "          'P. Eisert, “Gaussian splatting decoder for 3d-aware generative\\n'\n",
      "          'adversarial networks,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit. Worksh._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[217] L. Jiang and L. Wang, “Brightdreamer: Generic 3d gaussian\\n'\n",
      "          'generative framework for fast text-to-3d synthesis,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2403.11273_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[218] W. Zhuo, F. Ma, H. Fan, and Y. Yang, “Vividdreamer: '\n",
      "          'Invariant\\n'\n",
      "          'score distillation for hyper-realistic text-to-3d generation,” in\\n'\n",
      "          '_Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[219] Z. Wu, C. Yu, Y. Jiang, C. Cao, F. Wang, and X. Bai, “Sc4d:\\n'\n",
      "          'Sparse-controlled video-to-4d generation and motion transfer,”\\n'\n",
      "          '_arXiv preprint arXiv:2404.03736_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[220] X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan,\\n'\n",
      "          'W. Ouyang, and T. He, “Gvgen: Text-to-3d generation with\\n'\n",
      "          'volumetric representation,” in _Proc. Eur. Conf. Comput. Vis._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[221] X. Yang and X. Wang, “Hash3d: Training-free acceleration for '\n",
      "          '3d\\n'\n",
      "          'generation,” _arXiv preprint arXiv:2404.06091_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[222] J. Kim, J. Koo, K. Yeo, and M. Sung, “Synctweedies: A '\n",
      "          'general\\n'\n",
      "          'generative framework based on synchronized diffusions,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2403.14370_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[223] Q. Feng, Z. Xing, Z. Wu, and Y.-G. Jiang, “Fdgaussian: Fast '\n",
      "          'gaussian splatting from single image via geometric-aware diffusion\\n'\n",
      "          'model,” _arXiv preprint arXiv:2403.10242_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[224] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L.-h.\\n'\n",
      "          'Lee, and P. Zhou, “Dreamscene: 3d gaussian-based text-to-3d\\n'\n",
      "          'scene generation via formation pattern sampling,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.03575_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[225] L. Melas-Kyriazi, I. Laina, C. Rupprecht, N. Neverova,\\n'\n",
      "          'A. Vedaldi, O. Gafni, and F. Kokkinos, “Im-3d: Iterative multiview '\n",
      "          'diffusion and reconstruction for high-quality 3d generation,” in '\n",
      "          '_Proc. ACM Int. Conf. Mach. Learn._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[226] B. Zhang, Y. Cheng, J. Yang, C. Wang, F. Zhao, Y. Tang, D. '\n",
      "          'Chen,\\n'\n",
      "          'and B. Guo, “Gaussiancube: Structuring gaussian splatting using\\n'\n",
      "          'optimal transport for 3d generative modeling,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.19655_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[227] Y.-C. Lee, Y.-T. Chen, A. Wang, T.-H. Liao, B. Y. Feng, and\\n'\n",
      "          'J.-B. Huang, “Vividdream: Generating 3d scene with ambient\\n'\n",
      "          'dynamics,” _arXiv preprint arXiv:2405.20334_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[228] J. Huang and H. Yu, “Point’n move: Interactive scene object '\n",
      "          'manipulation on gaussian splatting radiance fields,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2311.16737_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[229] K. Lan, H. Li, H. Shi, W. Wu, Y. Liao, L. Wang, and\\n'\n",
      "          'P. Zhou, “2d-guided 3d gaussian segmentation,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.16047_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[230] J. Zhuang, D. Kang, Y.-P. Cao, G. Li, L. Lin, and Y. Shan, '\n",
      "          '“Tipeditor: An accurate 3d editor following both text-prompts and\\n'\n",
      "          'image-prompts,” _arXiv preprint arXiv:2401.14828_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[231] B. Dou, T. Zhang, Y. Ma, Z. Wang, and Z. Yuan, '\n",
      "          '“Cosseggaussians: Compact and swift scene segmenting 3d gaussians,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2401.05925_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[232] X. Hu, Y. Wang, L. Fan, J. Fan, J. Peng, Z. Lei, Q. Li, and\\n'\n",
      "          'Z. Zhang, “Semantic anything in 3d gaussians,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2401.17857_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[233] F. Palandra, A. Sanchietti, D. Baieri, and E. Rodol`a, '\n",
      "          '“Gsedit:\\n'\n",
      "          'Efficient text-guided editing of 3d objects via gaussian '\n",
      "          'splatting,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.05154_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[234] Q. Gu, Z. Lv, D. Frost, S. Green, J. Straub, and C. Sweeney, '\n",
      "          '“Egolifter: Open-world 3d segmentation for egocentric perception,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.18118_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[235] W. Lyu, X. Li, A. Kundu, Y.-H. Tsai, and M.-H. Yang, “Gaga:\\n'\n",
      "          'Group any gaussians via 3d-aware memory bank,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.07977_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[236] Z. Liu, H. Ouyang, Q. Wang, K. L. Cheng, J. Xiao, K. Zhu, N. '\n",
      "          'Xue,\\n'\n",
      "          'Y. Liu, Y. Shen, and Y. Cao, “Infusion: Inpainting 3d gaussians '\n",
      "          'via\\n'\n",
      "          'learning depth completion from diffusion prior,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.11613_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[237] D. Zhang, Z. Chen, Y.-J. Yuan, F.-L. Zhang, Z. He, S. Shan,\\n'\n",
      "          'and L. Gao, “Stylizedgs: Controllable stylization for 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2404.05220_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[238] Q. Zhang, Y. Xu, C. Wang, H.-Y. Lee, G. Wetzstein, B. Zhou,\\n'\n",
      "          'and C. Yang, “3ditscene: Editing any scene via language-guided\\n'\n",
      "          'disentangled gaussian splatting,” _arXiv preprint '\n",
      "          'arXiv:2405.18424_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[239] J. Wu, J.-W. Bian, X. Li, G. Wang, I. Reid, P. Torr, and V. '\n",
      "          'A.\\n'\n",
      "          'Prisacariu, “Gaussctrl: Multi-view consistent text-driven 3d '\n",
      "          'gaussian splatting editing,” in _Proc. Eur. Conf. Comput. Vis._, '\n",
      "          '2024, pp.\\n'\n",
      "          '55–71.\\n'\n",
      "          '\\n'\n",
      "          '[240] Y. Wang, X. Yi, Z. Wu, N. Zhao, L. Chen, and H. Zhang, '\n",
      "          '“Viewconsistent 3d editing with gaussian splatting,” in _Proc. Eur. '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis._, 2024, pp. 404–420.\\n'\n",
      "          '\\n'\n",
      "          '[241] R. Jena, G. S. Iyer, S. Choudhary, B. Smith, P. Chaudhari,\\n'\n",
      "          'and J. Gee, “Splatarmor: Articulated gaussian splatting for '\n",
      "          'animatable humans from monocular rgb videos,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2311.10812_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[242] K. Ye, T. Shao, and K. Zhou, “Animatable 3d gaussians\\n'\n",
      "          'for high-fidelity synthesis of human motions,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2311.13404_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[243] A. Moreau, J. Song, H. Dhamo, R. Shaw, Y. Zhou, and E. '\n",
      "          'P´erezPellitero, “Human gaussian splatting: Real-time rendering of\\n'\n",
      "          'animatable avatars,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[244] M. Kocabas, J.-H. R. Chang, J. Gabriel, O. Tuzel, and A. '\n",
      "          'Ranjan,\\n'\n",
      "          '“Hugs: Human gaussian splats,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[245] R. Abdal, W. Yifan, Z. Shi, Y. Xu, R. Po, Z. Kuang, Q. Chen, '\n",
      "          'D.Y. Yeung, and G. Wetzstein, “Gaussian shell maps for efficient\\n'\n",
      "          '3d human generation,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[246] S. Zheng, B. Zhou, R. Shao, B. Liu, S. Zhang, L. Nie, and Y. '\n",
      "          'Liu,\\n'\n",
      "          '“Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 21,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 21\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'for real-time human novel view synthesis,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[247] L. Hu, H. Zhang, Y. Zhang, B. Zhou, B. Liu, S. Zhang, and L. '\n",
      "          'Nie,\\n'\n",
      "          '“Gaussianavatar: Towards realistic human avatar modeling from\\n'\n",
      "          'a single video via animatable 3d gaussians,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[248] H. Pang, H. Zhu, A. Kortylewski, C. Theobalt, and M. '\n",
      "          'Habermann, “Ash: Animatable gaussian splats for efficient and '\n",
      "          'photoreal human rendering,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[249] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang, '\n",
      "          '“3dgsavatar: Animatable avatars via deformable 3d gaussian '\n",
      "          'splatting,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[250] H. Jung, N. Brasch, J. Song, E. Perez-Pellitero, Y. Zhou, Z. '\n",
      "          'Li,\\n'\n",
      "          'N. Navab, and B. Busam, “Deformable 3d gaussian splatting\\n'\n",
      "          'for animatable human avatars,” _arXiv preprint arXiv:2312.15059_,\\n'\n",
      "          '2023.\\n'\n",
      "          '\\n'\n",
      "          '[251] M. Li, J. Tao, Z. Yang, and Y. Yang, “Human101: Training\\n'\n",
      "          '100+ fps human gaussians in 100s from 1 view,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.15258_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[252] M. Li, S. Yao, Z. Xie, K. Chen, and Y.-G. Jiang, '\n",
      "          '“Gaussianbody:\\n'\n",
      "          'Clothed human reconstruction via 3d gaussian splatting,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2401.09720_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[253] J. Xiang, X. Gao, Y. Guo, and J. Zhang, “Flashavatar: '\n",
      "          'Highfidelity digital avatar rendering at 300fps,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.02214_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[254] Y. Chen, L. Wang, Q. Li, H. Xiao, S. Zhang, H. Yao, and\\n'\n",
      "          'Y. Liu, “Monogaussianavatar: Monocular gaussian point-based\\n'\n",
      "          'head avatar,” _arXiv preprint arXiv:2312.04558_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[255] Z. Zhao, Z. Bao, Q. Li, G. Qiu, and K. Liu, “Psavatar: A '\n",
      "          'pointbased morphable shape model for real-time head avatar '\n",
      "          'creation\\n'\n",
      "          'with 3d gaussian splatting,” _arXiv preprint arXiv:2401.12900_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[256] A. Rivero, S. Athar, Z. Shu, and D. Samaras, “Rig3dgs: '\n",
      "          'Creating controllable portraits from casual monocular videos,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2402.03723_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[257] H. Luo, M. Ouyang, Z. Zhao, S. Jiang, L. Zhang, Q. Zhang,\\n'\n",
      "          'W. Yang, L. Xu, and J. Yu, “Gaussianhair: Hair modeling and '\n",
      "          'rendering with light-aware gaussians,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2402.10483_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering '\n",
      "          'for\\n'\n",
      "          'stereo 3d reconstruction of deformable tissues in robotic '\n",
      "          'surgery,”\\n'\n",
      "          'in _Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv._, '\n",
      "          '2022,\\n'\n",
      "          'pp. 431–441.\\n'\n",
      "          '\\n'\n",
      "          '[259] C. Yang, K. Wang, Y. Wang, X. Yang, and W. Shen, “Neural\\n'\n",
      "          'lerplane representations for fast 4d reconstruction of deformable\\n'\n",
      "          'tissues,” in _Proc. Int. Conf. Med. Image Comput. Comput. Assist._\\n'\n",
      "          '_Interv._, 2023, pp. 46–56.\\n'\n",
      "          '\\n'\n",
      "          '[260] R. Zha, X. Cheng, H. Li, M. Harandi, and Z. Ge, “Endosurf:\\n'\n",
      "          'Neural surface reconstruction of deformable tissues with stereo\\n'\n",
      "          'endoscope videos,” in _Proc. Int. Conf. Med. Image Comput. '\n",
      "          'Comput._\\n'\n",
      "          '_Assist. Interv._, 2023, pp. 13–23.\\n'\n",
      "          '\\n'\n",
      "          '[261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, '\n",
      "          'J. J. Engel, R. Mur-Artal, C. Ren, S. Verma _et al._, “The replica '\n",
      "          'dataset: A\\n'\n",
      "          'digital replica of indoor spaces,” _arXiv preprint '\n",
      "          'arXiv:1906.05797_,\\n'\n",
      "          '2019.\\n'\n",
      "          '\\n'\n",
      "          '[262] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, “imap: '\n",
      "          'Implicit\\n'\n",
      "          'mapping and positioning in real-time,” in _Proc. IEEE Int. Conf._\\n'\n",
      "          '_Comput. Vis._, 2021, pp. 6229–6238.\\n'\n",
      "          '\\n'\n",
      "          '[263] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, '\n",
      "          '“Voxfusion: Dense tracking and mapping with voxel-based neural '\n",
      "          'implicit representation,” in _IEEE International Symposium on '\n",
      "          'Mixed_\\n'\n",
      "          '_and Augmented Reality_, 2022, pp. 499–507.\\n'\n",
      "          '\\n'\n",
      "          '[264] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. '\n",
      "          'Oswald,\\n'\n",
      "          'and M. Pollefeys, “Nice-slam: Neural implicit scalable encoding\\n'\n",
      "          'for slam,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2022,\\n'\n",
      "          'pp. 12 786–12 796.\\n'\n",
      "          '\\n'\n",
      "          '[265] M. M. Johari, C. Carta, and F. Fleuret, “Eslam: Efficient '\n",
      "          'dense\\n'\n",
      "          'slam system based on hybrid representation of signed distance\\n'\n",
      "          'fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2023,\\n'\n",
      "          'pp. 17 408–17 419.\\n'\n",
      "          '\\n'\n",
      "          '[266] E. Sandstr¨om, Y. Li, L. Van Gool, and M. R. Oswald, '\n",
      "          '“Point-slam:\\n'\n",
      "          'Dense neural point cloud-based slam,” in _Proc. IEEE Int. Conf._\\n'\n",
      "          '_Comput. Vis._, 2023, pp. 18 433–18 444.\\n'\n",
      "          '\\n'\n",
      "          '[267] H. Wang, J. Wang, and L. Agapito, “Co-slam: Joint coordinate '\n",
      "          'and\\n'\n",
      "          'sparse parametric encodings for neural real-time slam,” in _Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 13 293–13 '\n",
      "          '302.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[268] Y. Hu, Y. Fang, Z. Ge, Z. Qu, Y. Zhu, A. Pradhana, and C. '\n",
      "          'Jiang,\\n'\n",
      "          '“A moving least squares material point method with displacement '\n",
      "          'discontinuity and two-way rigid body coupling,” _ACM_\\n'\n",
      "          '_Trans. Graph._, vol. 37, no. 4, pp. 1–14, 2018.\\n'\n",
      "          '\\n'\n",
      "          '[269] M. M¨uller, B. Heidelberger, M. Hennix, and J. Ratcliff, '\n",
      "          '“Position\\n'\n",
      "          'based dynamics,” _Journal of Visual Communication and Image Rep-_\\n'\n",
      "          '_resentation_, vol. 18, no. 2, pp. 109–118, 2007.\\n'\n",
      "          '\\n'\n",
      "          '[270] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and\\n'\n",
      "          'temples: Benchmarking large-scale scene reconstruction,” _ACM_\\n'\n",
      "          '_Trans. Graph._, vol. 36, no. 4, pp. 1–13, 2017.\\n'\n",
      "          '\\n'\n",
      "          '[271] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, '\n",
      "          '“Stereo\\n'\n",
      "          'magnification: learning view synthesis using multiplane images,”\\n'\n",
      "          '_ACM Trans. Graph._, vol. 37, no. 4, pp. 1–12, 2018.\\n'\n",
      "          '\\n'\n",
      "          '[272] P. Hedman, J. Philip, T. Price, J.-M. Frahm, G. Drettakis, '\n",
      "          'and\\n'\n",
      "          'G. Brostow, “Deep blending for free-viewpoint image-based '\n",
      "          'rendering,” _ACM Trans. Graph._, vol. 37, no. 6, pp. 1–15, 2018.\\n'\n",
      "          '\\n'\n",
      "          '[273] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. '\n",
      "          'Kalantari,\\n'\n",
      "          'R. Ramamoorthi, R. Ng, and A. Kar, “Local light field fusion:\\n'\n",
      "          'Practical view synthesis with prescriptive sampling guidelines,”\\n'\n",
      "          '_ACM Trans. Graph._, vol. 38, no. 4, pp. 1–14, 2019.\\n'\n",
      "          '\\n'\n",
      "          '[274] A. Liu, R. Tucker, V. Jampani, A. Makadia, N. Snavely, and\\n'\n",
      "          'A. Kanazawa, “Infinite nature: Perpetual view generation of\\n'\n",
      "          'natural scenes from a single image,” in _Proc. IEEE Int. Conf._\\n'\n",
      "          '_Comput. Vis._, 2021, pp. 14 458–14 467.\\n'\n",
      "          '\\n'\n",
      "          '[275] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. '\n",
      "          'Cremers,\\n'\n",
      "          '“A benchmark for the evaluation of rgb-d slam systems,” in _Proc._\\n'\n",
      "          '_IEEE/RSJ Int. Conf. Intell. Robot. Syst._, 2012, pp. 573–580.\\n'\n",
      "          '\\n'\n",
      "          '[276] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for '\n",
      "          'autonomous driving? the kitti vision benchmark suite,” in _Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2012, pp. 3354–3361.\\n'\n",
      "          '\\n'\n",
      "          '[277] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and\\n'\n",
      "          'M. Nießner, “Scannet: Richly-annotated 3d reconstructions of\\n'\n",
      "          'indoor scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2017, pp. 5828–5839.\\n'\n",
      "          '\\n'\n",
      "          '[278] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. '\n",
      "          'Patnaik,\\n'\n",
      "          'P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine _et al._, “Scalability '\n",
      "          'in\\n'\n",
      "          'perception for autonomous driving: Waymo open dataset,” in\\n'\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2020, pp. 2446–\\n'\n",
      "          '2454.\\n'\n",
      "          '\\n'\n",
      "          '[279] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. '\n",
      "          'Xu,\\n'\n",
      "          'A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A\\n'\n",
      "          'multimodal dataset for autonomous driving,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2020, pp. 11 621–11 631.\\n'\n",
      "          '\\n'\n",
      "          '[280] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, “Rlbench:\\n'\n",
      "          'The robot learning benchmark & learning environment,” _IEEE_\\n'\n",
      "          '_Robotics and Automation Letters_, vol. 5, no. 2, pp. 3019–3026, '\n",
      "          '2020.\\n'\n",
      "          '\\n'\n",
      "          '[281] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. '\n",
      "          'Kulkarni,\\n'\n",
      "          'L. Fei-Fei, S. Savarese, Y. Zhu, and R. Mart´ın-Mart´ın, “What\\n'\n",
      "          'matters in learning from offline human demonstrations for robot\\n'\n",
      "          'manipulation,” in _Proc. Annu. Conf. Robot Learn._, 2022, pp. '\n",
      "          '1678–\\n'\n",
      "          '1690.\\n'\n",
      "          '\\n'\n",
      "          '[282] Z. Yan, C. Li, and G. H. Lee, “Nerf-ds: Neural radiance '\n",
      "          'fields\\n'\n",
      "          'for dynamic specular objects,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2023, pp. 8285–8295.\\n'\n",
      "          '\\n'\n",
      "          '[283] K. Kania, K. M. Yi, M. Kowalski, T. Trzci´nski, and A. '\n",
      "          'Tagliasacchi,\\n'\n",
      "          '“Conerf: Controllable neural radiance fields,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2022, pp. 18 623–18 632.\\n'\n",
      "          '\\n'\n",
      "          '[284] A. Mirzaei, T. Aumentado-Armstrong, K. G. Derpanis, J. '\n",
      "          'Kelly,\\n'\n",
      "          'M. A. Brubaker, I. Gilitschenski, and A. Levinshtein, “Spin-nerf:\\n'\n",
      "          'Multiview segmentation and perceptual inpainting with neural\\n'\n",
      "          'radiance fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2023, pp. 20 669–20 679.\\n'\n",
      "          '\\n'\n",
      "          '[285] R. Shao, Z. Zheng, H. Tu, B. Liu, H. Zhang, and Y. Liu, '\n",
      "          '“Tensor4d:\\n'\n",
      "          'Efficient neural 4d decomposition for high-fidelity dynamic '\n",
      "          'reconstruction and rendering,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2023, pp. 16 632–16 642.\\n'\n",
      "          '\\n'\n",
      "          '[286] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. '\n",
      "          'Yang,\\n'\n",
      "          'J. Wang, C. Qian _et al._, “Omniobject3d: Large-vocabulary 3d\\n'\n",
      "          'object dataset for realistic perception, reconstruction and '\n",
      "          'generation,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2023, pp.\\n'\n",
      "          '803–814.\\n'\n",
      "          '\\n'\n",
      "          '[287] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. '\n",
      "          'VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi,\\n'\n",
      "          '“Objaverse: A universe of annotated 3d objects,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 13 142–13 153.\\n'\n",
      "          '\\n'\n",
      "          '[288] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. '\n",
      "          'Pons-Moll,\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 22,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 22\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '“Video based reconstruction of 3d people models,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2018, pp. 8387–8397.\\n'\n",
      "          '\\n'\n",
      "          '[289] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. '\n",
      "          'Black,\\n'\n",
      "          '“Capture, learning, and synthesis of 3d speaking styles,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2019, pp. 10 101–10 '\n",
      "          '111.\\n'\n",
      "          '\\n'\n",
      "          '[290] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu, “Deephuman: 3d\\n'\n",
      "          'human reconstruction from a single image,” in _Proc. IEEE Int._\\n'\n",
      "          '_Conf. Comput. Vis._, 2019, pp. 7739–7749.\\n'\n",
      "          '\\n'\n",
      "          '[291] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, and Y. Liu, '\n",
      "          '“Function4d:\\n'\n",
      "          'Real-time human volumetric capture from very sparse consumer\\n'\n",
      "          'rgbd sensors,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2021, pp. 5746–5756.\\n'\n",
      "          '\\n'\n",
      "          '[292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. '\n",
      "          'Zhou,\\n'\n",
      "          '“Neural body: Implicit neural representations with structured\\n'\n",
      "          'latent codes for novel view synthesis of dynamic humans,” in\\n'\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2021, pp. 9054–\\n'\n",
      "          '9063.\\n'\n",
      "          '\\n'\n",
      "          '[293] E. Ramon, G. Triginer, J. Escur, A. Pumarola, J. Garcia, X. '\n",
      "          'Giro-i\\n'\n",
      "          'Nieto, and F. Moreno-Noguer, “H3d-net: Few-shot high-fidelity\\n'\n",
      "          '3d head reconstruction,” in _Proc. IEEE Int. Conf. Comput. Vis._,\\n'\n",
      "          '2021, pp. 5620–5629.\\n'\n",
      "          '\\n'\n",
      "          '[294] Z. Su, T. Yu, Y. Wang, and Y. Liu, “Deepcloth: Neural '\n",
      "          'garment\\n'\n",
      "          'representation for shape and style editing,” _IEEE Trans. Pattern_\\n'\n",
      "          '_Anal. Mach. Intell._, vol. 45, no. 2, pp. 1581–1593, 2022.\\n'\n",
      "          '\\n'\n",
      "          '[295] M. Allan, J. Mcleod, C. Wang, J. C. Rosenthal, Z. Hu, N. '\n",
      "          'Gard,\\n'\n",
      "          'P. Eisert, K. X. Fu, T. Zeffiro, W. Xia _et al._, “Stereo '\n",
      "          'correspondence\\n'\n",
      "          'and reconstruction of endoscopic data challenge,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2101.01133_, 2021.\\n'\n",
      "          '\\n'\n",
      "          '[296] Y. Cai, J. Wang, A. Yuille, Z. Zhou, and A. Wang, '\n",
      "          '“Structureaware sparse-view x-ray 3d reconstruction,” in _Proc. '\n",
      "          'IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024, pp. 11 174–11 183.\\n'\n",
      "          '\\n'\n",
      "          '[297] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. '\n",
      "          'Dai,\\n'\n",
      "          'and D. Lin, “Bungeenerf: Progressive neural radiance field for\\n'\n",
      "          'extreme multi-scale scene rendering,” in _Proc. Eur. Conf. '\n",
      "          'Comput._\\n'\n",
      "          '_Vis._ Springer, 2022, pp. 106–122.\\n'\n",
      "          '\\n'\n",
      "          '[298] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. '\n",
      "          'P.\\n'\n",
      "          'Srinivasan, J. T. Barron, and H. Kretzschmar, “Block-nerf: '\n",
      "          'Scalable\\n'\n",
      "          'large scene neural view synthesis,” in _Proc. IEEE Conf. Comput._\\n'\n",
      "          '_Vis. Pattern Recognit._, 2022, pp. 8248–8258.\\n'\n",
      "          '\\n'\n",
      "          '[299] G. Yang, F. Xue, Q. Zhang, K. Xie, C.-W. Fu, and H. Huang, '\n",
      "          '“Urbanbis: a large-scale benchmark for fine-grained urban building\\n'\n",
      "          'instance segmentation,” in _Proc. ACM Spec. Interest Group '\n",
      "          'Comput._\\n'\n",
      "          '_Graph. Interact. Tech._, 2023, pp. 1–11.\\n'\n",
      "          '\\n'\n",
      "          '[300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, '\n",
      "          '“Image\\n'\n",
      "          'quality assessment: from error visibility to structural '\n",
      "          'similarity,”\\n'\n",
      "          '_IEEE Trans. Image Process._, vol. 13, no. 4, pp. 600–612, 2004.\\n'\n",
      "          '\\n'\n",
      "          '[301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,\\n'\n",
      "          '“The unreasonable effectiveness of deep features as a perceptual\\n'\n",
      "          'metric,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2018,\\n'\n",
      "          'pp. 586–595.\\n'\n",
      "          '\\n'\n",
      "          '[302] J. Fang, T. Yi, X. Wang, L. Xie, X. Zhang, W. Liu, M. '\n",
      "          'Nießner, and\\n'\n",
      "          'Q. Tian, “Fast dynamic radiance fields with time-aware neural\\n'\n",
      "          'voxels,” in _SIGGRAPH Asia_, 2022, pp. 1–9.\\n'\n",
      "          '\\n'\n",
      "          '[303] A. Cao and J. Johnson, “Hexplane: A fast representation for '\n",
      "          'dynamic scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2023, pp. 130–141.\\n'\n",
      "          '\\n'\n",
      "          '[304] F. Wang, Z. Chen, G. Wang, Y. Song, and H. Liu, “Masked '\n",
      "          'spacetime hash encoding for efficient dynamic scene '\n",
      "          'reconstruction,”\\n'\n",
      "          'in _Proc. Adv. Neural Inf. Process. Syst._, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[305] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and\\n'\n",
      "          'I. Kemelmacher-Shlizerman, “Humannerf: Free-viewpoint rendering of '\n",
      "          'moving people from monocular video,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 16 210–16 220.\\n'\n",
      "          '\\n'\n",
      "          '[306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, “Learning '\n",
      "          'neural\\n'\n",
      "          'volumetric representations of dynamic humans in minutes,” in\\n'\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 8759–\\n'\n",
      "          '8770.\\n'\n",
      "          '\\n'\n",
      "          '[307] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and\\n'\n",
      "          'H. Bao, “Animatable neural radiance fields for modeling dynamic '\n",
      "          'human bodies,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2021,\\n'\n",
      "          'pp. 14 314–14 323.\\n'\n",
      "          '\\n'\n",
      "          '[308] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, “pixelnerf: Neural\\n'\n",
      "          'radiance fields from one or few images,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2021, pp. 4578–4587.\\n'\n",
      "          '\\n'\n",
      "          '[309] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, “Neural human\\n'\n",
      "          'performer: Learning generalizable radiance fields for human\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'performance rendering,” in _Proc. Adv. Neural Inf. Process. '\n",
      "          'Syst._,\\n'\n",
      "          '2021, pp. 24 741–24 752.\\n'\n",
      "          '\\n'\n",
      "          '[310] J. Wang, Z. Zhang, Q. Zhang, J. Li, J. Sun, M. Sun, J. He, '\n",
      "          'and R. Xu,\\n'\n",
      "          '“Query-based semantic gaussian field for scene representation in\\n'\n",
      "          'reinforcement learning,” _arXiv preprint arXiv:2406.02370_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[311] Y. Qu, S. Dai, X. Li, J. Lin, L. Cao, S. Zhang, and R. Ji, '\n",
      "          '“Goi: Find\\n'\n",
      "          '3d gaussians of interest with an optimizable open-vocabulary\\n'\n",
      "          'semantic-space hyperplane,” _arXiv preprint arXiv:2405.17596_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[312] Y. Ji, H. Zhu, J. Tang, W. Liu, Z. Zhang, Y. Xie, L. Ma, and\\n'\n",
      "          'X. Tan, “Fastlgs: Speeding up language embedded gaussians with\\n'\n",
      "          'feature grid mapping,” _arXiv preprint arXiv:2406.01916_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[313] G. Liao, J. Li, Z. Bao, X. Ye, J. Wang, Q. Li, and K. Liu,\\n'\n",
      "          '“Clip-gs: Clip-informed gaussian splatting for real-time and\\n'\n",
      "          'view-consistent 3d semantic understanding,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.14249_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[314] S. Choi, H. Song, J. Kim, T. Kim, and H. Do, '\n",
      "          '“Click-gaussian:\\n'\n",
      "          'Interactive segmentation to any 3d gaussians,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2407.11793_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[315] S. Ji, G. Wu, J. Fang, J. Cen, T. Yi, W. Liu, Q. Tian, and X. '\n",
      "          'Wang,\\n'\n",
      "          '“Segment any 4d gaussians,” _arXiv preprint arXiv:2407.04504_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[316] A. Gu´edon and V. Lepetit, “Sugar: Surface-aligned gaussian\\n'\n",
      "          'splatting for efficient 3d mesh reconstruction and high-quality\\n'\n",
      "          'mesh rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recog-_\\n'\n",
      "          '_nit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[317] T. Liu, G. Wang, S. Hu, L. Shen, X. Ye, Y. Zang, Z. Cao, W. '\n",
      "          'Li,\\n'\n",
      "          'and Z. Liu, “Fast generalizable gaussian splatting reconstruction\\n'\n",
      "          'from multi-view stereo,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[318] Y. Li, X. Fu, S. Zhao, R. Jin, and S. K. Zhou, “Sparse-view\\n'\n",
      "          'ct reconstruction with 3d gaussian volumetric representation,”\\n'\n",
      "          '_arXiv preprint arXiv:2312.15676_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[319] Y. Cai, Y. Liang, J. Wang, A. Wang, Y. Zhang, X. Yang, Z. '\n",
      "          'Zhou,\\n'\n",
      "          'and A. Yuille, “Radiative gaussian splatting for efficient x-ray\\n'\n",
      "          'novel view synthesis,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[320] J. Chang, Y. Xu, Y. Li, Y. Chen, and X. Han, “Gaussreg: Fast '\n",
      "          '3d\\n'\n",
      "          'registration with gaussian splatting,” in _Proc. Eur. Conf. '\n",
      "          'Comput._\\n'\n",
      "          '_Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []}]\n"
     ]
    }
   ],
   "source": [
    "chunks = pymupdf4llm.to_markdown(doc='data/raw_papers/A survey on 3DGS.pdf',\n",
    "                                 page_chunks=True)\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16aa1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'graphics': [],\n",
      "  'images': [{'bbox': Rect(335.5150146484375, 319.2890319824219, 593.9107666015625, 514.4761352539062),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': True,\n",
      "              'height': 1439,\n",
      "              'number': 10,\n",
      "              'size': 124122,\n",
      "              'transform': (258.3957214355469,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            195.18710327148438,\n",
      "                            335.5150146484375,\n",
      "                            319.2890319824219),\n",
      "              'width': 1905,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(314.4906921386719, 329.46209716796875, 573.0220947265625, 524.5135498046875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': True,\n",
      "              'height': 1438,\n",
      "              'number': 9,\n",
      "              'size': 477351,\n",
      "              'transform': (258.5313720703125,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            195.0514678955078,\n",
      "                            314.4906921386719,\n",
      "                            329.46209716796875),\n",
      "              'width': 1906,\n",
      "              'xres': 96,\n",
      "              'yres': 96}],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 1,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1\\n'\n",
      "          '\\n'\n",
      "          '## A Survey on 3D Gaussian Splatting\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Guikun Chen, and Wenguan Wang, _Senior Member, IEEE_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Abstract** —3D Gaussian splatting (GS) has emerged as a '\n",
      "          'transformative technique in explicit radiance field and computer '\n",
      "          'graphics.\\n'\n",
      "          'This innovative approach, characterized by the use of millions of '\n",
      "          'learnable 3D Gaussians, represents a significant departure from\\n'\n",
      "          'mainstream neural radiance field approaches, which predominantly '\n",
      "          'use implicit, coordinate-based models to map spatial coordinates\\n'\n",
      "          'to pixel values. 3D GS, with its explicit scene representation and '\n",
      "          'differentiable rendering algorithm, not only promises real-time\\n'\n",
      "          'rendering capability but also introduces unprecedented levels of '\n",
      "          'editability. This positions 3D GS as a potential game-changer for '\n",
      "          'the\\n'\n",
      "          'next generation of 3D reconstruction and representation. In the '\n",
      "          'present paper, we provide the first systematic overview of the '\n",
      "          'recent\\n'\n",
      "          'developments and critical contributions in the domain of 3D GS. We '\n",
      "          'begin with a detailed exploration of the underlying principles and\\n'\n",
      "          'the driving forces behind the emergence of 3D GS, laying the '\n",
      "          'groundwork for understanding its significance. A focal point of '\n",
      "          'our\\n'\n",
      "          'discussion is the practical applicability of 3D GS. By enabling '\n",
      "          'unprecedented rendering speed, 3D GS opens up a plethora of\\n'\n",
      "          'applications, ranging from virtual reality to interactive media and '\n",
      "          'beyond. This is complemented by a comparative analysis of leading\\n'\n",
      "          '3D GS models, evaluated across various benchmark tasks to highlight '\n",
      "          'their performance and practical utility. The survey concludes by\\n'\n",
      "          'identifying current challenges and suggesting potential avenues for '\n",
      "          'future research. Through this survey, we aim to provide a valuable\\n'\n",
      "          'resource for both newcomers and seasoned researchers, fostering '\n",
      "          'further exploration and advancement in explicit radiance field.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Index Terms** —3D Gaussian Splatting, Explicit Radiance Field, '\n",
      "          'Real-time Rendering, Scene Understanding\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '✦\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**1** **I** **NTRODUCTION**\\n'\n",
      "          '\\n'\n",
      "          '# T is to convert a collection of views or videos capturing HE '\n",
      "          'objective of image based 3D scene reconstruction\\n'\n",
      "          '\\n'\n",
      "          'a scene into a digital 3D model that can be computationally '\n",
      "          'processed, analyzed, and manipulated. This hard\\n'\n",
      "          'and long-standing problem is fundamental for machines\\n'\n",
      "          'to comprehend the complexity of real-world environments,\\n'\n",
      "          'facilitating a wide array of applications such as 3D modeling\\n'\n",
      "          'and animation, robot navigation, historical preservation,\\n'\n",
      "          'augmented/virtual reality, and autonomous driving.\\n'\n",
      "          'The journey of 3D scene reconstruction began long\\n'\n",
      "          'before the surge of deep learning, with early endeavors\\n'\n",
      "          'focusing on light fields and basic scene reconstruction methods '\n",
      "          '[1]–[3]. These early attempts, however, were limited by\\n'\n",
      "          'their reliance on dense sampling and structured capture,\\n'\n",
      "          'leading to significant challenges in handling complex scenes\\n'\n",
      "          'and lighting conditions. The emergence of structure-frommotion [4] '\n",
      "          'and subsequent advancements in multi-view\\n'\n",
      "          'stereo [5] algorithms provided a more robust framework for\\n'\n",
      "          '3D scene reconstruction. Despite these advancements, such\\n'\n",
      "          'methods struggled with novel-view synthesis and texture\\n'\n",
      "          'loss. NeRF represents a quantum leap in this progression.\\n'\n",
      "          'By leveraging deep neural networks, NeRF enabled the\\n'\n",
      "          'direct mapping of spatial coordinates to color and density.\\n'\n",
      "          'The success of NeRF hinged on its ability to create continuous, '\n",
      "          'volumetric scene functions, producing results with\\n'\n",
      "          'unprecedented fidelity. However, as with any burgeoning\\n'\n",
      "          'technology, this implementation came at a cost: **i** ) '\n",
      "          'Computational Intensity. NeRF based methods are computationally\\n'\n",
      "          'intensive [6]–[9], often requiring extensive training times\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ _G. Chen and W. Wang are with College of Computer Science and_\\n'\n",
      "          '_Technology, Zhejiang University (Email: guikunchen@gmail.com, '\n",
      "          'wen-_\\n'\n",
      "          '_guanwang.ai@gmail.com)_\\n'\n",
      "          '\\n'\n",
      "          '_•_ _Corresponding Author: Wenguan Wang_\\n'\n",
      "          '\\n'\n",
      "          '_•_ _Paper List:_ '\n",
      "          '_[https://github.com/guikunchen/Awesome3DGS](https://github.com/guikunchen/Awesome3DGS)_\\n'\n",
      "          '\\n'\n",
      "          '_•_ _[Benchmarks: '\n",
      "          'https://github.com/guikunchen/3DGS-Benchmarks](https://github.com/guikunchen/3DGS-Benchmarks)_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-0-0.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-0-1.png)\\n'\n",
      "          '\\n'\n",
      "          'Fig. 1. The number of published papers and official GitHub stars on '\n",
      "          '3D\\n'\n",
      "          '[GS. The set of statistics is sourced from # Papers and # GitHub '\n",
      "          'Stars.](https://github.com/Awesome3DGS/3D-Gaussian-Splatting-Papers)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'and substantial resources for rendering, especially for '\n",
      "          'highresolution outputs. **ii** ) Editability. Manipulating scenes '\n",
      "          'represented implicitly is challenging, since direct modifications\\n'\n",
      "          'to the neural network’s weights are not intuitively related to\\n'\n",
      "          'changes in geometric or appearance properties of the scene.\\n'\n",
      "          'It is in this context that 3D Gaussian splatting (GS) [10]\\n'\n",
      "          'emerges, not merely as an incremental improvement but as\\n'\n",
      "          'a paradigm-shifting approach that redefines the boundaries\\n'\n",
      "          'of scene representation and rendering. While NeRF excelled\\n'\n",
      "          'in creating photorealistic images, the need for faster, more\\n'\n",
      "          'efficient rendering methods was becoming increasingly apparent, '\n",
      "          'especially for applications ( _e.g_ ., virtual reality and\\n'\n",
      "          'autonomous driving) that are highly sensitive to latency.\\n'\n",
      "          '3D GS addressed this need by introducing an advanced,\\n'\n",
      "          'explicit scene representation that models a scene using\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 2,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'millions of learnable 3D Gaussians in space. Unlike the implicit, '\n",
      "          'coordinate-based models [11], [12], 3D GS employs an\\n'\n",
      "          'explicit representation and highly parallelized workflows,\\n'\n",
      "          'facilitating more efficient computation and rendering. The\\n'\n",
      "          'innovation of 3D GS lies in its unique blend of the benefits of '\n",
      "          'differentiable pipelines and point-based rendering\\n'\n",
      "          'techniques [13]–[17]. By representing scenes with learnable\\n'\n",
      "          '3D Gaussians, it preserves the strong fitting capability of\\n'\n",
      "          'continuous volumetric radiance fields, essential for highquality '\n",
      "          'image synthesis, while simultaneously avoiding\\n'\n",
      "          'the computational overhead associated with NeRF based\\n'\n",
      "          'methods ( _e.g_ ., computationally expensive ray-marching, and\\n'\n",
      "          'unnecessary calculations in empty space).\\n'\n",
      "          'The introduction of 3D GS is not just a technical advancement; it '\n",
      "          'represents a fundamental shift in how we approach\\n'\n",
      "          'scene representation and rendering in computer vision and\\n'\n",
      "          'graphics. By enabling real-time rendering capabilities without '\n",
      "          'compromising on visual quality, 3D GS opens up a\\n'\n",
      "          'plethora of possibilities for applications ranging from virtual '\n",
      "          'reality and augmented reality to real-time cinematic\\n'\n",
      "          'rendering and beyond [18]–[21]. This technology holds the\\n'\n",
      "          'promise of not only enhancing existing applications but\\n'\n",
      "          'also enabling new ones that were previously unfeasible\\n'\n",
      "          'due to computational constraints. Furthermore, 3D GS’s\\n'\n",
      "          'explicit scene representation offers unprecedented flexibility\\n'\n",
      "          'to control the objects and scene dynamics, a crucial factor in\\n'\n",
      "          'complex scenarios involving intricate geometries and varying '\n",
      "          'lighting conditions [22]–[24]. This level of editability,\\n'\n",
      "          'combined with the efficiency of the training and rendering\\n'\n",
      "          'process, positions 3D GS as a transformative force in shaping '\n",
      "          'future developments in relevant fields.\\n'\n",
      "          'In an effort to assist readers in keeping pace with the\\n'\n",
      "          'swift evolution of 3D GS, we provide the first survey on 3D\\n'\n",
      "          'GS, which presents a systematic and timely collection of the\\n'\n",
      "          'most significant literature on the topic. Given that 3D GS\\n'\n",
      "          'is a very recent innovation (Fig. 1), this survey focuses in\\n'\n",
      "          'particular on its principles, and the diverse developments\\n'\n",
      "          'and contributions that have emerged since its introduction.\\n'\n",
      "          'The selected follow-up works are primarily sourced from\\n'\n",
      "          'top-tier conferences, to provide a thorough and up-to-date\\n'\n",
      "          '(Dec. 2024) analysis of the theoretical foundations, remarkable '\n",
      "          'developments, and burgeoning applications of 3D GS.\\n'\n",
      "          'Acknowledging the nascent yet rapidly evolving nature of\\n'\n",
      "          '3D GS, this survey is inevitably a biased view, but we strive\\n'\n",
      "          'to offer a balanced perspective that reflects both the current\\n'\n",
      "          'state and the future potential of this field. Our aim is to\\n'\n",
      "          'encapsulate the primary research trends and serve as a valuable '\n",
      "          'resource for both researchers and practitioners eager to\\n'\n",
      "          'understand and contribute to this rapidly evolving domain.\\n'\n",
      "          'The distinctions of this survey from existing literature [25]–\\n'\n",
      "          '\\n'\n",
      "          '[28] are evident in the following aspects:\\n'\n",
      "          '\\n'\n",
      "          '_•_ We provide the first systematic and comprehensive review\\n'\n",
      "          'that examines 3D GS from a macro-level perspective by\\n'\n",
      "          'establishing clear taxonomies and frameworks. This highlevel '\n",
      "          'systematization helps researchers identify trends and\\n'\n",
      "          'potential directions that might not be apparent from paperspecific '\n",
      "          'reviews. Our organizational structure serves as a\\n'\n",
      "          'roadmap for understanding how different approaches relate\\n'\n",
      "          'to and build upon each other within the 3D GS ecosystem.\\n'\n",
      "          '\\n'\n",
      "          '_•_ This paper is the first and only survey to thoroughly delve\\n'\n",
      "          'into the theoretical background and fundamental principles\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-1-0.png)\\n'\n",
      "          '\\n'\n",
      "          'Fig. 2. Structure of the overall review.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'of 3D GS. The comprehensive coverage makes the field more\\n'\n",
      "          'approachable for newcomers while providing valuable insights for '\n",
      "          'experienced researchers.\\n'\n",
      "          '\\n'\n",
      "          '_•_ To ensure our survey remains relevant and offer longterm value '\n",
      "          'in this rapidly evolving field, we maintain two\\n'\n",
      "          '[dynamic GitHub repositories: one that follows our '\n",
      "          'survey’s](https://github.com/guikunchen/Awesome3DGS)\\n'\n",
      "          '[organizational structure and another that includes '\n",
      "          'compre-](https://github.com/guikunchen/3DGS-Benchmarks)\\n'\n",
      "          'hensive performance comparisons with analysis data.\\n'\n",
      "          '\\n'\n",
      "          'A summary of the structure of this article can be found\\n'\n",
      "          'in Fig. 2, which is presented as follows: Sec. 2 provides\\n'\n",
      "          'a brief background on problem formulation, terminology,\\n'\n",
      "          'and related research domains. Sec. 3 introduces the essential\\n'\n",
      "          'insights of 3D GS, encompassing the rendering process with\\n'\n",
      "          'learned 3D Gaussians and the optimization details ( _i.e_ ., how\\n'\n",
      "          'to learn 3D Gaussians) of 3D GS. Sec. 4 presents several\\n'\n",
      "          'fruitful directions that aim to improve the capabilities of\\n'\n",
      "          'the original 3D GS. Sec. 5 unveils the diverse application\\n'\n",
      "          'areas and tasks where 3D GS has made significant impacts,\\n'\n",
      "          'showcasing its versatility. Sec. 6 conducts performance comparison '\n",
      "          'and analysis. Finally, Sec. 7 and 8 highlight the open\\n'\n",
      "          'questions for further research and conclude the survey.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 3,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**2** **B** **ACKGROUND**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'In this section, we first provide a brief formulation of\\n'\n",
      "          'radiance fields (Sec. 2.1), including both implicit and explicit\\n'\n",
      "          'ones. Sec. 2.2 further establishes linkages with relevant rendering '\n",
      "          'algorithms and terminologies. For a comprehensive\\n'\n",
      "          'overview of radiance fields, scene reconstruction and '\n",
      "          'representation, and rendering methods, please see the excellent\\n'\n",
      "          'surveys [29]–[33] for more insights.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**2.1** **Radiance Field**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Implicit Radiance Field.** An implicit radiance field '\n",
      "          'represents light distribution in a scene without explicitly '\n",
      "          'defining\\n'\n",
      "          'the geometry of the scene. In the deep learning era, neural\\n'\n",
      "          'networks are often used to learn a continuous volumetric\\n'\n",
      "          'scene representation [34], [35]. The most prominent example\\n'\n",
      "          'is NeRF [12]. In NeRF (Fig. 3a), one or more MLPs are used\\n'\n",
      "          'to map a set of spatial coordinates ( _x, y, z_ ) and viewing\\n'\n",
      "          'directions ( _θ, ϕ_ ) to color _c_ and volume density _σ_ :\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '( _c, σ_ ) _←_ MLP ( _x, y, z, θ, ϕ_ ) _._ (1)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'This format allows for a differentiable and compact representation '\n",
      "          'of complex scenes, albeit often at the cost of high\\n'\n",
      "          'computational load due to volumetric ray marching. Note\\n'\n",
      "          'that typically, the color _c_ is direction-dependent, whereas\\n'\n",
      "          'the volume density _σ_ is not [12].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Explicit Radiance Field.** An explicit radiance field '\n",
      "          'directly\\n'\n",
      "          'represents the distribution of light in a discrete spatial '\n",
      "          'structure, such as a voxel grid or a set of points [36], [37]. '\n",
      "          'Each\\n'\n",
      "          'element in this structure stores the radiance information\\n'\n",
      "          'for its respective location. This allows for direct and often\\n'\n",
      "          'faster access to radiance data but at the cost of higher\\n'\n",
      "          'memory usage and potentially lower resolution. Similar to\\n'\n",
      "          'the implicit radiance field, the explicit one is written as:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '( _c, σ_ ) _←_ DataStructure ( _x, y, z, θ, ϕ_ ) _,_ (2)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where DataStructure could be in the format of volumes,\\n'\n",
      "          'point clouds, _etc_ . DataStructure encodes directional color\\n'\n",
      "          'in two main ways. One is encoding high-dimensional features that '\n",
      "          'are subsequently decoded by a lightweight MLP.\\n'\n",
      "          'Another one is directly storing coefficients of directional\\n'\n",
      "          'basis functions, such as spherical harmonics or spherical\\n'\n",
      "          'Gaussians, where the final color is computed as a function\\n'\n",
      "          'of these coefficients and the viewing direction.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **3D Gaussian Splatting: Best-of-Both Worlds.** 3D GS [10]\\n'\n",
      "          'is an explicit radiance field with the advantages of implicit\\n'\n",
      "          'radiance fields. Concretely, it leverages the strengths of both\\n'\n",
      "          'paradigms by utilizing _learnable_ 3D Gaussians as the basis\\n'\n",
      "          'elements of DataStructure. Note that 3D GS encodes the\\n'\n",
      "          'opacity _α_ directly for each Gaussian, as opposed to approaches of '\n",
      "          'first establishing density _σ_ and then computing\\n'\n",
      "          'opacity based on that density. As in previous reconstruction\\n'\n",
      "          'work, 3D Gaussians are optimized under the supervision of\\n'\n",
      "          'multi-view images to represent the scene. Such a 3D Gaussian based '\n",
      "          'differentiable pipeline combines the benefits of\\n'\n",
      "          'neural network based optimization and explicit, structured\\n'\n",
      "          'data storage. This hybrid approach aims to achieve realtime, '\n",
      "          'high-quality rendering and requires less training time,\\n'\n",
      "          'particularly for complex scenes and high-resolution outputs.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**2.2** **Context and Terminology**\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Volumetric rendering** aims to transform a 3D volumetric\\n'\n",
      "          'representation into an image by integrating radiance along\\n'\n",
      "          'camera rays. A camera ray _**r**_ ( _t_ ) can be parameterized as:\\n'\n",
      "          '_**r**_ ( _t_ )= _**o**_ + _t_ _**d**_ _, t_ _∈_ [ _t_ near _, t_ '\n",
      "          'far ] _,_ where _**o**_ represents the ray origin\\n'\n",
      "          '(camera center), _**d**_ is the ray direction, and _t_ indicates '\n",
      "          'the\\n'\n",
      "          'distance along the ray between near and far clipping planes.\\n'\n",
      "          'The pixel color _C_ ( _**r**_ ) is computed through a line '\n",
      "          'integral\\n'\n",
      "          'along the ray _**r**_ ( _t_ ), mathematically expressed as [12]:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_t_ far\\n'\n",
      "          '_C_ ( _**r**_ ) = _T_ ( _t_ ) _σ_ ( _**r**_ ( _t_ )) _c_ ( _**r**_ '\n",
      "          '( _t_ ) _,_ _**d**_ ) _dt,_ (3)\\n'\n",
      "          '� _t_ near\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _σ_ ( _**r**_ ( _t_ )) is the volume density at point _**r**_ '\n",
      "          '( _t_ ), _c_ ( _**r**_ ( _t_ ) _,_ _**d**_ )\\n'\n",
      "          'is the color at that point, and _T_ ( _t_ ) is the transmittance. '\n",
      "          'Raymarching directly approximates the volumetric rendering\\n'\n",
      "          'integral by systematically “stepping” along a ray and sampling the '\n",
      "          'scene’s properties at discrete intervals. NeRF [12]\\n'\n",
      "          'shares the same spirit of ray-marching and introduces\\n'\n",
      "          'importance sampling and positional encoding to improve\\n'\n",
      "          'the quality of synthesized images. While providing highquality '\n",
      "          'results, ray-marching is computationally expensive,\\n'\n",
      "          'especially for high-resolution images.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Point-based rendering** represents another class of rendering '\n",
      "          'algorithms, of which 3D GS introduces a notable implementation. Its '\n",
      "          'simplest form [38] rasterizes point clouds\\n'\n",
      "          'with a fixed size, which introduces drawbacks such as holes\\n'\n",
      "          'and rendering artifacts. Seminal works addressed these\\n'\n",
      "          'limitations through various methods, including: **i** ) splatting\\n'\n",
      "          'point primitives with a spatial extent [14], [15], [39], [40],\\n'\n",
      "          'and **ii** ) more recently, embedding neural features directly\\n'\n",
      "          'into points for subsequent network-based rendering [41],\\n'\n",
      "          '\\n'\n",
      "          '[42]. 3D GS uses 3D Gaussian as the point primitive that\\n'\n",
      "          'contains explicit attributes ( _e.g_ ., color and opacity) instead '\n",
      "          'of\\n'\n",
      "          'implicit neural features. The rendering approach, _i.e_ ., '\n",
      "          'pointbased _α_ -blending (exemplified in Eq. 5), shares the same\\n'\n",
      "          'image formation model as NeRF-style volumetric rendering\\n'\n",
      "          '(Eq. 3) [10], but demonstrates substantial speed advantages.\\n'\n",
      "          'This advantage originates from fundamental algorithmic\\n'\n",
      "          'differences. NeRFs approximate a line integral along a ray\\n'\n",
      "          'for each pixel, requiring expensive sampling. Point-based\\n'\n",
      "          'methods render point clouds using rasterization, which inherently '\n",
      "          'benefits from parallel computational strategies [43].\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**3** **3D G** **AUSSIAN** **S** **PLATTING** **: P** '\n",
      "          '**RINCIPLES**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '3D GS offers a breakthrough in real-time, high-resolution\\n'\n",
      "          'image rendering, without relying on deep neural networks.\\n'\n",
      "          'This section aims to provide essential insights of 3D GS. We\\n'\n",
      "          'first elaborate on how 3D GS synthesizes an image given\\n'\n",
      "          'well-constructed 3D Gaussians in Sec. 3.1, _i.e_ ., the forward\\n'\n",
      "          'process of 3D GS. Then, we introduce how to obtain wellconstructed '\n",
      "          '3D Gaussians for a given scene in Sec. 3.2, _i.e_ .,\\n'\n",
      "          'the optimization process of 3D GS.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**3.1** **Rendering with Learned 3D Gaussians**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Consider a scene represented by (millions of) optimized\\n'\n",
      "          '3D Gaussians. The objective is to generate an image from\\n'\n",
      "          'a specified camera pose. Recall that NeRFs approach this\\n'\n",
      "          'task through computationally demanding volumetric raymarching, '\n",
      "          'sampling 3D space points per pixel. Such a\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [{'bbox': Rect(88.35691833496094, 88.7434310913086, 123.12225341796875, 141.17108154296875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'DeviceRGB',\n",
      "              'has-mask': True,\n",
      "              'height': 297,\n",
      "              'number': 2,\n",
      "              'size': 6216,\n",
      "              'transform': (34.76533126831055,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            52.427650451660156,\n",
      "                            88.35691833496094,\n",
      "                            88.7434310913086),\n",
      "              'width': 197,\n",
      "              'xres': 96,\n",
      "              'yres': 96}],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 4,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Rendering by Pixels.** Before delving into the final version\\n'\n",
      "          'of 3D GS which utilizes several techniques to boost parallel\\n'\n",
      "          'computation, we first elaborate on its simpler form to offer\\n'\n",
      "          'insights into its basic working mechanism. Given the position of a '\n",
      "          'pixel _**x**_, its distance to all overlapping Gaussians,\\n'\n",
      "          '_i.e_ ., the depths of these Gaussians, can be computed through\\n'\n",
      "          'the viewing transformation matrix _**W**_, forming a sorted list\\n'\n",
      "          'of Gaussians _N_ . Then, _α_ -blending is adopted to compute\\n'\n",
      "          'the final color of this pixel:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-3-3.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-3-4.png)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_c_ & _σ_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Image Space\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Splatting\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '(b) 3D GS\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '(a) NeRF\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_n−_ 1\\n'\n",
      "          '�\\n'\n",
      "          '\\n'\n",
      "          '_j_ =1\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Fig. 3. NeRFs _vs_ . 3D GS. (a) NeRF samples along the ray and '\n",
      "          'then\\n'\n",
      "          'queries the MLP to obtain corresponding colors and densities, '\n",
      "          'which\\n'\n",
      "          'can be seen as a _backward_ mapping (ray tracing). (b) In contrast, '\n",
      "          '3D GS\\n'\n",
      "          'projects all 3D Gaussians into the image space ( _i.e_ ., '\n",
      "          'splatting) and then\\n'\n",
      "          'performs parallel rendering, which can be viewed as a _forward_ '\n",
      "          'mapping\\n'\n",
      "          '(rasterization). Best viewed in color.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'paradigm struggles with high-resolution image synthesis,\\n'\n",
      "          'failing to achieve real-time rendering, especially for platforms '\n",
      "          'with limited computing resources [10]. By contrast,\\n'\n",
      "          '3D GS begins by projecting these 3D Gaussians onto a pixelbased '\n",
      "          'image plane, a process termed “splatting” [39], [40]\\n'\n",
      "          '(see Fig. 3b). Afterwards, 3D GS sorts these Gaussians and\\n'\n",
      "          'computes the value for each pixel. As shown in Fig. 3, the\\n'\n",
      "          'rendering of NeRFs and 3D GS can be viewed as an inverse\\n'\n",
      "          'process of each other. In what follows, we begin with the\\n'\n",
      "          'definition of a 3D Gaussian, which is the minimal element\\n'\n",
      "          'of the scene representation in 3D GS. Next, we describe how\\n'\n",
      "          'these 3D Gaussians can be used for differentiable rendering.\\n'\n",
      "          'Finally, we introduce the acceleration technique used in 3D\\n'\n",
      "          'GS, which is the key to fast rendering.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Properties of 3D Gaussian.** A 3D Gaussian is characterized '\n",
      "          'by its center (position) _**µ**_, opacity _α_, 3D covariance\\n'\n",
      "          'matrix **Σ**, and color _c_ . _c_ is represented by spherical '\n",
      "          'harmonics for view-dependent appearance. All the properties\\n'\n",
      "          'are learnable and optimized through back-propagation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Frustum Culling.** Given a specified camera pose, this step\\n'\n",
      "          'determines which 3D Gaussians are outside the camera’s\\n'\n",
      "          'frustum. By doing so, 3D Gaussians outside the given view\\n'\n",
      "          'will not be involved in the subsequent computation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Splatting.** In this step, 3D Gaussians (ellipsoids) in 3D\\n'\n",
      "          'space are projected into 2D image space (ellipses). The projection '\n",
      "          'proceeds through two transformations: first, transforming 3D '\n",
      "          'Gaussians from world coordinates to camera\\n'\n",
      "          'coordinates using the viewing transformation, and subsequently '\n",
      "          'splatting these Gaussians into 2D image space via\\n'\n",
      "          'an approximation of the projective transformation. Mathematically, '\n",
      "          'given the 3D covariance matrix **Σ** describing a 3D\\n'\n",
      "          'Gaussian’s spatial distribution, and the viewing transformation '\n",
      "          'matrix _**W**_, the 2D covariance matrix **Σ** _[′]_ '\n",
      "          'characterizing\\n'\n",
      "          'the projected 2D Gaussian is computed through:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Σ** _[′]_ = _**JW**_ **Σ** _**W**_ _[⊤]_ _**J**_ _[⊤]_ _,_ (4)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _**J**_ is the Jacobian of the affine approximation of the\\n'\n",
      "          'projective transformation [10], [39]. One might wonder why\\n'\n",
      "          'the standard camera intrinsics based projective transformation is '\n",
      "          'not used here. This is because its mappings are not\\n'\n",
      "          'affine and therefore cannot directly project **Σ** . 3D GS adopts\\n'\n",
      "          'an affine one proposed in [39] which approximates the projective '\n",
      "          'transformation using the first two terms (including\\n'\n",
      "          '_**J**_ ) of the Taylor expansion (see Sec. 4.4 in [39]).\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_C_ =\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_|N |_\\n'\n",
      "          '� _c_ _n_ _α_ _n_ _[′]_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_n_ =1\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '� 1 _−_ _α_ _j′_ � _,_ (5)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _c_ _n_ is the learned color. The final opacity _α_ _n_ _[′]_ '\n",
      "          '[is the]\\n'\n",
      "          'multiplication result of the learned opacity _α_ _n_ and the\\n'\n",
      "          'Gaussian, defined as follows:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '1\\n'\n",
      "          '_α_ _n_ _[′]_ [=] _[ α]_ _[n]_ _[×]_ [ exp] � _−_ _n_ [)] _[⊤]_ '\n",
      "          '**[Σ]** _[′−]_ _n_ [1] ( _**x**_ _[′]_ _−_ _**µ**_ _[′]_ _n_ [)] � '\n",
      "          '_,_ (6)\\n'\n",
      "          '\\n'\n",
      "          '2 [(] _**[x]**_ _[′]_ _[ −]_ _**[µ]**_ _[′]_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _**x**_ _[′]_ and _**µ**_ _[′]_ _n_ [are coordinates in the '\n",
      "          'projected space. It]\\n'\n",
      "          'is a reasonable concern that the rendering process described\\n'\n",
      "          'could be slower compared to NeRFs, given that generating\\n'\n",
      "          'the required sorted list is hard to parallelize. Indeed, this\\n'\n",
      "          'concern is justified; rendering speeds can be significantly\\n'\n",
      "          'impacted when utilizing such a simplistic, pixel-by-pixel\\n'\n",
      "          'approach. To achieve real-time rendering, 3D GS makes\\n'\n",
      "          'several concessions to accommodate parallel computation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Tiles (Patches).** To avoid the cost computation of deriving\\n'\n",
      "          'Gaussians for each pixel, 3D GS shifts the precision from\\n'\n",
      "          'pixel-level to patch-level detail, which is inspired by tilebased '\n",
      "          'rasterization [43]. Concretely, 3D GS initially divides\\n'\n",
      "          'the image into multiple non-overlapping patches (tiles).\\n'\n",
      "          'Fig. 4b provides an illustration of tiles. Each tile comprises\\n'\n",
      "          '16 _×_ 16 pixels as suggested in [10]. 3D GS further determines\\n'\n",
      "          'which **tiles** intersect with these projected Gaussians. Given\\n'\n",
      "          'that a projected Gaussian may cover several tiles, a logical\\n'\n",
      "          'method involves replicating the Gaussian, assigning each\\n'\n",
      "          'copy an identifier ( _i.e_ ., a tile ID) for the relevant tile.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Parallel Rendering.** After replication, 3D GS combines\\n'\n",
      "          'the respective tile ID with the depth value obtained from\\n'\n",
      "          'the view transformation for each Gaussian. This results in\\n'\n",
      "          'an unsorted list of bytes where the upper bits represent\\n'\n",
      "          'the tile ID and the lower bits signify depth. By doing so,\\n'\n",
      "          'the sorted list can be directly utilized for rendering ( _i.e_ .,\\n'\n",
      "          'alpha compositing). Fig. 4c and Fig. 4d provide the visual\\n'\n",
      "          'demonstration of such concepts. It’s worth highlighting that\\n'\n",
      "          'rendering each tile and pixel occurs **independently**, making\\n'\n",
      "          'this process highly suitable for parallel computations. An\\n'\n",
      "          'additional benefit is that each tile’s pixels can access a\\n'\n",
      "          'common **shared memory** and maintain an **uniform read**\\n'\n",
      "          '**sequence** (Fig. 5), enabling parallel execution of alpha '\n",
      "          'compositing with increased efficiency. In the official '\n",
      "          'implementation of the original paper [10], the framework regards '\n",
      "          'the\\n'\n",
      "          'processing of tiles and pixels as analogous to the blocks and\\n'\n",
      "          'threads, respectively, in CUDA programming architecture.\\n'\n",
      "          'In a nutshell, 3D GS introduces several approximations\\n'\n",
      "          'during rendering to enhance computational efficiency while\\n'\n",
      "          'maintaining a high standard of image synthesis quality.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**3.2** **Optimization of 3D Gaussian Splatting**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'At the heart of 3D GS lies an optimization procedure devised to '\n",
      "          'construct a copious collection of 3D Gaussians that\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [{'bbox': Rect(85.88441467285156, 120.70323181152344, 143.8795928955078, 178.6984100341797),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'DeviceRGB',\n",
      "              'has-mask': True,\n",
      "              'height': 305,\n",
      "              'number': 38,\n",
      "              'size': 1081,\n",
      "              'transform': (57.99517822265625,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            57.99517822265625,\n",
      "                            85.88441467285156,\n",
      "                            120.70323181152344),\n",
      "              'width': 305,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(71.31451416015625, 48.99148178100586, 109.46174621582031, 111.53778076171875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'DeviceRGB',\n",
      "              'has-mask': True,\n",
      "              'height': 329,\n",
      "              'number': 30,\n",
      "              'size': 51861,\n",
      "              'transform': (38.14723587036133,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            62.54629898071289,\n",
      "                            71.31451416015625,\n",
      "                            48.99148178100586),\n",
      "              'width': 201,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(104.04621887207031, 133.63125610351562, 144.57015991210938, 179.5570068359375),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'DeviceRGB',\n",
      "              'has-mask': True,\n",
      "              'height': 221,\n",
      "              'number': 37,\n",
      "              'size': 16365,\n",
      "              'transform': (16.4140567779541,\n",
      "                            -11.493159294128418,\n",
      "                            24.109874725341797,\n",
      "                            34.432594299316406,\n",
      "                            104.04621887207031,\n",
      "                            145.12442016601562),\n",
      "              'width': 105,\n",
      "              'xres': 96,\n",
      "              'yres': 96}],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 5,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [{'bbox': (3.6166000366210938,\n",
      "                       94.6661605834961,\n",
      "                       559.9910278320312,\n",
      "                       401.82958984375),\n",
      "              'columns': 2,\n",
      "              'rows': 2},\n",
      "             {'bbox': (69.34789276123047,\n",
      "                       260.0520324707031,\n",
      "                       146.24896240234375,\n",
      "                       288.3853454589844),\n",
      "              'columns': 2,\n",
      "              'rows': 2},\n",
      "             {'bbox': (216.38955688476562,\n",
      "                       309.7534484863281,\n",
      "                       294.7477111816406,\n",
      "                       337.3543701171875),\n",
      "              'columns': 2,\n",
      "              'rows': 2},\n",
      "             {'bbox': (260.1175537109375,\n",
      "                       115.7151107788086,\n",
      "                       330.50714111328125,\n",
      "                       172.6157684326172),\n",
      "              'columns': 2,\n",
      "              'rows': 3},\n",
      "             {'bbox': (353.526123046875,\n",
      "                       115.49913787841797,\n",
      "                       423.9157409667969,\n",
      "                       172.6157684326172),\n",
      "              'columns': 2,\n",
      "              'rows': 3}],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '(a)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-4-2.png)\\n'\n",
      "          '\\n'\n",
      "          'Splatting\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Image Space 3D Gaussians (c) (d)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Sorted 2D Gaussians\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Tile 1 Depth\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Tile 1 Depth\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Tile1\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_C_ 1 _C_ 2\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_C_ 3 _C_ 4\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-4-1.png)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Replication\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Tile 1** **Depth**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Tile 2** **Depth**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Tile 2|Depth|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '|Tile 3|Depth|\\n'\n",
      "          '|Tile 4|Depth|\\n'\n",
      "          '|Tile 3|Depth|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Tile 2|Depth|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '|Tile 3|Depth|\\n'\n",
      "          '|Tile 3|Depth|\\n'\n",
      "          '|Tile 4|Depth|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-4-3.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-4-4.png)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Tile 1|Depth|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '|Tile 1|Depth|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Tile 1|Depth|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '|**Tile 1**|**Depth**|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'through the list of Gaussians just once. The computation for the '\n",
      "          'red\\n'\n",
      "          '\\n'\n",
      "          '|Col1|Parallel Rendering|\\n'\n",
      "          '|---|---|\\n'\n",
      "          '||Parallel Rendering<br>_C_1 =_α′_<br>1_c_1 +_ α′_<br>1_c_2(1_ '\n",
      "          '−α′_<br>1)<br>_C_2 =_α′_<br>2_c_1 +_ α′_<br>2_c_2(1_ '\n",
      "          '−α′_<br>2)<br>_C_3 =_α′_<br>3_c_1 +_ α′_<br>3_c_2(1_ '\n",
      "          '−α′_<br>3)<br>_C_4 =_α′_<br>4_c_1 +_ α′_<br>4_c_2(1_ −α′_<br>4)|\\n'\n",
      "          '|**Depth**<br>**Depth**<br>Tile3<br>Tile4<br>Depth<br>Tile '\n",
      "          '3<br>Depth<br>Tile 4<br>Depth<br>Tile 4<br>Depth<br>Tile 3<br>_C_3 '\n",
      "          '=_α′_<br>3_c_1 +_ α′_<br>3_c_2(<br>_C_4 =_α′_<br>4_c_1 +_ '\n",
      "          'α′_<br>4_c_2(<br>Fig. 4. An illustration of the forward process of '\n",
      "          '3D GS (see Sec. 3.1). (a) The splatting step projects 3D Gaussians '\n",
      "          'into image space.<br>divides the image into multiple '\n",
      "          'non-overlapping patches,_ i.e_., tiles. (c) 3D GS replicates the '\n",
      "          'Gaussians which cover several tiles, assig<br>copy an identifer,_ '\n",
      "          'i.e_., a tile ID. (d) By rendering the sorted Gaussians, we can '\n",
      "          'obtain all pixels within the tile. Note that the '\n",
      "          'computational<br>for pixels and tiles are independent and can be '\n",
      "          'done in parallel. Best viewed in color.<br>**Depth**<br>**Tile '\n",
      "          '1**<br>Sorted 2D Gaussians<br>Tile1<br>Depth<br>Tile '\n",
      "          '1<br>Depth<br>Tile 1<br>Shared<br>……<br>**Depth**<br>**Tile '\n",
      "          '1**<br>Uniform '\n",
      "          'Read<br>Sequence<br>Overlap<br>Non-overlap<br>Parallel '\n",
      "          'Access<br>Pass<br>Eq.<br>Pass<br>Pass<br>5<br>Fig. 5. An '\n",
      "          'illustration of the tile based parallel (at the pixel-level) '\n",
      "          'ren-<br>dering. All the pixels within a tile (Tile1 here) access '\n",
      "          'the same ordered<br>Gaussian list stored in a shared memory for '\n",
      "          'rendering. As the system<br>processes each Gaussian sequentially, '\n",
      "          'every pixel in the tile evaluates<br>the Gaussian’s contribution '\n",
      "          'according to the distance (_i.e_., the exp term<br>in Eq. 6). '\n",
      "          'Therefore, the rendering for a tile can be completed by '\n",
      "          'iterating<br><br>which would not adhere to the physical '\n",
      "          'interpreta<br>ically associated with covariance matrices. To '\n",
      "          'cir<br>this issue, 3D GS chooses to optimize a quaternio<br>a 3D '\n",
      "          'vector**_ s_**. Here**_ q_** and**_ s_** represent rotation '\n",
      "          'a<br>respectively. This approach allows the covariance<br>to be '\n",
      "          'reconstructed as follows:<br>**Σ** =**_ '\n",
      "          'RSS_**_⊤_**_R_**_⊤,_<br>where**_ R_** is the rotation matrix '\n",
      "          'derived from the qu<br>**_q_**, and**_ S_** is the scaling matrix '\n",
      "          'given by diag(**_s_**).<br>there is a complex computational graph '\n",
      "          'to obtain th<br>_α_,_ i.e_.,**_ q_** and**_ s_**_ →_**Σ**,** Σ**_ '\n",
      "          '→_**Σ**_′_, and** Σ**_′ →α_. To avoid<br>of automatic '\n",
      "          'differentiation, 3D GS derives the grad<br>**_q_** and**_ s_** so '\n",
      "          'as to compute them directly during opti|_C_3 =_α′_<br>3_c_1 +_ '\n",
      "          'α′_<br>3_c_2(<br>_C_4 =_α′_<br>4_c_1 +_ α′_<br>4_c_2(|\\n'\n",
      "          '\\n'\n",
      "          'Gaussian follows a similar way and is omitted here for simplicity.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'accurately captures the scene’s essence, thereby facilitating\\n'\n",
      "          'free-viewpoint rendering. On the one hand, the properties\\n'\n",
      "          'of 3D Gaussians should be optimized via differentiable\\n'\n",
      "          'rasterization to fit the textures of a given scene. On the\\n'\n",
      "          'other hand, the number of 3D Gaussians that can represent a\\n'\n",
      "          'given scene well is unknown in advance. We will introduce\\n'\n",
      "          'how to optimize the properties of each Gaussian in Sec. 3.2.1\\n'\n",
      "          'and how to adaptively control the density of the Gaussians\\n'\n",
      "          'in Sec. 3.2.2. The two procedures are interleaved within the\\n'\n",
      "          'optimization workflow. Since there are many manually set\\n'\n",
      "          'hyperparameters in the optimization process, we omit the\\n'\n",
      "          'notations of most hyperparameters for clarity.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_3.2.1_ _Parameter Optimization_\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Loss Function.** Once the synthesis of the image is '\n",
      "          'completed, the difference between the rendered image and\\n'\n",
      "          'ground truth can be measured. All the learnable parameters\\n'\n",
      "          'are optimized by stochastic gradient descent using the _ℓ_ 1\\n'\n",
      "          'and D-SSIM loss functions:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_L_ = (1 _−_ _λ_ ) _L_ 1 + _λL_ D-SSIM _,_ (7)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'where _λ ∈_ [0 _,_ 1] is a weighting factor.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Parameter Update.** Most properties of a 3D Gaussian\\n'\n",
      "          'can be optimized directly through back-propagation. It is\\n'\n",
      "          'essential to note that directly optimizing the covariance\\n'\n",
      "          'matrix **Σ** can result in a non-positive semi-definite matrix,\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_3.2.2_ _Density Control_\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Initialization.** 3D GS starts with the initial set of '\n",
      "          'sparse\\n'\n",
      "          'points from SfM or random initialization. Note that a good\\n'\n",
      "          'initialization is essential to convergence and reconstruction\\n'\n",
      "          'quality [44]. Afterwards, point densification and pruning are\\n'\n",
      "          'adopted to control the density of 3D Gaussians.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Point Densification.** In the point densification phase, 3D\\n'\n",
      "          'GS adaptively increases the density of Gaussians to better\\n'\n",
      "          'capture the details of a scene. This process focuses on areas\\n'\n",
      "          'with missing geometric features or regions where Gaussians\\n'\n",
      "          'are too spread out. The densification procedure will be\\n'\n",
      "          'performed at regular intervals ( _i.e_ ., after a certain number '\n",
      "          'of\\n'\n",
      "          'training iterations), focusing on those Gaussians with large\\n'\n",
      "          'view-space positional gradients ( _i.e_ ., above a specific '\n",
      "          'threshold). It involves either cloning small Gaussians in '\n",
      "          'underreconstructed areas or splitting large Gaussians in '\n",
      "          'overreconstructed regions. For cloning, a copy of the Gaussian\\n'\n",
      "          'is created and moved towards the positional gradient. For\\n'\n",
      "          'splitting, a large Gaussian is replaced with two smaller ones,\\n'\n",
      "          'reducing their scale by a specific factor. This step seeks an\\n'\n",
      "          'optimal distribution and representation of Gaussians in 3D\\n'\n",
      "          'space, enhancing the overall quality of the reconstruction.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Point Pruning.** The point pruning stage involves the\\n'\n",
      "          'removal of superfluous or less impactful Gaussians, which\\n'\n",
      "          'can be viewed as a regularization process. It is executed by\\n'\n",
      "          'eliminating Gaussians that are virtually transparent (with _α_\\n'\n",
      "          'below a specified threshold) and those that are excessively\\n'\n",
      "          'large in either world-space or view-space. In addition, to\\n'\n",
      "          'prevent unjustified increases in Gaussian density near input\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 6,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'cameras, the alpha value of the Gaussians is set close to\\n'\n",
      "          'zero after a certain number of iterations. This allows for a\\n'\n",
      "          'controlled increase in the density of necessary Gaussians\\n'\n",
      "          'while enabling the culling of redundant ones. The process\\n'\n",
      "          'not only helps in conserving computational resources but\\n'\n",
      "          'also ensures that the Gaussians in the model remain precise\\n'\n",
      "          'and effective for the representation of the scene.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4** **3D G** **AUSSIAN** **S** **PLATTING** **: D** '\n",
      "          '**IRECTIONS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Though 3D GS has achieved impressive milestones, significant room '\n",
      "          'for improvement remains, _e.g_ ., data and hardware\\n'\n",
      "          'requirement, rendering and optimization algorithm, and applications '\n",
      "          'in downstream tasks. In the subsequent sections,\\n'\n",
      "          'we seek to elaborate on select extended versions. These are:\\n'\n",
      "          '**i** ) 3D GS for Sparse Input [45]–[55] (Sec. 4.1), **ii** ) '\n",
      "          'Memoryefficient 3D GS [56]–[64] (Sec. 4.2), **iii** ) '\n",
      "          'Photorealistic 3D\\n'\n",
      "          'GS [65]–[80] (Sec. 4.3), **iv** ) Improved Optimization Algorithms '\n",
      "          '[22], [77], [81]–[86] (Sec. 4.4), **v** ) 3D Gaussian with\\n'\n",
      "          'More Properties [87]–[93] (Sec. 4.5), **vi** ) Hybrid '\n",
      "          'Representation [94]–[96] (Sec. 4.6), and **vii** ) New Rendering '\n",
      "          'Algorithm\\n'\n",
      "          '(Sec. 4.7). While we have carefully selected several key\\n'\n",
      "          'directions, we acknowledge that it is inevitably a biased\\n'\n",
      "          '[view. A more comprehensive collection is given in '\n",
      "          'Github.](https://github.com/guikunchen/Awesome3DGS)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.1** **3D GS for Sparse Input**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'A notable issue of 3D GS is the emergence of artifacts in\\n'\n",
      "          'areas with insufficient observational data. This challenge is\\n'\n",
      "          'a prevalent limitation in radiance field rendering, where\\n'\n",
      "          'sparse data often leads to inaccuracies in reconstruction.\\n'\n",
      "          'From a practical perspective, reconstructing scenes from\\n'\n",
      "          'limited viewpoints is of significant interest, particularly for\\n'\n",
      "          'the potential to enhance functionality with minimal input.\\n'\n",
      "          'Existing methods can be categorized into two primary\\n'\n",
      "          'groups. **i) Regularization** based methods introduce additional '\n",
      "          'constraints such as depth information to enhance\\n'\n",
      "          'the detail and global consistency [46], [49], [51], [55]. For\\n'\n",
      "          'example, DNGaussian [49] introduced a depth-regularized\\n'\n",
      "          'approach to address the challenge of geometry degradation\\n'\n",
      "          'in sparse input. FSGS [46] devised a Gaussian Unpooling\\n'\n",
      "          'process for initialization and also introduced depth '\n",
      "          'regularization. MVSplat [51] proposed a cost volume representation '\n",
      "          'so as to provide geometry cues. Unfortunately, when\\n'\n",
      "          'dealing with a limited number of views, or even just one,\\n'\n",
      "          'the efficacy of regularization techniques tends to diminish,\\n'\n",
      "          'which leads to **ii) generalizability** based methods that use\\n'\n",
      "          'learned priors [47], [48], [53], [97]. One approach involves\\n'\n",
      "          'synthesizing additional views through generative models,\\n'\n",
      "          'which can be seamlessly integrated into existing reconstruction '\n",
      "          'pipelines [98]. However, this augmentation strategy is\\n'\n",
      "          'computationally intensive and inherently bounded by the\\n'\n",
      "          'capabilities of the used generative model. Another wellknown '\n",
      "          'paradigm employs feed-forward Gaussian model to\\n'\n",
      "          'directly generates the properties of a set of 3D Gaussians.\\n'\n",
      "          'This paradigm typically requires multiple views for training\\n'\n",
      "          'but can reconstruct 3D scenes with only one input image.\\n'\n",
      "          'For instance, PixelSplat [47] proposed to sample Gaussians\\n'\n",
      "          'from dense probability distributions. Splatter Image [48] '\n",
      "          'introduced a 2D image-to-image network that maps an input\\n'\n",
      "          'image to a 3D Gaussian per pixel. However, as the generated\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'pixel-aligned Gaussians are distributed nearly evenly in the\\n'\n",
      "          'space, they struggle to represent high-frequency details and\\n'\n",
      "          'smoother regions with an appropriate number of Gaussians.\\n'\n",
      "          '\\n'\n",
      "          'The challenge of 3D GS for sparse inputs centers on\\n'\n",
      "          'the modeling of priors, whether through depth information,\\n'\n",
      "          'generative models, or feed-forward Gaussian models. The\\n'\n",
      "          'fundamental trade-off lies between overfitting to available\\n'\n",
      "          'views and using learned priors for generalization. Future\\n'\n",
      "          'research could explore adaptive mechanisms for controlling\\n'\n",
      "          'this trade-off, potentially through learned confidence measures, '\n",
      "          'context-aware prior selection, user preferences, _etc_ . In\\n'\n",
      "          'addition, while current methods focus on static scenes, extending '\n",
      "          'these approaches to dynamic scenarios presents an\\n'\n",
      "          'exciting frontier for investigation, particularly in handling\\n'\n",
      "          'temporal consistency and motion-induced artifacts.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.2** **Memory-efficient 3D GS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'While 3D GS demonstrates remarkable capabilities, its scalability '\n",
      "          'poses significant challenges, particularly when juxtaposed with '\n",
      "          'NeRF-based methods. The latter benefits from\\n'\n",
      "          'the simplicity of storing merely the parameters of a learned\\n'\n",
      "          'MLP. This scalability issue becomes increasingly acute in\\n'\n",
      "          'the context of large-scale scene management, where the\\n'\n",
      "          'computational and memory demands escalate substantially.\\n'\n",
      "          'Consequently, there is an urgent need to optimize memory\\n'\n",
      "          'usage in both model training and storage.\\n'\n",
      "          'Recent research has pursued two primary directions to\\n'\n",
      "          'address memory efficiency. First, several approaches focus\\n'\n",
      "          'on **reducing the number of 3D Gaussians** [58], [62], [63].\\n'\n",
      "          'These methods either employ strategic pruning of lowimpact '\n",
      "          'Gaussians, such as the volume-based masking [58],\\n'\n",
      "          'or represent neighboring Gaussians using the same properties stored '\n",
      "          'within a “local anchor” obtained by clustering [22], hash-grid '\n",
      "          '[62], _etc_ . Second, researchers have developed methods for '\n",
      "          '**compressing Gaussian’s properties** [58],\\n'\n",
      "          '\\n'\n",
      "          '[61], [62]. For instance, Niedermayr _et al_ . [61] compressed\\n'\n",
      "          'color and Gaussian parameters into compact codebooks,\\n'\n",
      "          'using sensitivity measures for effective quantization and\\n'\n",
      "          'fine-tuning. HAC [62] predicted the probability of each\\n'\n",
      "          'quantized attribute using Gaussian distributions and then\\n'\n",
      "          'devise an adaptive quantization module. These directions\\n'\n",
      "          'are not mutually exclusive; instead, one framework might\\n'\n",
      "          'use a hybrid approach combining multiple strategies.\\n'\n",
      "          '\\n'\n",
      "          'While current compression techniques have achieved\\n'\n",
      "          'significant storage reduction ratios (often by factors of 1020 _×_ '\n",
      "          '), several challenges remain. The field particularly needs\\n'\n",
      "          'advances in memory efficiency during the training phase,\\n'\n",
      "          'potentially through quantization-aware training protocols,\\n'\n",
      "          'the development of scene-agnostic, reusable codebooks, _etc_ .\\n'\n",
      "          'Furthermore, optimizing the trade-off between compression\\n'\n",
      "          'efficiency and visual fidelity remains an open problem.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.3** **Photorealistic 3D GS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The current rendering pipeline of 3D GS (Sec. 3.1) is '\n",
      "          'straightforward and involves several drawbacks. For instance, the\\n'\n",
      "          'simple visibility algorithm may lead to a drastic switch\\n'\n",
      "          'in the depth/blending order of Gaussians [10]. The visual\\n'\n",
      "          'fidelity of rendered images, including aspects such as aliasing, '\n",
      "          'reflections, and artifacts, can be further optimized.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 7,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Recent research has focused on addressing three main\\n'\n",
      "          'aspects of visual quality, with aliasing being specific to 3D\\n'\n",
      "          'GS’s rendering algorithm, while reflection and blur handling '\n",
      "          'represent broader challenges in 3D reconstruction. **i)**\\n'\n",
      "          '**Aliasing** . Due to the discrete sampling paradigm (viewing\\n'\n",
      "          'each pixel as a single point instead of an area), 3D GS is\\n'\n",
      "          'susceptible to aliasing when dealing with varying resolutions, '\n",
      "          'which leads to blurring or jagged edges. Solutions\\n'\n",
      "          'emerged at both training and inference stages. Researchers\\n'\n",
      "          'developed training-time improvements from the sampling\\n'\n",
      "          'rate perspective and introduced schemes such as multi-scale\\n'\n",
      "          'Gaussians [67], 2D Mip filter [65], and conditioned logistic\\n'\n",
      "          'function [78]. Inference-time solutions, such as 2D scaleadaptive '\n",
      "          'filtering [80], offer enhanced fidelity that can be\\n'\n",
      "          'integrated into any existing 3D GS frameworks. **ii) Reflec-**\\n'\n",
      "          '**tion** . Achieving realistic rendering of reflective materials '\n",
      "          'is\\n'\n",
      "          'a hard, long-standing problem in 3D scene reconstruction.\\n'\n",
      "          'Recent works have introduced various approaches to model\\n'\n",
      "          'reflective materials [68], [73], [99] and enable relightable\\n'\n",
      "          'Gaussian representation [23], though achieving physically\\n'\n",
      "          'accurate specular effects remains challenging. **iii) Blur** .\\n'\n",
      "          'While 3D GS excels on carefully curated datasets, realworld '\n",
      "          'captures often suffer from blurs such as motion blur\\n'\n",
      "          'and defocus blur. Recent approaches explicitly incorporated\\n'\n",
      "          'blur modeling during training, employing techniques such\\n'\n",
      "          'as coarse-to-fine kernel optimization [74] and photometric\\n'\n",
      "          'bundle adjustment [75] to address this challenge.\\n'\n",
      "          '\\n'\n",
      "          'While the approximations made in 3D GS (Sec. 3.1)\\n'\n",
      "          'contribute to its computational efficiency, they also lead to\\n'\n",
      "          'aliasing, difficulties in illumination estimation, _etc_ . Current\\n'\n",
      "          'solutions, though impressive, typically address individual\\n'\n",
      "          'problems rather than providing a universal solution. A practical '\n",
      "          'intermediate approach involves first detecting specific\\n'\n",
      "          'issues ( _e.g_ ., aliasing, blur) and then applying targeted '\n",
      "          'optimization strategies. The ultimate goal remains developing\\n'\n",
      "          'an advanced reconstruction system that overcomes these\\n'\n",
      "          'limitations, either through fundamental improvements to\\n'\n",
      "          '3D GS or through brand-new architectures.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.4** **Improved Optimization Algorithms**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The optimization of 3D GS presents several challenges that\\n'\n",
      "          'affect the quality of reconstruction. These include issues\\n'\n",
      "          'with convergence speed, visual artifacts from improper\\n'\n",
      "          'Gaussians, and the need for better regularization during\\n'\n",
      "          'optimization. The raw optimization method (Sec. 3.2) might\\n'\n",
      "          'lead to overreconstruction in some regions while underrepresenting '\n",
      "          'others, resulting in blur and visual inconsistencies.\\n'\n",
      "          '\\n'\n",
      "          'Three main directions stand out for improving the optimization of '\n",
      "          '3D GS. **i) Additional Regularization** ( _e.g_ .,\\n'\n",
      "          'frequency [84] and geometry [22], [77]). Geometry-aware\\n'\n",
      "          'approaches have been particularly successful, preserving\\n'\n",
      "          'scene structure through the incorporation of local anchor\\n'\n",
      "          'points [22], depth and surface constraints [100]–[102], Gaussian '\n",
      "          'volumes [103], _etc_ . **ii) Optimization Procedure En-**\\n'\n",
      "          '**hancement** [44], [101], [104]. While the original strategy of\\n'\n",
      "          'density control (Sec. 3.2.2) has proven valuable, considerable room '\n",
      "          'for improvement remains. For example, GaussianPro [44] addresses '\n",
      "          'the challenge of dense initialization\\n'\n",
      "          'in texture-less surfaces and large-scale scenes through an\\n'\n",
      "          'advanced Gaussian densification strategy. **iii) Constraint**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**Relaxation** . Reliance on external tools/algorithms can '\n",
      "          'introduce errors and cap the system’s performance potential. For\\n'\n",
      "          'instance, SfM, commonly used in the initialization process,\\n'\n",
      "          'is error-prone and struggle with complex scenes. Recent\\n'\n",
      "          'works have begun exploring COLMAP-free approaches\\n'\n",
      "          'utilizing stream continuity [81], [105], potentially enabling\\n'\n",
      "          'learning from internet-scale unposed video datasets.\\n'\n",
      "          '\\n'\n",
      "          'Though impressive, existing methods primarily concentrate on '\n",
      "          'optimizing Gaussians to accurately reconstruct\\n'\n",
      "          'scenes from scratch, neglecting a challenging yet promising\\n'\n",
      "          'solution which reconstructs scenes in a few-shot manner\\n'\n",
      "          'through established “meta representations”. Such solution\\n'\n",
      "          'could enable adaptive meta-learning strategies that combine\\n'\n",
      "          'scene-specific and general knowledge. See “learning physical priors '\n",
      "          'from large-scale data” in Sec. 7 for further insights.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.5** **3D Gaussian with More Properties**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Despite impressive, the properties of 3D Gaussian (Sec. 3.1)\\n'\n",
      "          'are designed to be used for novel-view synthesis only. By\\n'\n",
      "          'augmenting 3D Gaussian with additional properties, such as\\n'\n",
      "          'linguistic [87]–[89], semantic/instance [90]–[92], and '\n",
      "          'spatialtemporal [93] properties, 3D GS demonstrates its '\n",
      "          'considerable potential to revolutionize various domains.\\n'\n",
      "          'Here we list several interesting applications using 3D\\n'\n",
      "          'Gaussians with specially designed properties. **i** ) **Language**\\n'\n",
      "          '**Embedded Scene Representation** [87]–[89]. Due to the high\\n'\n",
      "          'computational and memory demands of current languageembedded scene '\n",
      "          'representations, Shi _et al_ . [87] proposed\\n'\n",
      "          'a quantization scheme that augments 3D Gaussian with\\n'\n",
      "          'streamlined language embeddings instead of the original '\n",
      "          'high-dimensional embeddings. This method also mitigated semantic '\n",
      "          'ambiguity and enhanced the precision of\\n'\n",
      "          'open-vocabulary querying by smoothing out semantic features across '\n",
      "          'different views, guided by uncertainty values.\\n'\n",
      "          '**ii** ) **Scene Understanding and Editing** [90]–[92]. Feature\\n'\n",
      "          '3DGS [90] integrated 3D GS with feature field distillation from 2D '\n",
      "          'foundation models. By learning a lowerdimensional feature field and '\n",
      "          'applying a lightweight convolutional decoder for upsampling, '\n",
      "          'Feature 3DGS achieved\\n'\n",
      "          'faster training and rendering speeds while enabling highquality '\n",
      "          'feature field distillation, supporting applications\\n'\n",
      "          'like semantic segmentation and language-guided editing.\\n'\n",
      "          '**iii** ) **Spatiotemporal Modeling** [93], [106]. To capture the\\n'\n",
      "          'complex spatial and temporal dynamics of 3D scenes, Yang\\n'\n",
      "          '_et al_ . [93] conceptualized spacetime as a unified entity and\\n'\n",
      "          'approximates the spatiotemporal volume of dynamic scenes\\n'\n",
      "          'using a collection of 4D Gaussians. The proposed 4D Gaussian '\n",
      "          'representation and corresponding rendering pipeline\\n'\n",
      "          'are capable of modeling arbitrary rotations in space and\\n'\n",
      "          'time and allow for end-to-end training.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.6** **Hybrid Representation**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Rather than augmenting 3D Gaussian with additional properties, '\n",
      "          'another promising avenue of adapting to downstream tasks is to '\n",
      "          'introduce structured information ( _e.g_ .,\\n'\n",
      "          'spatial MLPs and grids) tailored for specific applications.\\n'\n",
      "          'Next we showcase various fascinating uses of 3D GS\\n'\n",
      "          'with specially devised structured information. **i** ) **Facial '\n",
      "          'Ex-**\\n'\n",
      "          '**pression Modeling** . Considering the challenge of creating\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 8,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'high-fidelity 3D head avatars under sparse view conditions, '\n",
      "          'Gaussian Head Avatar [96] introduced controllable 3D\\n'\n",
      "          'Gaussians and an MLP-based deformation field. Concretely,\\n'\n",
      "          'it captured detailed facial expressions and dynamics by\\n'\n",
      "          'optimizing neutral 3D Gaussians alongside the deformation field, '\n",
      "          'thus ensuring both detail fidelity and expression\\n'\n",
      "          'accuracy. **ii** ) **Spatiotemporal Modeling** . Yang _et al_ . '\n",
      "          '[94]\\n'\n",
      "          'proposed to reconstruct dynamic scenes with deformable\\n'\n",
      "          '3D Gaussians. The deformable 3D Gaussians are learned in a\\n'\n",
      "          'canonical space, coupled with a deformation field ( _i.e_ ., a '\n",
      "          'spatial MLP) that models the spatial-temporal dynamics. The\\n'\n",
      "          'proposed method also incorporated an annealing smoothing training '\n",
      "          'mechanism to enhance temporal smoothness\\n'\n",
      "          'without additional computational costs. **iii** ) **Style '\n",
      "          'Transfer** .\\n'\n",
      "          'Saroha _et al_ . [107] proposed GS in style, an advanced approach '\n",
      "          'for real-time neural scene stylization. To maintain a\\n'\n",
      "          'cohesive stylized appearance across multiple views without\\n'\n",
      "          'compromising on rendering speed, they used pre-trained 3D\\n'\n",
      "          'Gaussians coupled with a multi-resolution hash grid and a\\n'\n",
      "          'small MLP to produce stylized views. In a nutshell, incorporating '\n",
      "          'structured information can serve as a complementary\\n'\n",
      "          'part for adapting to tasks that are incompatible with the\\n'\n",
      "          'sparsity and disorder of 3D Gaussians.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**4.7** **New Rendering Algorithm for 3D Gaussians**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'While the rasterization-based pipeline of 3D GS offers impressive '\n",
      "          'real-time performance, it still suffers from the inherent '\n",
      "          'limitations, including inefficient handling of highlydistorted '\n",
      "          'cameras (crucial for robotics), secondary rays (for\\n'\n",
      "          'optical effects like reflections and shadows), and stochastic\\n'\n",
      "          'ray sampling (needed in various existing pipelines). In\\n'\n",
      "          'addition, the assumptions that Gaussians do not overlap\\n'\n",
      "          'and can be sorted accurately using only centers are often\\n'\n",
      "          'violated in practice, leading to temporal artifacts when\\n'\n",
      "          'camera movement changes sorting order.\\n'\n",
      "          '\\n'\n",
      "          'Recent works [108]–[110] explored ray tracing based\\n'\n",
      "          'rendering algorithms as an alternative. For instance, '\n",
      "          'GaussianTracer [108] introduced a new ray tracing implementation '\n",
      "          'for Gaussian primitives, and devised several accelerating '\n",
      "          'strategies according to the uneven density and interleaved nature '\n",
      "          'of Gaussians. EVER [109] deivsed a physically\\n'\n",
      "          'accurate, constant density ellipsoid representation that allows for '\n",
      "          'the exact computation of the volume rendering integral, rather than '\n",
      "          'relying on somewhat satisfactory approximations. This advancement '\n",
      "          'eliminates popping artifacts.\\n'\n",
      "          '\\n'\n",
      "          'Thanks to the fundamental paradigm shift, several exciting '\n",
      "          'possibilities might emerge, including advanced optical\\n'\n",
      "          'effects (reflection, refraction, shadows, global illumination,\\n'\n",
      "          '_etc_ .), support for complex camera models (highly-distorted\\n'\n",
      "          'lenses, rolling shutter effects, _etc_ .), physically accurate '\n",
      "          'rendering with true directional appearance evaluation ( _vs_ . '\n",
      "          'tile\\n'\n",
      "          'based approximation), and more. While these capabilities\\n'\n",
      "          'currently come with additional computational costs, they\\n'\n",
      "          'provide essential building blocks for future research in\\n'\n",
      "          'inverse rendering, physical material modeling, relighting,\\n'\n",
      "          'and complex scene reconstruction.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5** **A** **PPLICATION** **A** **REAS AND** **T** **ASKS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Building on the rapid advancements in 3D GS, a wide range\\n'\n",
      "          'of innovative applications has emerged across multiple do\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'mains (Fig. 6) such as robotics (Sec. 5.1), dynamic scene\\n'\n",
      "          'reconstruction and representation (Sec. 5.2), generation and\\n'\n",
      "          'editing (Sec. 5.3), avatar (Sec. 5.4), medical systems (Sec. 5.5),\\n'\n",
      "          'large-scale scene reconstruction (Sec. 5.6), physics (Sec. 5.7),\\n'\n",
      "          'and even other scientific disciplines [24], [174]–[176]. Here,\\n'\n",
      "          'we highlight key examples that underscore the transformative impact '\n",
      "          'and potential of 3D GS and offer a more\\n'\n",
      "          '[comprehensive collection in '\n",
      "          'Github.](https://github.com/guikunchen/Awesome3DGS)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.1** **Robotics**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The evolution of scene representation in robotics has been\\n'\n",
      "          'profoundly shaped by the emergence of NeRF, which revolutionized '\n",
      "          'dense mapping and environmental interaction\\n'\n",
      "          'through implicit neural models. However, NeRF’s computational cost '\n",
      "          'poses a critical bottleneck for real-time robotic\\n'\n",
      "          'applications. The shift from implicit to explicit representation '\n",
      "          'not only accelerates optimization but also unlocks\\n'\n",
      "          'direct access to spatial and structural scene data, making 3D\\n'\n",
      "          'GS a transformative tool for robotics. Its ability to balance\\n'\n",
      "          'high-fidelity reconstruction with computational efficiency\\n'\n",
      "          'positions 3D GS as a cornerstone for advancing robotic\\n'\n",
      "          'perception, manipulation, and navigation in dynamic, realworld '\n",
      "          'environments.\\n'\n",
      "          '\\n'\n",
      "          'The integration of GS into robotic systems has yielded\\n'\n",
      "          'significant advancements across three core domains. In\\n'\n",
      "          '**SLAM**, GS-based methods [111]–[117], [123], [124], [177]–\\n'\n",
      "          '\\n'\n",
      "          '[182] excel in real-time dense mapping but face inherent\\n'\n",
      "          'trade-offs. Visual SLAM frameworks, particularly RGBD variants '\n",
      "          '[112], [114], [178], leverage depth supervision\\n'\n",
      "          'for geometric fidelity but falter in low-texture or motiondegraded '\n",
      "          'environments. RGB-only approaches [113], [115],\\n'\n",
      "          '\\n'\n",
      "          '[183] circumvent depth sensors but grapple with scale ambiguity and '\n",
      "          'drift. Multi-sensor fusion strategies, such as\\n'\n",
      "          'LiDAR integration [159], [177], [182], enhance robustness in\\n'\n",
      "          'unstructured settings at the cost of calibration complexity.\\n'\n",
      "          'Semantic SLAM [116], [117], [123] extends scene understanding '\n",
      "          'through object-level semantics but struggles with\\n'\n",
      "          'scalability due to lighting sensitivity in color-based methods\\n'\n",
      "          'or computational overhead in feature-based methods. 3D\\n'\n",
      "          'GS based **manipulation** [118]–[122] bypasses the need for\\n'\n",
      "          'auxiliary pose estimation in NeRF-based methods, enabling\\n'\n",
      "          'rapid single-stage tasks like grasping in static environments\\n'\n",
      "          'via geometric and semantic attributes encoded in Gaussian '\n",
      "          'properties. Multi-stage manipulation [118], [120], where\\n'\n",
      "          'environmental dynamics demand real-time map updates,\\n'\n",
      "          'requires explicit modeling of dynamic adjustments ( _e.g_ .,\\n'\n",
      "          'object motions and interactions), material compliance, _etc_ .\\n'\n",
      "          '\\n'\n",
      "          'The advancement of 3D GS in robotics faces three pivotal\\n'\n",
      "          'challenges. First, adaptability in dynamic and unstructured\\n'\n",
      "          'environments remains critical: real-world scenes are rarely\\n'\n",
      "          'static, requiring systems to continuously update representations '\n",
      "          'amid motion, occlusions, and sensor noise without sacrificing '\n",
      "          'accuracy. Second, current semantic mapping methods rely on costly, '\n",
      "          'scene-specific optimization processes,\\n'\n",
      "          'limiting generalizability and scalability for real-world '\n",
      "          'deployment. Third, unlike NeRF based systems which can\\n'\n",
      "          'use MLP parameters as input features for downstream\\n'\n",
      "          'decision-making, 3D Gaussians’ inherent lack of spatial\\n'\n",
      "          'order complicates feature aggregation, with no standardized\\n'\n",
      "          'framework yet established. Bridging the gap between high\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [{'bbox': Rect(332.0238952636719, 240.26486206054688, 489.68603515625, 324.9046630859375),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 306,\n",
      "              'number': 5,\n",
      "              'size': 61842,\n",
      "              'transform': (157.66213989257812,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            84.63980102539062,\n",
      "                            332.0238952636719,\n",
      "                            240.26486206054688),\n",
      "              'width': 570,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(119.4323501586914, 80.66507720947266, 224.21754455566406, 171.0806427001953),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 1120,\n",
      "              'number': 17,\n",
      "              'size': 297450,\n",
      "              'transform': (104.78519439697266,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            90.41556549072266,\n",
      "                            119.4323501586914,\n",
      "                            80.66507720947266),\n",
      "              'width': 1298,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(420.6471862792969, 80.72288513183594, 559.964599609375, 125.10945129394531),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 173,\n",
      "              'number': 21,\n",
      "              'size': 31227,\n",
      "              'transform': (139.3174285888672,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            44.386566162109375,\n",
      "                            420.6471862792969,\n",
      "                            80.72288513183594),\n",
      "              'width': 543,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(425.5146789550781, 244.84066772460938, 503.2488708496094, 322.5748596191406),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 796,\n",
      "              'number': 1,\n",
      "              'size': 80355,\n",
      "              'transform': (77.73419189453125,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            77.73419189453125,\n",
      "                            425.5146789550781,\n",
      "                            244.84066772460938),\n",
      "              'width': 796,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(275.435302734375, 257.9797058105469, 324.48870849609375, 323.8699035644531),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 407,\n",
      "              'number': 28,\n",
      "              'size': 67806,\n",
      "              'transform': (49.05339050292969,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            65.89019775390625,\n",
      "                            275.435302734375,\n",
      "                            257.9797058105469),\n",
      "              'width': 303,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(507.0722351074219, 255.87501525878906, 556.5585327148438, 316.983154296875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 326,\n",
      "              'number': 40,\n",
      "              'size': 25278,\n",
      "              'transform': (49.48631286621094,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            61.10813522338867,\n",
      "                            507.0722351074219,\n",
      "                            255.87501525878906),\n",
      "              'width': 264,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(51.070430755615234, 288.0701599121094, 106.21868896484375, 327.8208923339844),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 253,\n",
      "              'number': 3,\n",
      "              'size': 21399,\n",
      "              'transform': (55.14825439453125,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            39.75072479248047,\n",
      "                            51.070430755615234,\n",
      "                            288.0701599121094),\n",
      "              'width': 351,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(231.565673828125, 91.22856140136719, 263.7178649902344, 153.20867919921875),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 160,\n",
      "              'number': 11,\n",
      "              'size': 6598,\n",
      "              'transform': (32.152183532714844,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            61.98012161254883,\n",
      "                            231.565673828125,\n",
      "                            91.22856140136719),\n",
      "              'width': 83,\n",
      "              'xres': 96,\n",
      "              'yres': 96},\n",
      "             {'bbox': Rect(124.30836486816406, 256.5342712402344, 159.76148986816406, 300.11566162109375),\n",
      "              'bpc': 8,\n",
      "              'colorspace': 3,\n",
      "              'cs-name': 'ICCBased(RGB,sRGB IEC61966-2.1)',\n",
      "              'has-mask': False,\n",
      "              'height': 252,\n",
      "              'number': 2,\n",
      "              'size': 10848,\n",
      "              'transform': (35.453121185302734,\n",
      "                            0.0,\n",
      "                            -0.0,\n",
      "                            43.58139419555664,\n",
      "                            124.30836486816406,\n",
      "                            256.5342712402344),\n",
      "              'width': 205,\n",
      "              'xres': 96,\n",
      "              'yres': 96}],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 9,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-1.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-2.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-3.png)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-0.png)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-10.png)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-4.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-5.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-6.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-7.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-8.png)\\n'\n",
      "          '\\n'\n",
      "          '![](output_images/A-survey-on-3DGS.pdf-8-9.png)\\n'\n",
      "          '\\n'\n",
      "          'Fig. 6. Typical applications benefited from GS (Sec. 5). Some '\n",
      "          'images are borrowed from [132], [135], [146], [154], [160], [166] '\n",
      "          'and redrawn.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'fidelity reconstruction and actionable semantic/physical '\n",
      "          'understanding will define the next frontier for 3D GS, moving\\n'\n",
      "          'beyond passive mapping towards embodied intelligence.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.2** **Dynamic Scene Reconstruction**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Dynamic scene reconstruction refers to the process of capturing and '\n",
      "          'representing the three-dimensional structure and\\n'\n",
      "          'appearance of a scene that changes over time [184]–[187].\\n'\n",
      "          'This involves creating a digital model that accurately reflects\\n'\n",
      "          'the geometry, motion, and visual aspects of the objects in\\n'\n",
      "          'the scene as they evolve. Dynamic scene reconstruction is\\n'\n",
      "          'crucial in various applications, _e.g_ ., VR/AR, 3D animation,\\n'\n",
      "          'and autonomous driving [188]–[190].\\n'\n",
      "          'The key to adapt 3D GS to dynamic scenes is the\\n'\n",
      "          'modeling of temporal dimension which allows for the\\n'\n",
      "          'representation of scenes that change over time. 3D GS\\n'\n",
      "          'based methods [93]–[95], [106], [125]–[130], [191]–[199] for\\n'\n",
      "          'dynamic scene reconstruction can generally be divided into\\n'\n",
      "          'two main categories as discussed in Sec. 4.5 and Sec. 4.6. The\\n'\n",
      "          'first category utilizes **additional fields** like spatial MLPs or\\n'\n",
      "          'grids to **model deformation** (Sec. 4.6). For example, Yang _et_\\n'\n",
      "          '_al_ . [94] first proposed deformable 3D Gaussians tailored for\\n'\n",
      "          'dynamic scenes. These 3D Gaussians are learned in a canonical space '\n",
      "          'and can be used to model spatial-temporal deformation with an '\n",
      "          'implicit deformation field (implemented\\n'\n",
      "          'as an MLP). GaGS [132] devised the voxelization of a set\\n'\n",
      "          'of Gaussian distributions, followed by the use of sparse\\n'\n",
      "          'convolutions to extract geometry-aware features, which are\\n'\n",
      "          'then utilized for deformation learning. On the other hand,\\n'\n",
      "          'the second category is based on the idea that scene changes\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'can be **encoded into the 3D Gaussian representation** with a\\n'\n",
      "          'specially designed rendering process (Sec. 4.5). For instance,\\n'\n",
      "          'Luiten _et al_ . [125] introduced dynamic 3D Gaussians to\\n'\n",
      "          'model dynamic scenes by keeping the properties of 3D\\n'\n",
      "          'Gaussians unchanged over time while allowing their positions and '\n",
      "          'orientations to change. Yang _et al_ . [93] designed a\\n'\n",
      "          '4D Gaussian representation, where additional properties are\\n'\n",
      "          'used to represent 4D rotations and spherindrical harmonics,\\n'\n",
      "          'to approximate the spatial-temporal volume of scenes.\\n'\n",
      "          '\\n'\n",
      "          'While 3D GS advances dynamic scene reconstruction by\\n'\n",
      "          'modeling per-Gaussian deformations, its reliance on finegrained '\n",
      "          'primitives limits scalability and robustness. Current\\n'\n",
      "          'methods struggle to balance computational efficiency and\\n'\n",
      "          'precision: small-scale reconstructions unify dynamic and\\n'\n",
      "          'static elements but become intractable in large environments, often '\n",
      "          'requiring manual priors to segment regions —–\\n'\n",
      "          'a barrier in unstructured settings. Furthermore, the absence\\n'\n",
      "          'of object-level motion reasoning leads to artifacts and poor\\n'\n",
      "          'generalization over long sequences. Future work might prioritize '\n",
      "          'object-centric frameworks that hierarchically group\\n'\n",
      "          'Gaussians into persistent entities, enabling efficient largescale '\n",
      "          'reconstruction through inherent motion disentanglement (dynamic '\n",
      "          '_vs_ . static).\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.3** **Generation and Editing**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Content generation and editing represent two fundamental\\n'\n",
      "          'and inherently interconnected capabilities in modern AI\\n'\n",
      "          'systems. While generation enables the synthesis of novel\\n'\n",
      "          'digital content from scratch or conditional inputs [200]–\\n'\n",
      "          '\\n'\n",
      "          '[202], editing provides the crucial ability to refine, adapt,\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 10,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'and manipulate existing content with precise control [203].\\n'\n",
      "          'Together, these capabilities revolutionize creative workflows\\n'\n",
      "          'by combining initial content creation with iterative refinement, '\n",
      "          'enabling applications from professional content production to '\n",
      "          'interactive consumer tools.\\n'\n",
      "          '\\n'\n",
      "          'Recent advances in generation [133]–[138], [204]–[227]\\n'\n",
      "          'have led to the emergence of three main approaches. **Op-**\\n'\n",
      "          '**timization** based methods [133], [134], [204] distill diffusion\\n'\n",
      "          'priors (gradients) to guide 3D model updates with the score\\n'\n",
      "          'functions. While these methods demonstrate impressive fidelity, '\n",
      "          'they face significant computational overhead due to\\n'\n",
      "          'the necessity of comparing multiple viewpoints during the\\n'\n",
      "          'optimization process. **Reconstruction** based methods [135],\\n'\n",
      "          '\\n'\n",
      "          '[225], [227] reframe the generation problem as a multiview '\n",
      "          'reconstruction task utilizing pre-trained multi-view\\n'\n",
      "          'diffusion models. Although this approach offers an intuitive\\n'\n",
      "          'and straightforward solution, it grapples with fundamental\\n'\n",
      "          'limitations in maintaining view consistency. The lack of\\n'\n",
      "          'strict geometric constraints across different viewpoints often\\n'\n",
      "          'results in inconsistent surface geometry and degraded texture '\n",
      "          'quality, particularly in regions with complex visual features. '\n",
      "          '**Direct 3D generation** methods train diffusion models\\n'\n",
      "          'on 3D representations [138], [220], [226]. While the learned\\n'\n",
      "          '3D diffusion models facilitate multi-view consistency, the\\n'\n",
      "          'demanding computational costs impede the expansion of\\n'\n",
      "          'training scales necessary for improved generative diversity.\\n'\n",
      "          '\\n'\n",
      "          'Current editing works [90]–[92], [126]–[128], [140]–[143],\\n'\n",
      "          '\\n'\n",
      "          '[228]–[239] fall into two primary classes. The first class '\n",
      "          'leverages **2D image-editing** models ( _e.g_ ., diffusion-based '\n",
      "          'editors)\\n'\n",
      "          'to iteratively refine 3D Gaussians. Early efforts [141], [142],\\n'\n",
      "          '\\n'\n",
      "          '[233] adopt optimization- or reconstruction-based strategies\\n'\n",
      "          'akin to methods in generation, but introduce task-specific\\n'\n",
      "          'control signals. However, naively applying 2D edits independently '\n",
      "          'across views often introduces multi-view inconsistencies. '\n",
      "          'Subsequent works [140], [238]–[240] mitigate this\\n'\n",
      "          'through iterative refinement or cross-view attention, albeit\\n'\n",
      "          'at increased computational costs for alignment. A notable\\n'\n",
      "          'challenge is unintended object deformations, attributed to\\n'\n",
      "          'the weak 3D geometric priors in 2D editing models and\\n'\n",
      "          'the difficulty of reconciling 2D edits with underlying 3D\\n'\n",
      "          'structures. The second class exploits the explicit nature of\\n'\n",
      "          '3D GS to enable **direct manipulation** based on embedded\\n'\n",
      "          'properties such as semantics [91], [92], [143], [232] and key\\n'\n",
      "          'points [128]. However, this class remains underexplored\\n'\n",
      "          'due to essentail challenges: the lack of inherent ordering\\n'\n",
      "          'of Gaussians complicates the design of efficient indexing\\n'\n",
      "          'schemes, while editing attributes ( _e.g_ ., texture and geometry) '\n",
      "          'requires careful regularization and alignment to preserve '\n",
      "          'plausibility.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.4** **Avatar**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Avatars, the digital representations of users in virtual\\n'\n",
      "          'spaces, bridge physical and digital realms, enabling immersive '\n",
      "          'interaction, identity expression, and remote collaboration. '\n",
      "          'Spanning entertainment (gaming, virtual influencers),\\n'\n",
      "          'enterprise (AI agents, virtual meetings), healthcare, and '\n",
      "          'education, they underpin metaverse economies. Advances in\\n'\n",
      "          'AR and VR amplify their role in redefining social, industrial,\\n'\n",
      "          'and creative landscapes.\\n'\n",
      "          '\\n'\n",
      "          '3D GS has emerged as a powerful tool for human avatar\\n'\n",
      "          'reconstruction, primarily advancing along two directions:\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'full-body modeling and head-centric modeling. For **full-**\\n'\n",
      "          '**body avatars** [139], [144]–[147], [241]–[252], the current\\n'\n",
      "          'methods typically anchor 3D Gaussians in a canonical space\\n'\n",
      "          'and deform them via parametric body models ( _e.g_ ., SMPL)\\n'\n",
      "          'or cage-based rigging to model dynamic motions. These\\n'\n",
      "          'approaches adopt a hybrid deformation strategy: linear\\n'\n",
      "          'blend skinning handles rigid skeletal transformations such\\n'\n",
      "          'as joint rotations, while pose-conditioned deformation fields\\n'\n",
      "          'account for secondary non-rigid effects like muscle jiggles.\\n'\n",
      "          'For **head avatars** [23], [148]–[151], [253]–[256], the emphasis\\n'\n",
      "          'shifts to modeling intricate facial expressions, fine-grained\\n'\n",
      "          'geometry (e.g., wrinkles, hair [257]), and dynamic speechdriven '\n",
      "          'animations. Techniques mainly combine parametric\\n'\n",
      "          'morphable face models ( _e.g_ ., FLAME) with deformable 3D\\n'\n",
      "          'Gaussians, employing diffusion strategies and expressionaware '\n",
      "          'deformation fields to disentangle rigid head poses\\n'\n",
      "          'from non-rigid facial movements. Both directions exploit\\n'\n",
      "          'the speed advantage and editability of 3D GS to enable\\n'\n",
      "          'efficient training, real-time rendering, and precise control\\n'\n",
      "          'over deformations, while addressing challenges in crossframe '\n",
      "          'correspondence, topology flexibility, and multi-view\\n'\n",
      "          'consistency.\\n'\n",
      "          '\\n'\n",
      "          'Reconstruction in challenging scenes ( _e.g_ ., occlusions,\\n'\n",
      "          'sparse single-view inputs, or loose clothing) and enhancing\\n'\n",
      "          'avatar interactivity represent critical challenges and '\n",
      "          'opportunities. Parametric model-free methods, which bypass '\n",
      "          'predefined priors by learning skinning weights directly from\\n'\n",
      "          'data, show promise for such scenarios. Complementary to\\n'\n",
      "          'this, generative models can mitigate ambiguities inherent\\n'\n",
      "          'in underconstrained settings. Further integrating physicsbased '\n",
      "          'constraints might bridge the gap between static reconstructions and '\n",
      "          'responsive, lifelike interactions, unlocking\\n'\n",
      "          'applications in AR, embodied AI, _etc_ .\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.5** **Endoscopic Scene Reconstruction**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Surgical 3D reconstruction represents a fundamental task\\n'\n",
      "          'in robot-assisted minimally invasive surgery, aimed at enhancing '\n",
      "          'intraoperative navigation, preoperative planning,\\n'\n",
      "          'and educational simulations through precise modeling of\\n'\n",
      "          'dynamic surgical scenes. Pioneering the integration of dynamic '\n",
      "          'radiance fields into this domain, recent advancements\\n'\n",
      "          'have focused on surmounting the inherent challenges of\\n'\n",
      "          'single-viewpoint video reconstructions such as occlusions\\n'\n",
      "          'by surgical instruments and sparse viewpoint diversity\\n'\n",
      "          'within the confined spaces of endoscopic exploration [258]–\\n'\n",
      "          '\\n'\n",
      "          '[260]. Despite the progress, the call for high fidelity in tissue\\n'\n",
      "          'deformability and topological variation remains, coupled\\n'\n",
      "          'with the pressing demand for faster rendering to bridge\\n'\n",
      "          'the utility in applications sensitive to latency [152]–[154].\\n'\n",
      "          'This synthesis of immediacy and precision in reconstructing\\n'\n",
      "          'deformable tissues from endoscopic videos is essential in\\n'\n",
      "          'propelling robotic surgery towards reduced patient trauma\\n'\n",
      "          'and AR/VR applications, ultimately fostering a more intuitive '\n",
      "          'surgical environment and nurturing the future of\\n'\n",
      "          'surgical automation and robotic proficiency.\\n'\n",
      "          'Endoscopic scene reconstruction introduces distinct\\n'\n",
      "          'challenges compared to general dynamic scenes, including\\n'\n",
      "          'sparse training data from limited camera mobility in narrow\\n'\n",
      "          'cavities, frequent tool occlusions obscuring critical regions,\\n'\n",
      "          'and single-view geometry ambiguities. Existing approaches\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 11,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'mainly used additional depth guidance to infer the geometry of '\n",
      "          'tissues [152]–[154]. For instance, EndoGS [154]\\n'\n",
      "          'integrated depth-guided supervision with spatial-temporal\\n'\n",
      "          'weight masks and surface-aligned regularization terms to\\n'\n",
      "          'enhance the quality and speed of 3D tissue rendering\\n'\n",
      "          'while addressing tool occlusion. EndoGaussian [153] introduced two '\n",
      "          'new strategies: holistic Gaussian initialization for\\n'\n",
      "          'dense initialization and spatiotemporal Gaussian tracking\\n'\n",
      "          'for modeling surface dynamics. Zhao _et al_ . [155] argued that\\n'\n",
      "          'these methods suffer from under-reconstruction and proposed to '\n",
      "          'alleviate this problem from frequency perspectives.\\n'\n",
      "          'In addition, EndoGSLAM [156] and Gaussian Pancake [157]\\n'\n",
      "          'devised SLAM systems for endoscopic scenes and showed\\n'\n",
      "          'significant speed advantages.\\n'\n",
      "          '\\n'\n",
      "          'Advancing endoscopic 3D reconstruction requires targeted efforts in '\n",
      "          'both data and dynamics modeling. Data limitations arise from '\n",
      "          'single-viewpoint videos, which produce\\n'\n",
      "          'ill-posed reconstruction problems due to instrument occlusions and '\n",
      "          'constrained camera mobility, leaving critical tissue\\n'\n",
      "          'regions unobserved. While depth estimators provide temporary '\n",
      "          'workarounds, integrating multi-view camera systems\\n'\n",
      "          'addresses the root cause. In addition, existing datasets often\\n'\n",
      "          'feature truncated sequences ( _e.g_ ., 4 _∼_ 8 _s_ in EndoNeRF '\n",
      "          '[258]),\\n'\n",
      "          'which fail to capture prolonged tissue deformation dynamics or '\n",
      "          'complex surgical workflows. Extending temporal coverage to include '\n",
      "          'longer, clinically representative sequences\\n'\n",
      "          'would benefit downstream applications as aforementioned.\\n'\n",
      "          'Modeling limitations persist in current methods, which often '\n",
      "          'represent tissue dynamics at the Gaussian level rather\\n'\n",
      "          'than object- or 3D region-level. This reduces their capacity\\n'\n",
      "          'to encode semantically meaningful anatomical interactions\\n'\n",
      "          'and deserves further explorations.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.6** **Large-scale Scene Reconstruction**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Large-scale scene reconstruction is a critical component in\\n'\n",
      "          'fields such as autonomous driving, aerial surveying, and\\n'\n",
      "          'AR/VR, demanding both photorealistic visual quality and\\n'\n",
      "          'real-time rendering capabilities. Before the emergence of\\n'\n",
      "          '3D GS, the task has been approached using NeRF based\\n'\n",
      "          'methods, which, while effective for smaller scenes, often fall\\n'\n",
      "          'short in detail and rendering speed when scaled to larger\\n'\n",
      "          'areas ( _e.g_ ., over 1.5 _km_ [2] ). Though 3D GS has '\n",
      "          'demonstrated\\n'\n",
      "          'considerable advantages over NeRFs, the direct application\\n'\n",
      "          'of 3D GS to large-scale environments introduces significant '\n",
      "          'challenges. 3D GS requires an immense number of\\n'\n",
      "          'Gaussians to maintain visual quality over extensive areas,\\n'\n",
      "          'leading to prohibitive GPU memory demands and considerable '\n",
      "          'computational burdens during rendering. For instance,\\n'\n",
      "          'a scene spanning 2.7 _km_ [2] may require over 20 million\\n'\n",
      "          'Gaussians, pushing the limits of even the most advanced\\n'\n",
      "          'hardware ( _e.g_ ., NVIDIA A100 with 40GB memory) [163].\\n'\n",
      "          'To address the highlighted challenges, researchers have\\n'\n",
      "          'made significant strides in two key areas: **i** ) For '\n",
      "          '**training**, a\\n'\n",
      "          'divide-and-conquer strategy [162]–[165] has been adopted,\\n'\n",
      "          'which segments a large scene into multiple, independent\\n'\n",
      "          'cells. This facilitates parallel optimization for expansive\\n'\n",
      "          'environments. With the same spirit, Zhao _et al_ . [161] proposed a '\n",
      "          'distributed implementation of 3D GS training. An\\n'\n",
      "          'additional challenge lies in maintaining visual quality, as\\n'\n",
      "          'large-scale scenes often feature texture-less surfaces that can\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'hamper the effectiveness of optimization such as Gaussian\\n'\n",
      "          'initialization and density control (Sec. 3.2). Enhancing the\\n'\n",
      "          'optimization algorithm presents a viable solution to mitigate this '\n",
      "          'issue [44], [164]. **ii** ) Regarding **rendering**, the\\n'\n",
      "          'adoption of the Level of Details (LoD) technique from\\n'\n",
      "          'computer graphics has proven instrumental. LoD adjusts\\n'\n",
      "          'the complexity of 3D scenes to balance visual quality with\\n'\n",
      "          'computational efficiency. Current implementations involve\\n'\n",
      "          'feeding only the essential Gaussians to the rasterizer [164],\\n'\n",
      "          'or designing explicit LoD structures like the Octree [165]\\n'\n",
      "          'and hierarchy [162]. Furthermore, integrating extra input\\n'\n",
      "          'modalities like LiDAR can further enhanced the reconstruction '\n",
      "          'process [158]–[160].\\n'\n",
      "          'One prominent challenge in large-scale scene reconstruction lies in '\n",
      "          'handling sparse or incomplete capture\\n'\n",
      "          'data, which can be mitigated through few-shot adaptation\\n'\n",
      "          'schemes (see Sec. 4.1) or generalizable priors (see “learning\\n'\n",
      "          'physical priors from large-scale data” in Sec. 7). Meanwhile,\\n'\n",
      "          'memory and computational bottlenecks can be addressed\\n'\n",
      "          'via distributed learning strategies [161], such as parameter\\n'\n",
      "          'partitioning across GPU clusters and parallel batched multiview '\n",
      "          'optimization.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**5.7** **Physics**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The simulation of complex real-world dynamics, such as\\n'\n",
      "          'seed dispersal or fluid motion, is pivotal for applications\\n'\n",
      "          'spanning virtual reality, animation, and scientific modeling,\\n'\n",
      "          'where realism hinges on accurate physical behavior. Advances in '\n",
      "          'video diffusion models have driven progress in\\n'\n",
      "          '4D content generation, yet these methods might produce\\n'\n",
      "          'visually plausible results that violate fundamental physical\\n'\n",
      "          'laws. 3D GS emerges as a promising solution by embedding\\n'\n",
      "          'physical constraints and properties into scene representations, '\n",
      "          'enabling both visually convincing and physically\\n'\n",
      "          'coherent simulations.\\n'\n",
      "          '\\n'\n",
      "          'Existing methods differ in how they formulate and integrate '\n",
      "          'physics-based priors into their frameworks. The most\\n'\n",
      "          'common approach is employing physics simulation engines\\n'\n",
      "          '( _e.g_ ., MLS-MPM [268]) to guide the dynamics generation.\\n'\n",
      "          'The material point method [268] and position based dynamics [269] — '\n",
      "          'numerical methods used in computer graphics\\n'\n",
      "          'for simulating deformations in materials like fluids, granular '\n",
      "          'media, and fracturing solids — have been extensively\\n'\n",
      "          'explored by the community through various customizations [21], '\n",
      "          '[143], [166]–[171]. Analytical material models,\\n'\n",
      "          'such as mass-spring systems, have also demonstrated success in '\n",
      "          'approximating deformations by explicitly encoding\\n'\n",
      "          'material properties into 3D Gaussians [172]. Across these\\n'\n",
      "          'methods, 3D Gaussians are treated as discrete particles (with\\n'\n",
      "          'one exception [173] using a continuous representation) and\\n'\n",
      "          'serve as computational units within the chosen simulator.\\n'\n",
      "          'Unknown material properties or physical parameters are\\n'\n",
      "          'typically learned through video-based supervision from\\n'\n",
      "          'conditional generative models.\\n'\n",
      "          '\\n'\n",
      "          'Despite advancements in physics based 3D GS frameworks, critical '\n",
      "          'limitations persist. Current systems struggle\\n'\n",
      "          'to unify diverse physical behaviors ( _e.g_ ., rigid, elastic, or\\n'\n",
      "          'soft-body dynamics) into cohesive simulations, handle complex '\n",
      "          'multi-object interactions without manual intervention,\\n'\n",
      "          'and model scene-level interactions such as environmental\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 12,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [{'bbox': (55.273075103759766,\n",
      "                       75.0974349975586,\n",
      "                       557.890380859375,\n",
      "                       183.87701416015625),\n",
      "              'columns': 4,\n",
      "              'rows': 2},\n",
      "             {'bbox': (55.273075103759766,\n",
      "                       232.72474670410156,\n",
      "                       298.72357177734375,\n",
      "                       574.1319580078125),\n",
      "              'columns': 3,\n",
      "              'rows': 8}],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 1\\n'\n",
      "          'Comparison of **localization** methods (§6.1) on Replica [261] '\n",
      "          '(static scenes), in terms of absolute trajectory error (ATE, cm). '\n",
      "          '(The three best\\n'\n",
      "          'scores are marked in **red**, **blue**, and **green**, '\n",
      "          'respectively. These notes also apply to the other tables.)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|Room0 Room1 Room2 Office0 Office1 Office2 Offci e3 '\n",
      "          'Office4|Avarage|\\n'\n",
      "          '|---|---|---|---|\\n'\n",
      "          '|iMAP [262]<br>[ICCV21]<br>Vox-Fusion '\n",
      "          '[263]<br>[ISMAR22]<br>NICE-SLAM [264]<br>[CVPR22]<br>ESLAM '\n",
      "          '[265]<br>[CVPR23]<br>Point-SLAM [266]<br>[ICCV23]<br>Co-SLAM '\n",
      "          '[267]<br>[CVPR23]<br>Gaussian-SLAM [114]<br>[arXiv]<br>GSSLAM '\n",
      "          '[113]<br>[CVPR24]<br>GS-SLAM [111]<br>[CVPR24]<br>SplaTAM '\n",
      "          '[112]<br>[CVPR24]|✓<br>✓<br>✓<br>✓|3.12<br>2.54<br>2.31<br>1.69<br>1.03<br>3.99<br>4.05<br>1.93<br>1.37<br>4.70<br>1.47<br>8.48<br>2.04<br>2.58<br>1.11<br>2.94<br>0.97<br>1.31<br>1.07<br>0.88<br>1.00<br>1.06<br>1.10<br>1.13<br>0.71<br>0.70<br>0.52<br>0.57<br>0.55<br>0.58<br>0.72<br>0.63<br>0.61<br>0.41<br>0.37<br>0.38<br>0.48<br>0.54<br>0.69<br>0.72<br>0.70<br>0.95<br>1.35<br>0.59<br>0.55<br>2.03<br>1.56<br>0.72<br>3.35<br>8.74<br>3.13<br>1.11<br>0.81<br>0.78<br>1.08<br>7.21<br>0.47<br>0.43<br>0.31<br>0.70<br>0.57<br>0.31<br>0.31<br>3.20<br>0.48<br>0.53<br>0.33<br>0.52<br>0.41<br>0.59<br>0.46<br>0.70<br>0.31<br>0.40<br>0.29<br>0.47<br>0.27<br>0.29<br>0.32<br>0.55|2.58<br>3.09<br>1.06<br>0.63<br>0.52<br>1.00<br>3.27<br>0.79<br>0.50<br>0.36|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 2\\n'\n",
      "          'Collection of representative datasets for 3D GS. Here PC '\n",
      "          'represents\\n'\n",
      "          'point clouds.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Name|Type # Sample|Task|\\n'\n",
      "          '|---|---|---|\\n'\n",
      "          '|Tanks&Temples [270][TOG17]<br>RealEstate10K '\n",
      "          '[271][TOG18]<br>DeepBlending [272][TOG18]<br>LLFF '\n",
      "          '[273][TOG19]<br>NeRF [12][ECCV20]<br>ACID [274][ICCV21]<br>Mip-NeRF '\n",
      "          '360 '\n",
      "          '[8][CVPR22]|RGB<br>14<br>RGB<br>1,000<br>RGB<br>19<br>RGB<br>8<br>RGB<br>8<br>RGB<br>700+<br>RGB<br>9|Novel '\n",
      "          'View<br>Synthesis|\\n'\n",
      "          '|TUM RGB-D [275][IROS12]<br>KITTI [276][CVPR12]<br>ScanNet '\n",
      "          '[277][CVPR17]<br>Replica [261][arXiv19]<br>Waymo '\n",
      "          '[278][CVPR20]<br>nuScenes [279][CVPR20]<br>RLBench '\n",
      "          '[280][RA-L20]<br>Robomimic '\n",
      "          '[281][CoRL22]|RGB-D<br>39<br>RGB-D&PC<br>11<br>RGB-D<br>1,513<br>RGB-D<br>18<br>RGB-D&PC<br>1,150<br>RGB-D&PC<br>1,000<br>RGB<br>100<br>RGB<br>800|Robotics|\\n'\n",
      "          '|D-NeRF [184][CVPR21]<br>HyperNeRF [185][TOG21]<br>NeRF-DS '\n",
      "          '[282][CVPR23]|RGB<br>8<br>RGB<br>6<br>RGB<br>8|Dynamic '\n",
      "          'Scene<br>Reconstruction|\\n'\n",
      "          '|CoNeRF [283][CVPR22]<br>SPIn-NeRF [284][CVPR23]<br>Tensor4D '\n",
      "          '[285][CVPR23]<br>OmniObject3D [286][CVPR23]<br>Objaverse '\n",
      "          '[287][CVPR23]|RGB<br>7<br>RGB<br>10<br>RGB<br>4<br>3D '\n",
      "          'Object<br>6,000<br>3D Object<br>800K+|Generation<br>and Editing|\\n'\n",
      "          '|People-Snapshot [288][CVPR18]<br>VOCASET [289][CVPR19]<br>THUman '\n",
      "          '[290][ICCV19]<br>THUman2.0 [291][CVPR21]<br>ZJU-Mocap '\n",
      "          '[292][CVPR21]<br>H3DS [293][ICCV21]<br>THUman3.0 '\n",
      "          '[294][TPAMI22]|RGB<br>24<br>RGB<br>12<br>RGB<br>200<br>RGB-D<br>500<br>RGB<br>9<br>RGB<br>23<br>3D '\n",
      "          'Scan<br>20|Avatar|\\n'\n",
      "          '|SCARED [295][MICCAI19]<br>EndoNeRF [258][MICCAI22]<br>X3D '\n",
      "          '[296][CVPR24]|RGB-D<br>9<br>RGB<br>2<br>X-ray<br>15|Medical|\\n'\n",
      "          '|CityNeRF [297][ECCV22]<br>Waymo Block-NeRF '\n",
      "          '[298][CVPR22]<br>UrbanBIS [299][SIGGR23]<br>GauU-Scene '\n",
      "          '[160][arXiv24]|RGB<br>12<br>RGB&PC<br>1<br>RGB&PC<br>6<br>RGB&PC<br>1|Large-scale<br>Reconstrction|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'feedback and dynamic lighting changes. Integrating adaptive physics '\n",
      "          'engines capable of multi-object and multimaterial interactions, '\n",
      "          'developing new simulation architectures that are compatible with '\n",
      "          'priors learned from largescale data, and expanding datasets to '\n",
      "          'encompass diverse\\n'\n",
      "          'materials and dynamic scenarios are equally vital.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6** **P** **ERFORMANCE** **C** **OMPARISON**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'In this section, we provide more empirical evidence by\\n'\n",
      "          'presenting the performance of several 3D GS algorithms\\n'\n",
      "          'that we previously discussed. The diverse applications of\\n'\n",
      "          '3D GS across numerous tasks, coupled with the customtailored '\n",
      "          'algorithmic designs for each task, render a uniform\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'comparison of all 3D GS algorithms across a single task or\\n'\n",
      "          'dataset impracticable. For comprehensiveness, we provide a\\n'\n",
      "          'collection of representative datasets in Table 2 according to\\n'\n",
      "          'our analysis in Sec. 5. Due to the limited space, we have chosen '\n",
      "          'several representative tasks for an in-depth performance\\n'\n",
      "          'evaluation. The performance scores are primarily sourced\\n'\n",
      "          'from the original papers, except where indicated otherwise.\\n'\n",
      "          '[We also maintain a Github repository for this '\n",
      "          'section.](https://github.com/guikunchen/3DGS-Benchmarks)\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.1** **Performance Benchmarking: Localization**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'The localization task in SLAM involves determining the\\n'\n",
      "          'precise position and orientation of a robot or device within\\n'\n",
      "          'an environment, typically using sensor data.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** Replica [261] dataset is a collection of 18 '\n",
      "          'highly\\n'\n",
      "          'detailed 3D indoor scenes. These scenes are not only visually\\n'\n",
      "          'realistic but also offer comprehensive data including dense\\n'\n",
      "          'meshes, high-quality HDR textures, and detailed semantic\\n'\n",
      "          'information for each element. Following [262], three sequences '\n",
      "          'about rooms and five sequences about offices are\\n'\n",
      "          'used for the evaluation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve four recent 3D GS based algorithms [111]–[114]\\n'\n",
      "          'and six typical SLAM methods [262]–[267].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** The root mean square error (RMSE)\\n'\n",
      "          'of the absolute trajectory error (ATE) is a commonly used\\n'\n",
      "          'metric in evaluating SLAM systems [275], which measures\\n'\n",
      "          'the root mean square of the Euclidean distances between the\\n'\n",
      "          'estimated and true positions over the entire trajectory.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** As shown in Table 1, the recent 3D Gaussians\\n'\n",
      "          'based localization algorithms have a clear advantage over\\n'\n",
      "          'existing NeRF based dense visual SLAM. For example,\\n'\n",
      "          'SplaTAM [112] achieves a trajectory error improvement of\\n'\n",
      "          '_∼_ **50** %, decreasing it from 0.52cm to **0.36cm** compared to\\n'\n",
      "          'the previous state-of-the-art (SOTA) [266]. We attribute this\\n'\n",
      "          'to the dense and accurate 3D Gaussians reconstructed for\\n'\n",
      "          '\\n'\n",
      "          'scenes, which can handle the noise of real sensors. This\\n'\n",
      "          'reveals that effective scene representations can improve the\\n'\n",
      "          'accuracy of localization tasks.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.2** **Performance Benchmarking: Static Scenes**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Rendering focuses on transforming computer-readable information ( '\n",
      "          '_e.g_ ., 3D objects in the scene) to pixel-based\\n'\n",
      "          'images. This section focuses on evaluating the quality of\\n'\n",
      "          'rendering results in static scenes.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** The same dataset as in Sec. 6.1, _i.e_ ., Replica '\n",
      "          '[261],\\n'\n",
      "          'is used for comparison. The testing views are the same as\\n'\n",
      "          'those collected by [262].\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 13,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [{'bbox': (63.6470832824707,\n",
      "                       75.12527465820312,\n",
      "                       550.2225341796875,\n",
      "                       281.9548645019531),\n",
      "              'columns': 6,\n",
      "              'rows': 8},\n",
      "             {'bbox': (318.8070068359375,\n",
      "                       339.8204650878906,\n",
      "                       562.5723266601562,\n",
      "                       466.28314208984375),\n",
      "              'columns': 3,\n",
      "              'rows': 2}],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 13\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 3\\n'\n",
      "          'Comparison of **mapping** methods (§6.2) on Replica [261] (static '\n",
      "          'scenes), in terms of PSNR, SSIM, and LPIPS. The results for FPS are '\n",
      "          'taken\\n'\n",
      "          'from [113] using one 4090 GPU.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|Metric|Room0 Room1 Room2 Office0 Office1 Office2 Office3 '\n",
      "          'Office4|Avarage|FPS|\\n'\n",
      "          '|---|---|---|---|---|---|\\n'\n",
      "          '|NICE-SLAM [264] '\n",
      "          '[CVPR22]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|22.12<br>22.47<br>24.52<br>29.07<br>30.34<br>19.66<br>22.23<br>24.94<br>0.69<br>0.76<br>0.81<br>0.87<br>0.89<br>0.80<br>0.80<br>0.86<br>0.33<br>0.27<br>0.21<br>0.23<br>0.18<br>0.23<br>0.21<br>0.20|24.42<br>0.81<br>0.23|0.54|\\n'\n",
      "          '|Vox-Fusion [263] '\n",
      "          '[ISMAR22]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|22.39<br>22.36<br>23.92<br>27.79<br>29.83<br>20.33<br>23.47<br>25.21<br>0.68<br>0.75<br>0.80<br>0.86<br>0.88<br>0.79<br>0.80<br>0.85<br>0.30<br>0.27<br>0.23<br>0.24<br>0.18<br>0.24<br>0.21<br>0.20|24.41<br>0.80<br>0.24|2.17|\\n'\n",
      "          '|Point-SLAM [266] '\n",
      "          '[ICCV23]||PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|32.40<br>34.08<br>35.50<br>38.26<br>39.16<br>33.99<br>33.48<br>33.49<br>0.97<br>0.98<br>0.98<br>0.98<br>0.99<br>0.96<br>0.96<br>0.98<br>0.11<br>0.12<br>0.11<br>0.10<br>0.12<br>0.16<br>0.13<br>0.14|35.17<br>0.97<br>0.12|1.33|\\n'\n",
      "          '|SplaTAM [112] '\n",
      "          '[CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|32.86<br>33.89<br>35.25<br>38.26<br>39.17<br>31.97<br>29.70<br>31.81<br>0.98<br>0.97<br>0.98<br>0.98<br>0.98<br>0.97<br>0.95<br>0.95<br>0.07<br>0.10<br>0.08<br>0.09<br>0.09<br>0.10<br>0.12<br>0.15|34.11<br>0.97<br>0.10|-|\\n'\n",
      "          '|GS-SLAM [111] '\n",
      "          '[CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|31.56<br>32.86<br>32.59<br>38.70<br>41.17<br>32.36<br>32.03<br>32.92<br>0.97<br>0.97<br>0.97<br>0.99<br>0.99<br>0.98<br>0.97<br>0.97<br>0.09<br>0.07<br>0.09<br>0.05<br>0.03<br>0.09<br>0.11<br>0.11|34.27<br>0.97<br>0.08|-|\\n'\n",
      "          '|GSSLAM [113] '\n",
      "          '[CVPR24]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|34.83<br>36.43<br>37.49<br>39.95<br>42.09<br>36.24<br>36.70<br>36.07<br>0.95<br>0.96<br>0.96<br>0.97<br>0.98<br>0.96<br>0.96<br>0.96<br>0.07<br>0.08<br>0.07<br>0.07<br>0.06<br>0.08<br>0.07<br>0.10|37.50<br>0.96<br>0.07|769|\\n'\n",
      "          '|Gaussian-SLAM '\n",
      "          '[114][arXiv]|✓|PSNR_↑_<br>SSIM_↑_<br>LPIPS_↓_|34.31<br>37.28<br>38.18<br>43.97<br>43.56<br>37.39<br>36.48<br>40.19<br>0.99<br>0.99<br>0.99<br>1.00<br>0.99<br>0.99<br>0.99<br>1.00<br>0.08<br>0.07<br>0.07<br>0.04<br>0.07<br>0.08<br>0.08<br>0.07|38.90<br>0.99<br>0.07|-|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve four recent papers which introduce 3D Gaussians into '\n",
      "          'their systems [111]–[114], as well as three dense\\n'\n",
      "          'SLAM methods [263], [264], [266].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** Peak signal-to-noise ratio (PSNR),\\n'\n",
      "          'structural similarity (SSIM) [300], and learned perceptual\\n'\n",
      "          'image patch similarity (LPIPS) [301] are used for measuring\\n'\n",
      "          'RGB rendering performance.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** Table 3 shows that 3D Gaussians based systems\\n'\n",
      "          'generally outperform the three dense SLAM competitors.\\n'\n",
      "          'For example, Gaussian-SLAM [114] establishes new SOTA\\n'\n",
      "          'and outperforms previous methods by a large margin.\\n'\n",
      "          'Compared to Point-SLAM [266], GSSLAM [113] is about\\n'\n",
      "          '**578** times faster in achieving very competitive accuracy.\\n'\n",
      "          'In contrast to previous method [266] that relies on depth\\n'\n",
      "          'information, such as depth-guided ray sampling, for synthesizing '\n",
      "          'novel views, 3D GS based system eliminates this\\n'\n",
      "          'need, allowing for high-fidelity rendering for any views.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.3** **Performance Benchmarking: Dynamic Scenes**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'This section focuses on evaluating the rendering quality in\\n'\n",
      "          'dynamic scenes.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** D-NeRF [184] dataset includes videos with 50\\n'\n",
      "          'to 200 frames each, captured from unique viewpoints. It\\n'\n",
      "          'features synthetic, animated objects in complex scenes, with\\n'\n",
      "          'non-Lambertian materials. The dataset provides 50 to 200\\n'\n",
      "          'training images and 20 test images per scene, designed for\\n'\n",
      "          'evaluating models in the monocular setting. The testing\\n'\n",
      "          'views are the same as the original paper [184].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve five recent papers that model dynamic scenes\\n'\n",
      "          'with 3D GS [93]–[95], [126], [132], as well as six NeRF based\\n'\n",
      "          'approaches [37], [184], [187], [302]–[304].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** The same metrics as in Sec. 6.2, _i.e_ '\n",
      "          '.,\\n'\n",
      "          'PSNR, SSIM [300], and LPIPS [301], are used for evaluation.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** From Table 4 we can observe that 3D GS based\\n'\n",
      "          'methods outperform existing SOTAs by a clear margin. The\\n'\n",
      "          'static version of 3D GS [10] fails to reconstruct dynamic\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 4\\n'\n",
      "          'Comparison of **reconstruction** methods (§6.3) on D-NeRF [184]\\n'\n",
      "          '(dynamic scenes), in terms of PSNR, SSIM, and LPIPS. _[∗]_ denotes\\n'\n",
      "          'results reported in [95].\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|PSNR↑ SSIM↑ LPIPS↓|\\n'\n",
      "          '|---|---|---|\\n'\n",
      "          '|D-NeRF [184]<br>[CVPR21]<br>TiNeuVox-B [302]<br>[SGA22]<br>KPlanes '\n",
      "          '[37]<br>[CVPR23]<br>HexPlane-Slim [303]<br>[CVPR23]<br>FFDNeRF '\n",
      "          '[187]<br>[ICCV23]<br>MSTH [304]<br>[NeurIPS23]<br>3D '\n",
      "          'GS_∗_[10]<br>[TOG23]<br>4DGS [93]<br>[ICLR24]<br>4D-GS '\n",
      "          '[95]<br>[CVPR24]<br>GaGS [132]<br>[CVPR24]<br>CoGS '\n",
      "          '[126]<br>[CVPR24]<br>D-3DGS '\n",
      "          '[94]<br>[CVPR24]|✓<br>✓<br>✓<br>✓<br>✓<br>✓|30.50<br>0.95<br>0.07<br>32.67<br>0.97<br>0.04<br>31.61<br>0.97<br>-<br>32.68<br>0.97<br>0.02<br>32.68<br>0.97<br>0.02<br>31.34<br>0.98<br>0.02<br>23.19<br>0.93<br>0.08<br>34.09<br>0.98<br>-<br>34.05<br>0.98<br>0.02<br>37.36<br>0.99<br>0.01<br>37.90<br>0.98<br>0.02<br>39.51<br>0.99<br>0.01|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'scenes, resulting in a sharp drop in performance. By modeling the '\n",
      "          'dynamics, D-3DGS [94] outperforms the SOTA\\n'\n",
      "          'method, FFDNeRF [187], by **6.83** dB in terms of PSNR. These\\n'\n",
      "          'results indicate the effectiveness of introducing additional\\n'\n",
      "          'properties or structured information to model the deformation of '\n",
      "          'Gaussians so as to model the scene dynamics.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.4** **Performance Benchmarking: Human Avatar**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Human avatar modeling aims to create the model of human\\n'\n",
      "          'avatars from a given multi-view video.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** ZJU-MoCap [292] is a prevalent benchmark in human '\n",
      "          'modeling from videos, captured with 23 synchronized\\n'\n",
      "          'cameras at a 1024 _×_ 1024 resolution. Six subjects ( _i.e_ ., '\n",
      "          '377,\\n'\n",
      "          '386, 387, 392, 393, and 394) are used for evaluation [305].\\n'\n",
      "          'The same testing views following [306] are adopted.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve three recent papers which model human avatar\\n'\n",
      "          'with 3D GS [145], [146], [249], as well as six human rendering '\n",
      "          'approaches [292], [305]–[309].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** PSNR, SSIM [300], and LPIPS* [301]\\n'\n",
      "          'are used for measuring RGB rendering performance. Here\\n'\n",
      "          'LPIPS* equals to LPIPS _×_ 1000.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 14,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [{'bbox': (55.227783203125,\n",
      "                       83.96910858154297,\n",
      "                       294.1467590332031,\n",
      "                       189.20849609375),\n",
      "              'columns': 3,\n",
      "              'rows': 2},\n",
      "             {'bbox': (55.227783203125,\n",
      "                       247.83876037597656,\n",
      "                       298.76348876953125,\n",
      "                       320.5862121582031),\n",
      "              'columns': 4,\n",
      "              'rows': 2}],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 5\\n'\n",
      "          'Comparison of **reconstruction** methods (§6.4) on ZJU-MoCap [292]\\n'\n",
      "          '(avatar), in terms of PSNR, SSIM, and LPIPS*. The results for '\n",
      "          'non-GS\\n'\n",
      "          'methods are taken from [146].\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|PSNR↑ SSIM↑ LPIPS*↓|\\n'\n",
      "          '|---|---|---|\\n'\n",
      "          '|NeuralBody [292] [CVPR21]<br>AnimNeRF [307] [ICCV21]<br>PixelNeRF '\n",
      "          '[308] [ICCV21]<br>NHP [309] [NeurIPS21]<br>HumanNeRF [305] '\n",
      "          '[CVPR22]<br>Instant-NVR [306] [CVPR23]<br>GauHuman [145] '\n",
      "          '[CVPR24]<br>3DGS-Avatar [249] [CVPR24]<br>GART [146] '\n",
      "          '[CVPR24]|✓<br>✓<br>✓|29.03<br>0.96<br>42.47<br>29.77<br>0.96<br>46.89<br>24.71<br>0.89<br>121.86<br>28.25<br>0.95<br>64.77<br>30.66<br>0.97<br>33.38<br>31.01<br>0.97<br>38.45<br>31.34<br>0.97<br>30.51<br>30.61<br>0.97<br>29.58<br>32.22<br>0.98<br>29.21|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'TABLE 6\\n'\n",
      "          'Comparison of **reconstruction** methods (§6.5) on EndoNeRF [258]\\n'\n",
      "          '(surgical scenes), in terms of PSNR, SSIM, and LPIPS. The results '\n",
      "          'for\\n'\n",
      "          'non-GS methods are taken from [153]. FPS and GPU usage for '\n",
      "          'training\\n'\n",
      "          '(Mem.) are measured using one 4090 GPU [153].\\n'\n",
      "          '\\n'\n",
      "          '|Method|GS|PSNR↑ SSIM↑ LPIPS↓|FPS↑ Mem.↓|\\n'\n",
      "          '|---|---|---|---|\\n'\n",
      "          '|EndoNeRF [258][MICCAI22]<br>EndoSurf '\n",
      "          '[260][MICCAI23]<br>LerPlane-9k [259][MICCAI23]<br>LerPlane-32k '\n",
      "          '[259][MICCAI23]<br>Endo-4DGS [152][MICCAI24]<br>EndoGaussian '\n",
      "          '[153][arXiv]<br>HFGS '\n",
      "          '[155][BMVC24]|✓<br>✓<br>✓|36.06<br>0.93<br>0.09<br>36.53<br>0.95<br>0.07<br>35.00<br>0.93<br>0.08<br>37.38<br>0.95<br>0.05<br>37.00<br>0.96<br>0.05<br>37.85<br>0.96<br>0.05<br>38.14<br>0.97<br>0.03|0.04<br>19GB<br>0.04<br>17GB<br>0.91<br>20GB<br>0.87<br>20GB<br>-<br>4GB<br>195.09<br>2GB<br>-<br>-|\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** Table 5 presents the numerical results of '\n",
      "          'topleading solutions in human avatar modeling. We observe\\n'\n",
      "          'that introducing 3D GS into the framework leads to consistent '\n",
      "          'performance improvements in both rendering quality\\n'\n",
      "          'and speed. For instance, GART [146] outperforms current\\n'\n",
      "          'SOTA, Instant-NVR [306], by **1.21** dB in terms of PSNR.\\n'\n",
      "          'Considering the enhanced fidelity, inference speed and editability, '\n",
      "          '3D GS based avatar modeling may revolutionize\\n'\n",
      "          'the field of 3D animation, interactive gaming, _etc_ .\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**6.5** **Performance Benchmarking: Surgical Scenes**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '3D reconstruction from endoscopic video is critical to\\n'\n",
      "          'robotic-assisted minimally invasive surgery, enabling preoperative '\n",
      "          'planning, training through AR/VR simulations,\\n'\n",
      "          'and intraoperative guidance.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Dataset:** EndoNeRF [258] dataset presents a specialized\\n'\n",
      "          'collection of stereo camera captures, comprising two samples of '\n",
      "          'in-vivo prostatectomy. It is tailored to represent realworld '\n",
      "          'surgical complexities, including challenging scenes\\n'\n",
      "          'with tool occlusion and pronounced non-rigid deformation.\\n'\n",
      "          'The same testing views as in [260] are used.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Benchmarking Algorithms:** For performance comparison,\\n'\n",
      "          'we involve three recent papers which reconstruct dynamic\\n'\n",
      "          '3D endoscopic scenes with GS [152], [153], [155], as well as\\n'\n",
      "          'three NeRF-based surgical reconstruction approaches [258]–\\n'\n",
      "          '\\n'\n",
      "          '[260].\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Evaluation Metric:** PSNR, SSIM [300], and LPIPS [301]\\n'\n",
      "          'are adopted for evaluation. In addition, the requirement for\\n'\n",
      "          'GPU memory is also reported.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Result:** Table 6 shows that introducing the explicit '\n",
      "          'representation of 3D Gaussians leads to several significant '\n",
      "          'improvements. For instance, EndoGaussian [153] outperforms\\n'\n",
      "          'a strong baseline, LerPlane-32k [259], among all metrics. In\\n'\n",
      "          'particular, EndoGaussian demonstrates an approximate 224fold '\n",
      "          'increase in speed while consumes just 10% of the GPU\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'resources. These impressive results attest to the efficiency\\n'\n",
      "          'of GS-based methods, which not only expedite processing\\n'\n",
      "          'but also minimize GPU load, thus easing the demands on\\n'\n",
      "          'hardware. Such attributes are vitally significant for realworld '\n",
      "          'surgical application deployment, where optimized\\n'\n",
      "          'resource usage can be a key determinant of practical utility.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**7** **F** **UTURE** **R** **ESEARCH** **D** **IRECTIONS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'As impressive as those follow-up works on 3D GS are, and\\n'\n",
      "          'as much as those fields have been or might be revolutionized by 3D '\n",
      "          'GS, there is a general agreement that 3D GS still\\n'\n",
      "          'has considerable room for improvement.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Physics- and Semantics-aware Scene Representation.** As\\n'\n",
      "          'a new, explicit scene representation technique, 3D Gaussian\\n'\n",
      "          'offers transformative potential beyond merely enhancing\\n'\n",
      "          'novel-view synthesis. It has the potential to pave the way\\n'\n",
      "          'for simultaneous advancements in scene reconstruction and\\n'\n",
      "          'understanding by devising physics- and semantics-aware\\n'\n",
      "          '3D GS systems. While significant progress has been made\\n'\n",
      "          'in physics (Sec. 5.7) and semantics [310]–[315] individually, there '\n",
      "          'remains considerable untapped potential in their\\n'\n",
      "          'synergistic integration. This is poised to revolutionize a\\n'\n",
      "          'range of fields and downstream applications. For instance,\\n'\n",
      "          'incorporating prior knowledge such as the general shape\\n'\n",
      "          'of objects can reduce the need for extensive training viewpoints '\n",
      "          '[47], [48] while improving geometry/surface reconstruction [77], '\n",
      "          '[316]. A critical metric for assessing scene\\n'\n",
      "          'representation is the quality of its generated scenes, which\\n'\n",
      "          'encompasses challenges in geometry, texture, and lighting\\n'\n",
      "          'fidelity [66], [128], [141]. By merging physical principles\\n'\n",
      "          'and semantic information within the 3D GS framework,\\n'\n",
      "          'one can expect that the quality will be enhanced, thereby\\n'\n",
      "          'facilitating dynamics modeling [21], [166], editing [90], [92],\\n'\n",
      "          'generation [133], [134], and beyond. In a nutshell, pursuing\\n'\n",
      "          'this advanced and versatile scene representation opens up\\n'\n",
      "          'new possibilities for innovation in computational creativity\\n'\n",
      "          'and practical applications across diverse domains.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Learning Physical Priors from Large-scale Data.** As we\\n'\n",
      "          'explore the potential of physics- and semantics-aware scene\\n'\n",
      "          'representations, leveraging large-scale datasets to learn '\n",
      "          'generalizable, physical priors emerges as a promising direction.\\n'\n",
      "          'The goal is to model the inherent physical properties and\\n'\n",
      "          'dynamics embedded within real-world data, transforming\\n'\n",
      "          'them into actionable insights that can be applied across various '\n",
      "          'domains such as robotics and visual effects. Establishing\\n'\n",
      "          'a learning framework for extracting these generalizable\\n'\n",
      "          'priors enables the application of these insights to specific\\n'\n",
      "          'tasks in a few-shot manner. For instance, it allows for rapid\\n'\n",
      "          'adaptation to new objects and environments with minimal\\n'\n",
      "          'data input. Furthermore, integrating physical priors can enhance '\n",
      "          'not only the accuracy and quality of generated scenes\\n'\n",
      "          'but also their interactive and dynamic qualities. This is\\n'\n",
      "          'particularly valuable in AR/VR environments, where users\\n'\n",
      "          'interact with virtual objects that behave in ways consistent\\n'\n",
      "          'with their real-world counterparts. However, the existing\\n'\n",
      "          'body of work on capturing and distilling physics-based\\n'\n",
      "          'knowledge from extensive 2D and 3D datasets remains\\n'\n",
      "          'sparse. Notable efforts in related area include the continuum\\n'\n",
      "          'mechanics based GS systems (Sec. 5.7), and the generalizable\\n'\n",
      "          'Gaussian representation based on multi-view stereo [317].\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 15,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 15\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'Further exploration on real2sim and sim2real might offer\\n'\n",
      "          'viable routes for advancements in this field.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Modeling Internal Structures of Objects with 3D GS.**\\n'\n",
      "          'Despite the ability of 3D GS to produce highly photorealistic\\n'\n",
      "          'renderings, modeling internal structures of objects ( _e.g_ ., for '\n",
      "          'a\\n'\n",
      "          'scanned object in computed tomography) within the current\\n'\n",
      "          'GS framework presents a notable challenge. Due to the\\n'\n",
      "          'splatting and density control process, the current representation '\n",
      "          'of 3D Gaussian is unorganized and cannot align\\n'\n",
      "          'well with the object’s actual internal structures. Moreover,\\n'\n",
      "          'there is a strong preference in various applications to depict\\n'\n",
      "          'objects as volumes ( _e.g_ ., computed tomography). However,\\n'\n",
      "          'the disordered nature of 3D GS makes volume modeling\\n'\n",
      "          'particularly difficult. Li _et al_ . [318] used 3D Gaussians with\\n'\n",
      "          'density control as the basis for the volumetric representation\\n'\n",
      "          'and did not involve the splatting process. X-Gaussian [319]\\n'\n",
      "          'involves the splatting process for fast training and inference but '\n",
      "          'cannot generate volumetric representation. Using\\n'\n",
      "          '3D GS to model the internal structures of objects remains\\n'\n",
      "          'unanswered and deserves further exploration.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **3D GS for Simulation in Autonomous Driving and be-**\\n'\n",
      "          '**yond.** Collecting real-world datasets for autonomous driving is '\n",
      "          'both expensive and logistically challenging, yet crucial\\n'\n",
      "          'for training effective image-based perception systems. To\\n'\n",
      "          'mitigate these issues, simulation emerges as a cost-effective\\n'\n",
      "          'alternative, enabling the generation of synthetic datasets\\n'\n",
      "          'across diverse environments. However, the development of\\n'\n",
      "          'simulators capable of producing photorealistic and diverse\\n'\n",
      "          'synthetic data is fraught with challenges. These include\\n'\n",
      "          'achieving a high level of quality, accommodating various\\n'\n",
      "          'control methods, and accurately simulating a range of lighting '\n",
      "          'conditions. While early efforts [188]–[190] in reconstructing '\n",
      "          'urban/street scenes with 3D GS have been encouraging,\\n'\n",
      "          'they are just the tip of the iceberg in terms of the full '\n",
      "          'capabilities. There remain numerous critical aspects to be '\n",
      "          'explored,\\n'\n",
      "          'such as the integration of user-defined object models, the\\n'\n",
      "          'modeling of physics-aware scene changes ( _e.g_ ., the rotation\\n'\n",
      "          'of vehicle wheels), and the enhancement of controllability\\n'\n",
      "          'and quality ( _e.g_ ., in varying lighting conditions). Mastery\\n'\n",
      "          'of these capabilities would not only advance autonomous\\n'\n",
      "          'systems but also redefine computational understanding of\\n'\n",
      "          'physical spaces — a leap with implications for world models, '\n",
      "          'spatial intelligence, embodied AI, and beyond.\\n'\n",
      "          '\\n'\n",
      "          '_•_ **Empowering 3D GS with More Possibilities.** Despite the\\n'\n",
      "          'significant potential of 3D GS, the full scope of applications\\n'\n",
      "          'for 3D GS remains largely untapped. A promising avenue\\n'\n",
      "          'for exploration involves augmenting 3D Gaussians with additional '\n",
      "          'attributes ( _e.g_ ., linguistic and spatiotemporal properties as '\n",
      "          'mentioned in Sec. 4.5) and introducing structured\\n'\n",
      "          'information (e.g., spatial MLPs and grids as mentioned in\\n'\n",
      "          'Sec. 4.6), tailored for specific applications. Moreover, recent\\n'\n",
      "          'studies have begun to unveil the capability of 3D GS in\\n'\n",
      "          'several domains, _e.g_ ., point cloud registration [320], image '\n",
      "          'representation and compression [60], and fluid synthesis [171]. '\n",
      "          'These findings highlight a significant opportunity\\n'\n",
      "          'for interdisciplinary scholars to explore 3D GS further.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**8** **C** **ONCLUSIONS**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'To the best of our knowledge, this survey presents the\\n'\n",
      "          'first comprehensive overview of 3D GS, a groundbreaking\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'technique revolutionizing explicit radiance fields, computer\\n'\n",
      "          'graphics, and computer vision. It delineates the paradigm\\n'\n",
      "          'shift from traditional NeRF based methods, spotlighting the\\n'\n",
      "          'advantages of 3D GS in real-time rendering and enhanced\\n'\n",
      "          'editability. Our in-depth analysis and extensive quantitative\\n'\n",
      "          'studies demonstrate the superiority of 3D GS in practical\\n'\n",
      "          'applications, particularly those highly sensitive to latency.\\n'\n",
      "          'We offer insights into principles, prospective research directions, '\n",
      "          'and the unresolved challenges within this domain. Overall, 3D GS '\n",
      "          'stands as a transformative technology,\\n'\n",
      "          'poised to significantly influence future advancements in 3D\\n'\n",
      "          'reconstruction and representation. This survey is intended\\n'\n",
      "          'to serve as a foundational resource, propelling further exploration '\n",
      "          'and progress in this rapidly evolving field.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '**R** **EFERENCES**\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[1] S. J. Gortler, R. Grzeszczuk, R. Szeliski, and M. F. Cohen, '\n",
      "          '“The\\n'\n",
      "          'lumigraph,” in _Seminal Graphics Papers: Pushing the Boundaries,_\\n'\n",
      "          '_Volume 2_, 2023, pp. 453–464.\\n'\n",
      "          '\\n'\n",
      "          '[2] M. Levoy and P. Hanrahan, “Light field rendering,” in '\n",
      "          '_Seminal_\\n'\n",
      "          '_Graphics Papers: Pushing the Boundaries, Volume 2_, 2023, pp. '\n",
      "          '441–\\n'\n",
      "          '452.\\n'\n",
      "          '\\n'\n",
      "          '[3] C. Buehler, M. Bosse, L. McMillan, S. Gortler, and M. Cohen,\\n'\n",
      "          '“Unstructured lumigraph rendering,” in _Seminal Graphics Papers:_\\n'\n",
      "          '_Pushing the Boundaries, Volume 2_, 2023, pp. 497–504.\\n'\n",
      "          '\\n'\n",
      "          '[4] N. Snavely, S. M. Seitz, and R. Szeliski, “Photo tourism: '\n",
      "          'exploring\\n'\n",
      "          'photo collections in 3d,” in _ACM Trans. Graph._, 2006, pp. '\n",
      "          '835–846.\\n'\n",
      "          '\\n'\n",
      "          '[5] M. Goesele, N. Snavely, B. Curless, H. Hoppe, and S. M. Seitz,\\n'\n",
      "          '“Multi-view stereo for community photo collections,” in _Proc._\\n'\n",
      "          '_IEEE Int. Conf. Comput. Vis._, 2007, pp. 1–8.\\n'\n",
      "          '\\n'\n",
      "          '[6] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. '\n",
      "          'Valentin,\\n'\n",
      "          '“Fastnerf: High-fidelity neural rendering at 200fps,” in _Proc. '\n",
      "          'IEEE_\\n'\n",
      "          '_Int. Conf. Comput. Vis._, 2021, pp. 14 346–14 355.\\n'\n",
      "          '\\n'\n",
      "          '[7] C. Reiser, S. Peng, Y. Liao, and A. Geiger, “Kilonerf: Speeding '\n",
      "          'up\\n'\n",
      "          'neural radiance fields with thousands of tiny mlps,” in _Proc. '\n",
      "          'IEEE_\\n'\n",
      "          '_Int. Conf. Comput. Vis._, 2021, pp. 14 335–14 345.\\n'\n",
      "          '\\n'\n",
      "          '[8] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and '\n",
      "          'P. Hedman, “Mip-nerf 360: Unbounded anti-aliased neural radiance\\n'\n",
      "          'fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2022,\\n'\n",
      "          'pp. 5470–5479.\\n'\n",
      "          '\\n'\n",
      "          '[9] T. M¨uller, A. Evans, C. Schied, and A. Keller, “Instant '\n",
      "          'neural\\n'\n",
      "          'graphics primitives with a multiresolution hash encoding,” _ACM_\\n'\n",
      "          '_Trans. Graph._, vol. 41, no. 4, pp. 1–15, 2022.\\n'\n",
      "          '\\n'\n",
      "          '[10] B. Kerbl, G. Kopanas, T. Leimk¨uhler, and G. Drettakis, “3d\\n'\n",
      "          'gaussian splatting for real-time radiance field rendering,” _ACM_\\n'\n",
      "          '_Trans. Graph._, vol. 42, no. 4, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[11] V. Sitzmann, J. Thies, F. Heide, M. Nießner, G. Wetzstein, '\n",
      "          'and\\n'\n",
      "          'M. Zollhofer, “Deepvoxels: Learning persistent 3d feature '\n",
      "          'embeddings,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2019,\\n'\n",
      "          'pp. 2437–2446.\\n'\n",
      "          '\\n'\n",
      "          '[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. '\n",
      "          'Ramamoorthi, and R. Ng, “Nerf: Representing scenes as neural\\n'\n",
      "          'radiance fields for view synthesis,” in _Proc. Eur. Conf. Comput._\\n'\n",
      "          '_Vis._, 2020, pp. 405–421.\\n'\n",
      "          '\\n'\n",
      "          '[13] H. Pfister, M. Zwicker, J. Van Baar, and M. Gross, “Surfels: '\n",
      "          'Surface\\n'\n",
      "          'elements as rendering primitives,” in _Proceedings of the 27th '\n",
      "          'annual_\\n'\n",
      "          '_conference on Computer graphics and interactive techniques_, 2000, '\n",
      "          'pp.\\n'\n",
      "          '335–342.\\n'\n",
      "          '\\n'\n",
      "          '[14] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Surface\\n'\n",
      "          'splatting,” in _Proceedings of the 28th annual conference on '\n",
      "          'Computer_\\n'\n",
      "          '_graphics and interactive techniques_, 2001, pp. 371–378.\\n'\n",
      "          '\\n'\n",
      "          '[15] L. Ren, H. Pfister, and M. Zwicker, “Object space ewa surface\\n'\n",
      "          'splatting: A hardware accelerated approach to high quality point\\n'\n",
      "          'rendering,” in _Comput. Graph. Forum_, no. 3, 2002, pp. 461–470.\\n'\n",
      "          '\\n'\n",
      "          '[16] W. Yifan, F. Serena, S. Wu, C. Oztireli, and O. '\n",
      "          'Sorkine-Hornung, [¨]\\n'\n",
      "          '“Differentiable surface splatting for point-based geometry '\n",
      "          'processing,” _ACM Trans. Graph._, vol. 38, no. 6, pp. 1–14, 2019.\\n'\n",
      "          '\\n'\n",
      "          '[17] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, “Synsin: '\n",
      "          'Endto-end view synthesis from a single image,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2020, pp. 7467–7477.\\n'\n",
      "          '\\n'\n",
      "          '[18] D. Kalkofen, E. Mendez, and D. Schmalstieg, “Comprehensible\\n'\n",
      "          'visualization for augmented reality,” _IEEE Trans. Vis. Comput._\\n'\n",
      "          '_Graph._, vol. 15, no. 2, pp. 193–204, 2008.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 16,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 16\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[19] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. '\n",
      "          'Benty,\\n'\n",
      "          'D. Luebke, and A. Lefohn, “Towards foveated rendering for '\n",
      "          'gazetracked virtual reality,” _ACM Trans. Graph._, vol. 35, no. 6, '\n",
      "          'pp. 1–12,\\n'\n",
      "          '2016.\\n'\n",
      "          '\\n'\n",
      "          '[20] R. Albert, A. Patney, D. Luebke, and J. Kim, “Latency '\n",
      "          'requirements for foveated rendering in virtual reality,” _ACM '\n",
      "          'Transactions_\\n'\n",
      "          '_on Applied Perception_, vol. 14, no. 4, pp. 1–13, 2017.\\n'\n",
      "          '\\n'\n",
      "          '[21] Y. Jiang, C. Yu, T. Xie, X. Li, Y. Feng, H. Wang, M. Li, H. '\n",
      "          'Lau,\\n'\n",
      "          'F. Gao, Y. Yang _et al._, “Vr-gs: A physical dynamics-aware '\n",
      "          'interactive gaussian splatting system in virtual reality,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2401.16663_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[22] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai,\\n'\n",
      "          '“Scaffold-gs: Structured 3d gaussians for view-adaptive rendering,” '\n",
      "          'in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[23] S. Saito, G. Schwartz, T. Simon, J. Li, and G. Nam, '\n",
      "          '“Relightable\\n'\n",
      "          'gaussian codec avatars,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[24] T. Zhang, K. Huang, W. Zhi, and M. Johnson-Roberson, “Darkgs:\\n'\n",
      "          'Learning neural illumination and 3d gaussians relighting for\\n'\n",
      "          'robotic exploration in the dark,” in _Proc. IEEE/RSJ Int. Conf. '\n",
      "          'Intell._\\n'\n",
      "          '_Robot. Syst._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[25] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He, “3d '\n",
      "          'gaussian\\n'\n",
      "          'splatting as new era: A survey,” _IEEE Trans. Vis. Comput. '\n",
      "          'Graph._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[26] A. Dalal, D. Hagen, K. G. Robbersmyr, and K. M. Knausg˚ard,\\n'\n",
      "          '“Gaussian splatting: 3d reconstruction and novel view synthesis,\\n'\n",
      "          'a review,” _IEEE Access_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[27] Y. Bao, T. Ding, J. Huo, Y. Liu, Y. Li, W. Li, Y. Gao, and J. '\n",
      "          'Luo,\\n'\n",
      "          '“3d gaussian splatting: Survey, technologies, challenges, and\\n'\n",
      "          'opportunities,” _arXiv preprint arXiv:2407.17418_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[28] T. Wu, Y.-J. Yuan, L.-X. Zhang, J. Yang, Y.-P. Cao, L.-Q. Yan, '\n",
      "          'and\\n'\n",
      "          'L. Gao, “Recent advances in 3d gaussian splatting,” _Comput. Vis._\\n'\n",
      "          '_Media_, pp. 1–30, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[29] L. Kobbelt and M. Botsch, “A survey of point-based techniques '\n",
      "          'in\\n'\n",
      "          'computer graphics,” _Comput. Graph._, vol. 28, no. 6, pp. 801–814,\\n'\n",
      "          '2004.\\n'\n",
      "          '\\n'\n",
      "          '[30] Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan,\\n'\n",
      "          'F. Tombari, J. Tompkin, V. Sitzmann, and S. Sridhar, “Neural\\n'\n",
      "          'fields in visual computing and beyond,” in _Comput. Graph. Forum_,\\n'\n",
      "          'no. 2, 2022, pp. 641–676.\\n'\n",
      "          '\\n'\n",
      "          '[31] W. Wang, Y. Yang, and Y. Pan, “Visual knowledge in\\n'\n",
      "          'the big model era: Retrospect and prospect,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.04308_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[32] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. '\n",
      "          'Tretschk,\\n'\n",
      "          'W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi\\n'\n",
      "          '_et al._, “Advances in neural rendering,” in _Comput. Graph. '\n",
      "          'Forum_,\\n'\n",
      "          'no. 2, 2022, pp. 703–735.\\n'\n",
      "          '\\n'\n",
      "          '[33] X.-F. Han, H. Laga, and M. Bennamoun, “Image-based 3d object\\n'\n",
      "          'reconstruction: State-of-the-art and trends in the deep learning\\n'\n",
      "          'era,” _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 43, no. 5, '\n",
      "          'pp.\\n'\n",
      "          '1578–1604, 2019.\\n'\n",
      "          '\\n'\n",
      "          '[34] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and\\n'\n",
      "          'A. Geiger, “Occupancy networks: Learning 3d reconstruction in\\n'\n",
      "          'function space,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2019, pp. 4460–4470.\\n'\n",
      "          '\\n'\n",
      "          '[35] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. '\n",
      "          'Lovegrove,\\n'\n",
      "          '“Deepsdf: Learning continuous signed distance functions for\\n'\n",
      "          'shape representation,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2019, pp. 165–174.\\n'\n",
      "          '\\n'\n",
      "          '[36] C. Sun, M. Sun, and H.-T. Chen, “Direct voxel grid '\n",
      "          'optimization:\\n'\n",
      "          'Super-fast convergence for radiance fields reconstruction,” in\\n'\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 5459–\\n'\n",
      "          '5469.\\n'\n",
      "          '\\n'\n",
      "          '[37] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and\\n'\n",
      "          'A. Kanazawa, “K-planes: Explicit radiance fields in space, time,\\n'\n",
      "          'and appearance,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recog-_\\n'\n",
      "          '_nit._, 2023, pp. 12 479–12 488.\\n'\n",
      "          '\\n'\n",
      "          '[38] J. P. Grossman and W. J. Dally, “Point sample rendering,” in\\n'\n",
      "          '_Render. Tech._, 1998, pp. 181–192.\\n'\n",
      "          '\\n'\n",
      "          '[39] M. Zwicker, H. Pfister, J. Van Baar, and M. Gross, “Ewa '\n",
      "          'volume\\n'\n",
      "          'splatting,” in _Proceedings Visualization, 2001. VIS’01._, 2001, '\n",
      "          'pp. 29–\\n'\n",
      "          '538.\\n'\n",
      "          '\\n'\n",
      "          '[40] ——, “Ewa splatting,” _IEEE Trans. Vis. Comput. Graph._, vol. '\n",
      "          '8,\\n'\n",
      "          'no. 3, pp. 223–238, 2002.\\n'\n",
      "          '\\n'\n",
      "          '[41] K.-A. Aliev, A. Sevastopolsky, M. Kolos, D. Ulyanov, and V. '\n",
      "          'Lempitsky, “Neural point-based graphics,” in _Proc. Eur. Conf. '\n",
      "          'Comput._\\n'\n",
      "          '_Vis._, 2020, pp. 696–712.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[42] D. R¨uckert, L. Franke, and M. Stamminger, “Adop: Approximate\\n'\n",
      "          'differentiable one-pixel point rendering,” _ACM Trans. Graph._,\\n'\n",
      "          'vol. 41, no. 4, pp. 1–14, 2022.\\n'\n",
      "          '\\n'\n",
      "          '[43] C. Lassner and M. Zollhofer, “Pulsar: Efficient sphere-based '\n",
      "          'neural rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2021, pp. 1440–1449.\\n'\n",
      "          '\\n'\n",
      "          '[44] K. Cheng, X. Long, K. Yang, Y. Yao, W. Yin, Y. Ma, W. Wang, '\n",
      "          'and\\n'\n",
      "          'X. Chen, “Gaussianpro: 3d gaussian splatting with progressive\\n'\n",
      "          'propagation,” in _Proc. ACM Int. Conf. Mach. Learn._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[45] H. Xiong, S. Muttukuru, R. Upadhyay, P. Chari, and A. '\n",
      "          'Kadambi,\\n'\n",
      "          '“Sparsegs: Real-time 360 _{\\\\_ deg _}_ sparse view synthesis using\\n'\n",
      "          'gaussian splatting,” _arXiv preprint arXiv:2312.00206_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[46] Z. Zhu, Z. Fan, Y. Jiang, and Z. Wang, “Fsgs: Real-time '\n",
      "          'fewshot view synthesis using gaussian splatting,” in _Proc. Eur. '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[47] D. Charatan, S. Li, A. Tagliasacchi, and V. Sitzmann, '\n",
      "          '“pixelsplat:\\n'\n",
      "          '3d gaussian splats from image pairs for scalable generalizable 3d\\n'\n",
      "          'reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[48] S. Szymanowicz, C. Rupprecht, and A. Vedaldi, “Splatter '\n",
      "          'image:\\n'\n",
      "          'Ultra-fast single-view 3d reconstruction,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[49] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. '\n",
      "          'Gu,\\n'\n",
      "          '“Dngaussian: Optimizing sparse-view 3d gaussian radiance\\n'\n",
      "          'fields with global-local depth normalization,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[50] A. Swann, M. Strong, W. K. Do, G. S. Camps, M. Schwager, and\\n'\n",
      "          'M. Kennedy III, “Touch-gs: Visual-tactile supervised 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2403.09875_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[51] Y. Chen, H. Xu, C. Zheng, B. Zhuang, M. Pollefeys, A. Geiger,\\n'\n",
      "          'T.-J. Cham, and J. Cai, “Mvsplat: Efficient 3d gaussian splatting\\n'\n",
      "          'from sparse multi-view images,” in _Proc. Eur. Conf. Comput. '\n",
      "          'Vis._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[52] C. Wewer, K. Raj, E. Ilg, B. Schiele, and J. E. Lenssen, '\n",
      "          '“latentsplat:\\n'\n",
      "          'Autoencoding variational gaussians for fast generalizable 3d\\n'\n",
      "          'reconstruction,” _arXiv preprint arXiv:2403.16292_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[53] Y. Xu, Z. Shi, W. Yifan, H. Chen, C. Yang, S. Peng, Y. Shen,\\n'\n",
      "          'and G. Wetzstein, “Grm: Large gaussian reconstruction model\\n'\n",
      "          'for efficient 3d reconstruction and generation,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.14621_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[54] Q. Shen, X. Yi, Z. Wu, P. Zhou, H. Zhang, S. Yan, and X. '\n",
      "          'Wang,\\n'\n",
      "          '“Gamba: Marry gaussian splatting with mamba for single view\\n'\n",
      "          '3d reconstruction,” _arXiv preprint arXiv:2403.18795_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[55] J. Zhang, J. Li, X. Yu, L. Huang, L. Gu, J. Zheng, and X. Bai, '\n",
      "          '“Corgs: Sparse-view 3d gaussian splatting via co-regularization,” '\n",
      "          'in\\n'\n",
      "          '_Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[56] Z. Fan, K. Wang, K. Wen, Z. Zhu, D. Xu, and Z. Wang, '\n",
      "          '“Lightgaussian: Unbounded 3d gaussian compression with 15x '\n",
      "          'reduction and 200+ fps,” _arXiv preprint arXiv:2311.17245_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[57] K. Navaneet, K. P. Meibodi, S. A. Koohpayegani, and\\n'\n",
      "          'H. Pirsiavash, “Compact3d: Compressing gaussian splat radiance '\n",
      "          'field models with vector quantization,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2311.18159_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[58] J. C. Lee, D. Rho, X. Sun, J. H. Ko, and E. Park, “Compact 3d\\n'\n",
      "          'gaussian representation for radiance field,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[59] W. Morgenstern, F. Barthel, A. Hilsmann, and P. Eisert, '\n",
      "          '“Compact\\n'\n",
      "          '3d scene representation via self-organizing gaussian grids,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2312.13299_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[60] X. Zhang, X. Ge, T. Xu, D. He, Y. Wang, H. Qin, G. Lu, J. '\n",
      "          'Geng,\\n'\n",
      "          'and J. Zhang, “Gaussianimage: 1000 fps image representation\\n'\n",
      "          'and compression by 2d gaussian splatting,” in _Proc. Eur. Conf._\\n'\n",
      "          '_Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[61] S. Niedermayr, J. Stumpfegger, and R. Westermann, “Compressed '\n",
      "          '3d gaussian splatting for accelerated novel view synthesis,” in '\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[62] Y. Chen, Q. Wu, J. Cai, M. Harandi, and W. Lin, “Hac: '\n",
      "          'Hash-grid\\n'\n",
      "          'assisted context for 3d gaussian splatting compression,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[63] P. Papantonakis, G. Kopanas, B. Kerbl, A. Lanvin, and G. '\n",
      "          'Drettakis, “Reducing the memory footprint of 3d gaussian '\n",
      "          'splatting,”\\n'\n",
      "          'in _I3D_, 2024, pp. 1–17.\\n'\n",
      "          '\\n'\n",
      "          '[64] G. Fang and B. Wang, “Mini-splatting: Representing scenes\\n'\n",
      "          'with a constrained number of gaussians,” _arXiv_ _preprint_\\n'\n",
      "          '_arXiv:2403.14166_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 17,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 17\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[65] Z. Yu, A. Chen, B. Huang, T. Sattler, and A. Geiger, '\n",
      "          '“Mipsplatting: Alias-free 3d gaussian splatting,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024, pp. 19 447–19 456.\\n'\n",
      "          '\\n'\n",
      "          '[66] J. Gao, C. Gu, Y. Lin, H. Zhu, X. Cao, L. Zhang, and Y. Yao, '\n",
      "          '“Relightable 3d gaussian: Real-time point cloud relighting with '\n",
      "          'brdf\\n'\n",
      "          'decomposition and ray tracing,” _arXiv preprint arXiv:2311.16043_,\\n'\n",
      "          '2023.\\n'\n",
      "          '\\n'\n",
      "          '[67] Z. Yan, W. F. Low, Y. Chen, and G. H. Lee, “Multi-scale 3d\\n'\n",
      "          'gaussian splatting for anti-aliased rendering,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[68] Y. Jiang, J. Tu, Y. Liu, X. Gao, X. Long, W. Wang, and Y. Ma,\\n'\n",
      "          '“Gaussianshader: 3d gaussian splatting with shading functions\\n'\n",
      "          'for reflective surfaces,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[69] B. Lee, H. Lee, X. Sun, U. Ali, and E. Park, “Deblurring 3d\\n'\n",
      "          'gaussian splatting,” _arXiv preprint arXiv:2401.00834_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[70] D. Malarz, W. Smolak, J. Tabor, S. Tadeja, and P. Spurek, '\n",
      "          '“Gaussian splitting algorithm with color and opacity depended on\\n'\n",
      "          'viewing direction,” _arXiv preprint arXiv:2312.13729_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[71] L. Bolanos, S.-Y. Su, and H. Rhodin, “Gaussian shadow casting\\n'\n",
      "          'for neural characters,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[72] L. Radl, M. Steiner, M. Parger, A. Weinrauch, B. Kerbl, and\\n'\n",
      "          'M. Steinberger, “Stopthepop: Sorted gaussian splatting for '\n",
      "          'viewconsistent real-time rendering,” _ACM Trans. Graph._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[73] Z. Yang, X. Gao, Y. Sun, Y. Huang, X. Lyu, W. Zhou, S. Jiao, '\n",
      "          'X. Qi,\\n'\n",
      "          'and X. Jin, “Spec-gaussian: Anisotropic view-dependent appearance '\n",
      "          'for 3d gaussian splatting,” _arXiv preprint arXiv:2402.15870_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[74] C. Peng, Y. Tang, Y. Zhou, N. Wang, X. Liu, D. Li, and R. '\n",
      "          'Chellappa, “Bags: Blur agnostic gaussian splatting through '\n",
      "          'multiscale kernel modeling,” _arXiv preprint arXiv:2403.04926_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[75] L. Zhao, P. Wang, and P. Liu, “Bad-gaussians: Bundle adjusted\\n'\n",
      "          'deblur gaussian splatting,” _arXiv preprint arXiv:2403.11831_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[76] H. Dahmani, M. Bennehar, N. Piasco, L. Roldao, and D. '\n",
      "          'Tsishkou,\\n'\n",
      "          '“Swag: Splatting in the wild images with appearanceconditioned '\n",
      "          'gaussians,” _arXiv preprint arXiv:2403.10427_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[77] Y. Li, C. Lyu, Y. Di, G. Zhai, G. H. Lee, and F. Tombari, '\n",
      "          '“Geogaussian: Geometry-aware gaussian splatting for scene '\n",
      "          'rendering,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.11324_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[78] Z. Liang, Q. Zhang, W. Hu, Y. Feng, L. Zhu, and K. Jia, '\n",
      "          '“Analyticsplatting: Anti-aliased 3d gaussian splatting via analytic '\n",
      "          'integration,” _arXiv preprint arXiv:2403.11056_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[79] O. Seiskari, J. Ylilammi, V. Kaatrasalo, P. Rantalankila, M. '\n",
      "          'Turkulainen, J. Kannala, E. Rahtu, and A. Solin, “Gaussian '\n",
      "          'splatting\\n'\n",
      "          'on the move: Blur and rolling shutter compensation for natural\\n'\n",
      "          'camera motion,” _arXiv preprint arXiv:2403.13327_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[80] X. Song, J. Zheng, S. Yuan, H.-a. Gao, J. Zhao, X. He, W. Gu, '\n",
      "          'and\\n'\n",
      "          'H. Zhao, “Sa-gs: Scale-adaptive gaussian splatting for trainingfree '\n",
      "          'anti-aliasing,” _arXiv preprint arXiv:2403.19615_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[81] Y. Fu, S. Liu, A. Kulkarni, J. Kautz, A. A. Efros, and X. '\n",
      "          'Wang,\\n'\n",
      "          '“Colmap-free 3d gaussian splatting,” in _Proc. IEEE Conf. Comput._\\n'\n",
      "          '_Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[82] J. Jung, J. Han, H. An, J. Kang, S. Park, and S. Kim, '\n",
      "          '“Relaxing\\n'\n",
      "          'accurate initialization constraint for 3d gaussian splatting,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2403.09413_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[83] M. Yu, T. Lu, L. Xu, L. Jiang, Y. Xiangli, and B. Dai, “Gsdf: '\n",
      "          '3dgs\\n'\n",
      "          'meets sdf for improved rendering and reconstruction,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2403.16964_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[84] J. Zhang, F. Zhan, M. Xu, S. Lu, and E. Xing, “Fregs: 3d '\n",
      "          'gaussian\\n'\n",
      "          'splatting with progressive frequency regularization,” in _Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[85] L. Huang, J. Bai, J. Guo, and Y. Guo, “Gs++: Error analyzing '\n",
      "          'and\\n'\n",
      "          'optimal gaussian splatting,” _arXiv preprint arXiv:2402.00752_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[86] J. Li, L. Cheng, Z. Wang, T. Mu, and J. He, “Loopgaussian:\\n'\n",
      "          'Creating 3d cinemagraph with multi-view images via eulerian\\n'\n",
      "          'motion field,” _arXiv preprint arXiv:2404.08966_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[87] J.-C. Shi, M. Wang, H.-B. Duan, and S.-H. Guan, “Language '\n",
      "          'embedded 3d gaussians for open-vocabulary scene understanding,”\\n'\n",
      "          'in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[88] M. Qin, W. Li, J. Zhou, H. Wang, and H. Pfister, “Langsplat: '\n",
      "          '3d\\n'\n",
      "          'language gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[89] X. Zuo, P. Samangouei, Y. Zhou, Y. Di, and M. Li, “Fmgs:\\n'\n",
      "          'Foundation model embedded 3d gaussian splatting for holistic\\n'\n",
      "          '3d scene understanding,” _arXiv preprint arXiv:2401.01970_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[90] S. Zhou, H. Chang, S. Jiang, Z. Fan, Z. Zhu, D. Xu, P. Chari,\\n'\n",
      "          'S. You, Z. Wang, and A. Kadambi, “Feature 3dgs: Supercharging\\n'\n",
      "          '3d gaussian splatting to enable distilled feature fields,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[91] M. Ye, M. Danelljan, F. Yu, and L. Ke, “Gaussian grouping:\\n'\n",
      "          'Segment and edit anything in 3d scenes,” in _Proc. Eur. Conf._\\n'\n",
      "          '_Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[92] J. Cen, J. Fang, C. Yang, L. Xie, X. Zhang, W. Shen, and Q. '\n",
      "          'Tian,\\n'\n",
      "          '“Segment any 3d gaussians,” _arXiv preprint arXiv:2312.00860_,\\n'\n",
      "          '2023.\\n'\n",
      "          '\\n'\n",
      "          '[93] Z. Yang, H. Yang, Z. Pan, X. Zhu, and L. Zhang, “Real-time\\n'\n",
      "          'photorealistic dynamic scene representation and rendering with\\n'\n",
      "          '4d gaussian splatting,” in _Proc. Int. Conf. Learn. Represent._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[94] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, '\n",
      "          '“Deformable 3d gaussians for high-fidelity monocular dynamic scene\\n'\n",
      "          'reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[95] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. '\n",
      "          'Tian,\\n'\n",
      "          'and X. Wang, “4d gaussian splatting for real-time dynamic scene\\n'\n",
      "          'rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[96] Y. Xu, B. Chen, Z. Li, H. Zhang, L. Wang, Z. Zheng, and\\n'\n",
      "          'Y. Liu, “Gaussian head avatar: Ultra high-fidelity head avatar\\n'\n",
      "          'via dynamic gaussians,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[97] S. Szymanowicz, E. Insafutdinov, C. Zheng, D. Campbell, J. F.\\n'\n",
      "          'Henriques, C. Rupprecht, and A. Vedaldi, “Flash3d: Feedforward '\n",
      "          'generalisable 3d scene reconstruction from a single\\n'\n",
      "          'image,” _arXiv preprint arXiv:2406.04343_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[98] K. Sargent, Z. Li, T. Shah, C. Herrmann, H.-X. Yu, Y. Zhang, '\n",
      "          'E. R.\\n'\n",
      "          'Chan, D. Lagun, L. Fei-Fei, D. Sun _et al._, “Zeronvs: Zero-shot '\n",
      "          '360degree view synthesis from a single image,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024, pp. 9420–9429.\\n'\n",
      "          '\\n'\n",
      "          '[99] J. Meng, H. Li, Y. Wu, Q. Gao, S. Yang, J. Zhang, and S. Ma,\\n'\n",
      "          '“Mirror-3dgs: Incorporating mirror reflections into 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2404.01168_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[100] H. Chen, C. Li, and G. H. Lee, “Neusg: Neural implicit '\n",
      "          'surface reconstruction with 3d gaussian splatting guidance,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2312.00846_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[101] Z. Yu, T. Sattler, and A. Geiger, “Gaussian opacity fields: '\n",
      "          'Efficient\\n'\n",
      "          'and compact surface reconstruction in unbounded scenes,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2404.10772_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[102] B. Zhang, C. Fang, R. Shrestha, Y. Liang, X. Long, and P. '\n",
      "          'Tan,\\n'\n",
      "          '“Rade-gs: Rasterizing depth in gaussian splatting,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2406.01467_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[103] A. Chen, H. Xu, S. Esposito, S. Tang, and A. Geiger,\\n'\n",
      "          '“Lara: Efficient large-baseline radiance fields,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2407.04699_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[104] E. Ververas, R. A. Potamias, J. Song, J. Deng, and S. '\n",
      "          'Zafeiriou,\\n'\n",
      "          '“Sags: Structure-aware 3d gaussian splatting,” in _Proc. Eur. '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis._, 2024, pp. 221–238.\\n'\n",
      "          '\\n'\n",
      "          '[105] C. Smith, D. Charatan, A. Tewari, and V. Sitzmann, “Flowmap:\\n'\n",
      "          'High-quality camera poses, intrinsics, and depth via gradient\\n'\n",
      "          'descent,” _arXiv preprint arXiv:2404.15259_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[106] Y. Lin, Z. Dai, S. Zhu, and Y. Yao, “Gaussian-flow: 4d '\n",
      "          'reconstruction with dynamic 3d gaussian particle,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[107] A. Saroha, M. Gladkova, C. Curreli, T. Yenamandra, and\\n'\n",
      "          'D. Cremers, “Gaussian splatting in style,” _arXiv_ _preprint_\\n'\n",
      "          '_arXiv:2403.08498_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[108] N. Moenne-Loccoz, A. Mirzaei, O. Perel, R. de Lutio, J. '\n",
      "          'Martinez Esturo, G. State, S. Fidler, N. Sharp, and Z. Gojcic, “3d\\n'\n",
      "          'gaussian ray tracing: Fast tracing of particle scenes,” _ACM '\n",
      "          'Trans._\\n'\n",
      "          '_Graph._, vol. 43, no. 6, pp. 1–19, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[109] A. Mai, P. Hedman, G. Kopanas, D. Verbin, D. Futschik, Q. '\n",
      "          'Xu,\\n'\n",
      "          'F. Kuester, J. T. Barron, and Y. Zhang, “Ever: Exact volumetric\\n'\n",
      "          'ellipsoid rendering for real-time view synthesis,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2410.01804_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[110] J. Condor, S. Speierer, L. Bode, A. Bozic, S. Green, P. '\n",
      "          'Didyk, and\\n'\n",
      "          'A. Jarabo, “Don’t splat your gaussians: Volumetric ray-traced\\n'\n",
      "          'primitives for modeling and rendering scattering and emissive\\n'\n",
      "          'media,” _ACM Trans. Graph._, 2025.\\n'\n",
      "          '\\n'\n",
      "          '[111] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li,\\n'\n",
      "          '“Gs-slam: Dense visual slam with 3d gaussian splatting,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[112] N. Keetha, J. Karhade, K. M. Jatavallabhula, G. Yang, S. '\n",
      "          'Scherer,\\n'\n",
      "          'D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 18,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 18\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'gaussians for dense rgb-d slam,” in _Proc. IEEE Conf. Comput. '\n",
      "          'Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[113] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, '\n",
      "          '“Gaussian\\n'\n",
      "          'splatting slam,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[114] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, '\n",
      "          '“Gaussian-slam:\\n'\n",
      "          'Photo-realistic dense slam with gaussian splatting,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2312.10070_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[115] H. Huang, L. Li, H. Cheng, and S.-K. Yeung, “Photo-slam: '\n",
      "          'Realtime simultaneous localization and photorealistic mapping for\\n'\n",
      "          'monocular, stereo, and rgb-d cameras,” in _Proc. IEEE Conf. Com-_\\n'\n",
      "          '_put. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[116] M. Li, S. Liu, and H. Zhou, “Sgs-slam: Semantic gaussian '\n",
      "          'splatting for neural dense slam,” in _Proc. Eur. Conf. Comput. '\n",
      "          'Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[117] Y. Ji, Y. Liu, G. Xie, B. Ma, and Z. Xie, “Neds-slam: A '\n",
      "          'novel\\n'\n",
      "          'neural explicit dense semantic slam framework using 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2403.11679_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[118] G. Lu, S. Zhang, Z. Wang, C. Liu, J. Lu, and Y. Tang, '\n",
      "          '“Manigaussian: Dynamic gaussian splatting for multi-task robotic '\n",
      "          'manipulation,” _arXiv preprint arXiv:2403.08321_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[119] J. Abou-Chakra, K. Rana, F. Dayoub, and N. S¨underhauf, '\n",
      "          '“Physically embodied gaussian splatting: A realtime correctable '\n",
      "          'world\\n'\n",
      "          'model for robotics,” in _Proc. Annu. Conf. Robot Learn._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[120] O. Shorinwa, J. Tucker, A. Smith, A. Swann, T. Chen, R. '\n",
      "          'Firoozi,\\n'\n",
      "          'M. D. Kennedy, and M. Schwager, “Splat-mover: Multi-stage,\\n'\n",
      "          'open-vocabulary robotic manipulation via editable gaussian\\n'\n",
      "          'splatting,” in _Proc. Annu. Conf. Robot Learn._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[121] M. Ji, R.-Z. Qiu, X. Zou, and X. Wang, “Graspsplats: '\n",
      "          'Efficient manipulation with 3d feature splatting,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2409.02084_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[122] Y. Zheng, X. Chen, Y. Zheng, S. Gu, R. Yang, B. Jin, P. Li, '\n",
      "          'C. Zhong,\\n'\n",
      "          'Z. Wang, L. Liu _et al._, “Gaussiangrasper: 3d language gaussian\\n'\n",
      "          'splatting for open-vocabulary robotic grasping,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.09637_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[123] S. Zhu, R. Qin, G. Wang, J. Liu, and H. Wang, “Semgaussslam: '\n",
      "          'Dense semantic gaussian splatting slam,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.07494_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[124] Z. Peng, T. Shao, Y. Liu, J. Zhou, Y. Yang, J. Wang, and K. '\n",
      "          'Zhou,\\n'\n",
      "          '“Rtg-slam: Real-time 3d reconstruction at scale using gaussian\\n'\n",
      "          'splatting,” _ACM Trans. Graph._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[125] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, “Dynamic 3d\\n'\n",
      "          'gaussians: Tracking by persistent dynamic view synthesis,” in\\n'\n",
      "          '_Proc. Int. Conf. 3D Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[126] H. Yu, J. Julin, Z. A. Milacski, K. Niinuma, and L. A. Jeni, '\n",
      "          '“Cogs: [´]\\n'\n",
      "          'Controllable gaussian splatting,” in _Proc. IEEE Conf. Comput. '\n",
      "          'Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[127] R. Shao, J. Sun, C. Peng, Z. Zheng, B. Zhou, H. Zhang, and Y. '\n",
      "          'Liu,\\n'\n",
      "          '“Control4d: Efficient 4d portrait editing with text,” in _Proc. '\n",
      "          'IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[128] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. '\n",
      "          'Qi,\\n'\n",
      "          '“Sc-gs: Sparse-controlled gaussian splatting for editable dynamic\\n'\n",
      "          'scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[129] D. Das, C. Wewer, R. Yunus, E. Ilg, and J. E. Lenssen, '\n",
      "          '“Neural\\n'\n",
      "          'parametric gaussians for monocular non-rigid object '\n",
      "          'reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[130] Z. Li, Z. Chen, Z. Li, and Y. Xu, “Spacetime gaussian '\n",
      "          'feature\\n'\n",
      "          'splatting for real-time dynamic view synthesis,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[131] J. Sun, H. Jiao, G. Li, Z. Zhang, L. Zhao, and W. Xing, '\n",
      "          '“3dgstream:\\n'\n",
      "          'On-the-fly training of 3d gaussians for efficient streaming of\\n'\n",
      "          'photo-realistic free-viewpoint videos,” in _Proc. IEEE Conf. Com-_\\n'\n",
      "          '_put. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[132] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, '\n",
      "          'and\\n'\n",
      "          'Y. Dai, “3d geometry-aware deformable gaussian splatting for\\n'\n",
      "          'dynamic view synthesis,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[133] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, '\n",
      "          '“Dreamgaussian:\\n'\n",
      "          'Generative gaussian splatting for efficient 3d content creation,”\\n'\n",
      "          'in _Proc. Int. Conf. Learn. Represent._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[134] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. '\n",
      "          'Tian,\\n'\n",
      "          'and X. Wang, “Gaussiandreamer: Fast generation from text to 3d\\n'\n",
      "          'gaussians by bridging 2d and 3d diffusion models,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[135] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu, '\n",
      "          '“Lgm:\\n'\n",
      "          'Large multi-view gaussian model for high-resolution 3d content\\n'\n",
      "          'creation,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[136] S. Zhou, Z. Fan, D. Xu, H. Chang, P. Chari, T. Bharadwaj, S. '\n",
      "          'You,\\n'\n",
      "          'Z. Wang, and A. Kadambi, “Dreamscene360: Unconstrained textto-3d '\n",
      "          'scene generation with panoramic gaussian splatting,” in\\n'\n",
      "          '_Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[137] Z. Li, Y. Chen, L. Zhao, and P. Liu, “Controllable '\n",
      "          'text-to-3d\\n'\n",
      "          'generation via surface-aligned gaussian splatting,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2403.09981_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[138] Y. Mu, X. Zuo, C. Guo, Y. Wang, J. Lu, X. Wu, S. Xu, P. Dai, '\n",
      "          'Y. Yan,\\n'\n",
      "          'and L. Cheng, “Gsd: View-guided gaussian splatting diffusion\\n'\n",
      "          'for 3d reconstruction,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[139] Y. Jiang, Z. Shen, P. Wang, Z. Su, Y. Hong, Y. Zhang, J. Yu, '\n",
      "          'and\\n'\n",
      "          'L. Xu, “Hifi4g: High-fidelity human performance rendering via\\n'\n",
      "          'compact gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[140] Y. Wang, Q. Wu, G. Zhang, and D. Xu, “Gscream: Learning 3d\\n'\n",
      "          'geometry and feature consistent gaussian splatting for object\\n'\n",
      "          'removal,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[141] Y. Chen, Z. Chen, C. Zhang, F. Wang, X. Yang, Y. Wang, Z. '\n",
      "          'Cai,\\n'\n",
      "          'L. Yang, H. Liu, and G. Lin, “Gaussianeditor: Swift and '\n",
      "          'controllable 3d editing with gaussian splatting,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[142] J. Fang, J. Wang, X. Zhang, L. Xie, and Q. Tian, '\n",
      "          '“Gaussianeditor:\\n'\n",
      "          'Editing 3d gaussians delicately with text instructions,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[143] R.-Z. Qiu, G. Yang, W. Zeng, and X. Wang, “Feature '\n",
      "          'splatting:\\n'\n",
      "          'Language-driven physics-based scene synthesis and editing,”\\n'\n",
      "          '_arXiv preprint arXiv:2404.01223_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[144] Z. Li, Z. Zheng, L. Wang, and Y. Liu, “Animatable gaussians:\\n'\n",
      "          'Learning pose-dependent gaussian maps for high-fidelity human\\n'\n",
      "          'avatar modeling,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recog-_\\n'\n",
      "          '_nit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[145] S. Hu and Z. Liu, “Gauhuman: Articulated gaussian splatting\\n'\n",
      "          'from monocular human videos,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[146] J. Lei, Y. Wang, G. Pavlakos, L. Liu, and K. Daniilidis, '\n",
      "          '“Gart:\\n'\n",
      "          'Gaussian articulated template models,” in _Proc. IEEE Conf. Com-_\\n'\n",
      "          '_put. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[147] Y. Yuan, X. Li, Y. Huang, S. De Mello, K. Nagano, J. Kautz, '\n",
      "          'and\\n'\n",
      "          'U. Iqbal, “Gavatar: Animatable 3d gaussian avatars with implicit\\n'\n",
      "          'mesh learning,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[148] Z. Zhou, F. Ma, H. Fan, and Y. Yang, “Headstudio: Text to\\n'\n",
      "          'animatable head avatars with 3d gaussian splatting,” in _Proc. '\n",
      "          'Eur._\\n'\n",
      "          '_Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[149] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. '\n",
      "          'Giebenhain,\\n'\n",
      "          'and M. Nießner, “Gaussianavatars: Photorealistic head avatars\\n'\n",
      "          'with rigged 3d gaussians,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[150] H. Dhamo, Y. Nie, A. Moreau, J. Song, R. Shaw, Y. Zhou, and\\n'\n",
      "          'E. P´erez-Pellitero, “Headgas: Real-time animatable head avatars\\n'\n",
      "          'via 3d gaussian splatting,” _arXiv preprint arXiv:2312.02902_, '\n",
      "          '2023.\\n'\n",
      "          '\\n'\n",
      "          '[151] J. Li, J. Zhang, X. Bai, J. Zheng, X. Ning, J. Zhou, and L. '\n",
      "          'Gu,\\n'\n",
      "          '“Talkinggaussian: Structure-persistent 3d talking head synthesis\\n'\n",
      "          'via gaussian splatting,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[152] Y. Huang, B. Cui, L. Bai, Z. Guo, M. Xu, and H. Ren, '\n",
      "          '“Endo4dgs: Distilling depth ranking for endoscopic monocular scene\\n'\n",
      "          'reconstruction with 4d gaussian splatting,” in _Proc. Int. Conf. '\n",
      "          'Med._\\n'\n",
      "          '_Image Comput. Comput. Assist. Interv._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[153] Y. Liu, C. Li, C. Yang, and Y. Yuan, “Endogaussian: Gaussian\\n'\n",
      "          'splatting for deformable surgical scene reconstruction,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2401.12561_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[154] L. Zhu, Z. Wang, Z. Jin, G. Lin, and L. Yu, “Deformable '\n",
      "          'endoscopic tissues reconstruction with gaussian splatting,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2401.11535_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[155] H. Zhao, X. Zhao, L. Zhu, W. Zheng, and Y. Xu, “Hfgs: 4d\\n'\n",
      "          'gaussian splatting with emphasis on spatial and temporal '\n",
      "          'highfrequency components for endoscopic scene reconstruction,”\\n'\n",
      "          '_arXiv preprint arXiv:2405.17872_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[156] K. Wang, C. Yang, Y. Wang, S. Li, Y. Wang, Q. Dou, X. Yang,\\n'\n",
      "          'and W. Shen, “Endogslam: Real-time dense reconstruction and\\n'\n",
      "          'tracking in endoscopic surgeries using gaussian splatting,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2403.15124_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[157] S. Bonilla, S. Zhang, D. Psychogyios, D. Stoyanov, F. '\n",
      "          'Vasconcelos,\\n'\n",
      "          'and S. Bano, “Gaussian pancakes: Geometrically-regularized 3d\\n'\n",
      "          'gaussian splatting for realistic endoscopic reconstruction,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2404.06128_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 19,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 19\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[158] K. Wu, K. Zhang, Z. Zhang, S. Yuan, M. Tie, J. Wei, Z. Xu, J. '\n",
      "          'Zhao,\\n'\n",
      "          'Z. Gan, and W. Ding, “Hgs-mapping: Online dense mapping\\n'\n",
      "          'using hybrid gaussian representation in urban scenes,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2403.20159_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[159] C. Wu, Y. Duan, X. Zhang, Y. Sheng, J. Ji, and Y. Zhang, '\n",
      "          '“Mmgaussian: 3d gaussian-based multi-modal fusion for localization\\n'\n",
      "          'and reconstruction in unbounded scenes,” in _Proc. IEEE/RSJ Int._\\n'\n",
      "          '_Conf. Intell. Robot. Syst._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[160] B. Xiong, Z. Li, and Z. Li, “Gauu-scene: A scene '\n",
      "          'reconstruction\\n'\n",
      "          'benchmark on large scale 3d reconstruction dataset using gaussian '\n",
      "          'splatting,” _arXiv preprint arXiv:2401.14032_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[161] H. Zhao, H. Weng, D. Lu, A. Li, J. Li, A. Panda, and S. Xie,\\n'\n",
      "          '“On scaling up 3d gaussian splatting training,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2406.18533_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[162] B. Kerbl, A. Meuleman, G. Kopanas, M. Wimmer, A. Lanvin, and\\n'\n",
      "          'G. Drettakis, “A hierarchical 3d gaussian representation for '\n",
      "          'realtime rendering of very large datasets,” _ACM Trans. Graph._, '\n",
      "          'vol. 44,\\n'\n",
      "          'no. 3, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[163] Y. Liu, H. Guan, C. Luo, L. Fan, J. Peng, and Z. Zhang, '\n",
      "          '“Citygaussian: Real-time high-quality large-scale scene rendering '\n",
      "          'with\\n'\n",
      "          'gaussians,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[164] J. Lin, Z. Li, X. Tang, J. Liu, S. Liu, J. Liu, Y. Lu, X. Wu, '\n",
      "          'S. Xu,\\n'\n",
      "          'Y. Yan _et al._, “Vastgaussian: Vast 3d gaussians for large scene\\n'\n",
      "          'reconstruction,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[165] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, '\n",
      "          '“Octreegs: Towards consistent real-time rendering with '\n",
      "          'lod-structured\\n'\n",
      "          '3d gaussians,” _arXiv preprint arXiv:2403.17898_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[166] T. Xie, Z. Zong, Y. Qiu, X. Li, Y. Feng, Y. Yang, and C. '\n",
      "          'Jiang,\\n'\n",
      "          '“Physgaussian: Physics-integrated 3d gaussians for generative\\n'\n",
      "          'dynamics,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[167] F. Liu, H. Wang, S. Yao, S. Zhang, J. Zhou, and Y. Duan,\\n'\n",
      "          '“Physics3d: Learning physical properties of 3d gaussians via\\n'\n",
      "          'video diffusion,” _arXiv preprint arXiv:2406.04338_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[168] P. Borycki, W. Smolak, J. Waczy´nska, M. Mazur, S. Tadeja, '\n",
      "          'and\\n'\n",
      "          'P. Spurek, “Gasp: Gaussian splatting for physic-based simulations,” '\n",
      "          '_arXiv preprint arXiv:2409.05819_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[169] T. Huang, Y. Zeng, H. Li, W. Zuo, and R. W. Lau, '\n",
      "          '“Dreamphysics:\\n'\n",
      "          'Learning physical properties of dynamic 3d gaussians with video\\n'\n",
      "          'diffusion priors,” in _Proc. AAAI Conf. Artif. Intell._, 2025.\\n'\n",
      "          '\\n'\n",
      "          '[170] T. Zhang, H.-X. Yu, R. Wu, B. Y. Feng, C. Zheng, N. Snavely, '\n",
      "          'J. Wu,\\n'\n",
      "          'and W. T. Freeman, “Physdreamer: Physics-based interaction\\n'\n",
      "          'with 3d objects via video generation,” in _Proc. Eur. Conf. '\n",
      "          'Comput._\\n'\n",
      "          '_Vis._, 2024, pp. 388–406.\\n'\n",
      "          '\\n'\n",
      "          '[171] Y. Feng, X. Feng, Y. Shang, Y. Jiang, C. Yu, Z. Zong, T. '\n",
      "          'Shao,\\n'\n",
      "          'H. Wu, K. Zhou, C. Jiang _et al._, “Gaussian splashing: Dynamic '\n",
      "          'fluid synthesis with gaussian splatting,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2401.15318_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[172] L. Zhong, H.-X. Yu, J. Wu, and Y. Li, “Reconstruction and '\n",
      "          'simulation of elastic objects with spring-mass 3d gaussians,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[173] Y. Shao, M. Huang, C. C. Loy, and B. Dai, “Gausim: '\n",
      "          'Registering\\n'\n",
      "          'elastic objects into digital world by gaussian simulator,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2412.17804_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[174] S. Zhang, H. Zhao, Z. Zhou, G. Wu, C. Zheng, X. Wang, and\\n'\n",
      "          'W. Liu, “Togs: Gaussian splatting with temporal opacity offset\\n'\n",
      "          'for real-time 4d dsa rendering,” _arXiv preprint '\n",
      "          'arXiv:2403.19586_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[175] R. Wu, Z. Zhang, Y. Yang, and W. Zuo, “Dual-camera smooth\\n'\n",
      "          'zoom on mobile phones,” _arXiv preprint arXiv:2404.04908_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[176] H. Li, Y. Gao, D. Zhang, C. Wu, Y. Dai, C. Zhao, H. Feng, E. '\n",
      "          'Ding,\\n'\n",
      "          'J. Wang, and J. Han, “Ggrt: Towards generalizable 3d gaussians\\n'\n",
      "          'without pose priors in real-time,” _arXiv preprint '\n",
      "          'arXiv:2403.10147_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[177] S. Hong, J. He, X. Zheng, H. Wang, H. Fang, K. Liu, C. Zheng, '\n",
      "          'and\\n'\n",
      "          'S. Shen, “Liv-gaussmap: Lidar-inertial-visual fusion for real-time\\n'\n",
      "          '3d radiance field map rendering,” _arXiv preprint '\n",
      "          'arXiv:2401.14857_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[178] S. Sun, M. Mielle, A. J. Lilienthal, and M. Magnusson, '\n",
      "          '“Highfidelity slam using gaussian splatting with rendering-guided\\n'\n",
      "          'densification and regularized optimization,” in _Proc. IEEE/RSJ_\\n'\n",
      "          '_Int. Conf. Intell. Robot. Syst._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[179] F. Tosi, Y. Zhang, Z. Gong, E. Sandstr¨om, S. Mattoccia, M. '\n",
      "          'R.\\n'\n",
      "          'Oswald, and M. Poggi, “How nerfs and 3d gaussian splatting are\\n'\n",
      "          'reshaping slam: a survey,” _arXiv preprint arXiv:2402.13255_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[180] T. Deng, Y. Chen, L. Zhang, J. Yang, S. Yuan, D. Wang, and\\n'\n",
      "          'W. Chen, “Compact 3d gaussian splatting for dense visual slam,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.11247_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[181] J. Hu, X. Chen, B. Feng, G. Li, L. Yang, H. Bao, G. Zhang,\\n'\n",
      "          'and Z. Cui, “Cg-slam: Efficient dense rgb-d slam in a consistent '\n",
      "          'uncertainty-aware 3d gaussian field,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.16095_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[182] X. Lang, L. Li, H. Zhang, F. Xiong, M. Xu, Y. Liu, X. Zuo, '\n",
      "          'and J. Lv,\\n'\n",
      "          '“Gaussian-lic: Photo-realistic lidar-inertial-camera slam with 3d\\n'\n",
      "          'gaussian splatting,” _arXiv preprint arXiv:2404.06926_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[183] E. Sandstr¨om, K. Tateno, M. Oechsle, M. Niemeyer, L. Van '\n",
      "          'Gool,\\n'\n",
      "          'M. R. Oswald, and F. Tombari, “Splat-slam: Globally optimized '\n",
      "          'rgb-only slam with 3d gaussians,” _arXiv_ _preprint_\\n'\n",
      "          '_arXiv:2405.16544_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[184] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer,\\n'\n",
      "          '“D-nerf: Neural radiance fields for dynamic scenes,” in _Proc. '\n",
      "          'IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2021, pp. 10 318–10 327.\\n'\n",
      "          '\\n'\n",
      "          '[185] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. '\n",
      "          'B.\\n'\n",
      "          'Goldman, R. Martin-Brualla, and S. M. Seitz, “Hypernerf: a\\n'\n",
      "          'higher-dimensional representation for topologically varying neural '\n",
      "          'radiance fields,” _ACM Trans. Graph._, vol. 40, no. 6, pp. 1–12,\\n'\n",
      "          '2021.\\n'\n",
      "          '\\n'\n",
      "          '[186] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman,\\n'\n",
      "          'S. M. Seitz, and R. Martin-Brualla, “Nerfies: Deformable neural\\n'\n",
      "          'radiance fields,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2021, '\n",
      "          'pp.\\n'\n",
      "          '5865–5874.\\n'\n",
      "          '\\n'\n",
      "          '[187] X. Guo, J. Sun, Y. Dai, G. Chen, X. Ye, X. Tan, E. Ding, Y. '\n",
      "          'Zhang,\\n'\n",
      "          'and J. Wang, “Forward flow for novel view synthesis of dynamic\\n'\n",
      "          'scenes,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2023, pp. 16 '\n",
      "          '022–\\n'\n",
      "          '16 033.\\n'\n",
      "          '\\n'\n",
      "          '[188] X. Zhou, Z. Lin, X. Shan, Y. Wang, D. Sun, and M.-H. Yang,\\n'\n",
      "          '“Drivinggaussian: Composite gaussian splatting for surrounding\\n'\n",
      "          'dynamic autonomous driving scenes,” in _Proc. IEEE Conf. Com-_\\n'\n",
      "          '_put. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[189] Y. Yan, H. Lin, C. Zhou, W. Wang, H. Sun, K. Zhan, X. Lang,\\n'\n",
      "          'X. Zhou, and S. Peng, “Street gaussians for modeling dynamic\\n'\n",
      "          'urban scenes,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[190] H. Zhou, J. Shao, L. Xu, D. Bai, W. Qiu, B. Liu, Y. Wang, A. '\n",
      "          'Geiger,\\n'\n",
      "          'and Y. Liao, “Hugs: Holistic urban 3d scene understanding\\n'\n",
      "          'via gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024, pp. 21 336–21 345.\\n'\n",
      "          '\\n'\n",
      "          '[191] A. Kratimenos, J. Lei, and K. Daniilidis, “Dynmf: Neural '\n",
      "          'motion\\n'\n",
      "          'factorization for real-time dynamic view synthesis with 3d gaussian '\n",
      "          'splatting,” _arXiv preprint arXiv:2312.00112_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[192] R. Shaw, J. Song, A. Moreau, M. Nazarczuk, S. '\n",
      "          'Catley-Chandar,\\n'\n",
      "          'H. Dhamo, and E. Perez-Pellitero, “Swags: Sampling windows\\n'\n",
      "          'adaptively for dynamic 3d gaussian splatting,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.13308_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[193] Y. Liang, N. Khan, Z. Li, T. Nguyen-Phuoc, D. Lanman,\\n'\n",
      "          'J. Tompkin, and L. Xiao, “Gaufre: Gaussian deformation fields\\n'\n",
      "          'for real-time dynamic novel view synthesis,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.11458_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[194] K. Katsumata, D. M. Vo, and H. Nakayama, “An efficient 3d '\n",
      "          'gaussian representation for monocular/multi-view dynamic scenes,”\\n'\n",
      "          '_arXiv preprint arXiv:2311.12897_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[195] Z. Guo, W. Zhou, L. Li, M. Wang, and H. Li, “Motion-aware 3d\\n'\n",
      "          'gaussian splatting for efficient dynamic scene reconstruction,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.11447_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[196] J. Bae, S. Kim, Y. Yun, H. Lee, G. Bang, and Y. Uh, '\n",
      "          '“Per-gaussian\\n'\n",
      "          'embedding-based deformation for deformable 3d gaussian splatting,” '\n",
      "          '_arXiv preprint arXiv:2404.03613_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[197] J. Lei, Y. Weng, A. Harley, L. Guibas, and K. Daniilidis, '\n",
      "          '“Mosca:\\n'\n",
      "          'Dynamic gaussian fusion from casual videos via 4d motion\\n'\n",
      "          'scaffolds,” _arXiv preprint arXiv:2405.17421_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[198] Q. Wang, V. Ye, H. Gao, J. Austin, Z. Li, and A. Kanazawa, '\n",
      "          '“Shape\\n'\n",
      "          'of motion: 4d reconstruction from a single video,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2407.13764_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[199] Y. Duan, F. Wei, Q. Dai, Y. He, W. Chen, and B. Chen, '\n",
      "          '“4drotor gaussian splatting: towards efficient novel view '\n",
      "          'synthesis\\n'\n",
      "          'for dynamic scenes,” in _Proc. ACM Spec. Interest Group Comput._\\n'\n",
      "          '_Graph. Interact. Tech._, 2024, pp. 1–11.\\n'\n",
      "          '\\n'\n",
      "          '[200] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. '\n",
      "          'WardeFarley, S. Ozair, A. Courville, and Y. Bengio, “Generative '\n",
      "          'adversarial networks,” _Communications of the ACM_, vol. 63, no. '\n",
      "          '11, pp.\\n'\n",
      "          '139–144, 2020.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 20,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 20\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[201] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion '\n",
      "          'probabilistic\\n'\n",
      "          'models,” in _Proc. Adv. Neural Inf. Process. Syst._, 2020, pp. '\n",
      "          '6840–\\n'\n",
      "          '6851.\\n'\n",
      "          '\\n'\n",
      "          '[202] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\\n'\n",
      "          '“High-resolution image synthesis with latent diffusion models,”\\n'\n",
      "          'in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 10 '\n",
      "          '684–\\n'\n",
      "          '10 695.\\n'\n",
      "          '\\n'\n",
      "          '[203] L. Zhang, A. Rao, and M. Agrawala, “Adding conditional '\n",
      "          'control to text-to-image diffusion models,” in _Proc. IEEE Int. '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis._, 2023, pp. 3836–3847.\\n'\n",
      "          '\\n'\n",
      "          '[204] Z. Chen, F. Wang, and H. Liu, “Text-to-3d using gaussian '\n",
      "          'splatting,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[205] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, '\n",
      "          '“Luciddreamer: Towards high-fidelity text-to-3d generation via '\n",
      "          'interval\\n'\n",
      "          'score matching,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[206] X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu,\\n'\n",
      "          'and Z. Liu, “Humangaussian: Text-driven 3d human generation\\n'\n",
      "          'with gaussian splatting,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[207] X. Yang, Y. Chen, C. Chen, C. Zhang, Y. Xu, X. Yang, F. Liu, '\n",
      "          'and\\n'\n",
      "          'G. Lin, “Learn to optimize denoising scores for 3d generation: A\\n'\n",
      "          'unified and improved diffusion prior on nerf and 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2312.04820_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[208] Z.-X. Zou, Z. Yu, Y.-C. Guo, Y. Li, D. Liang, Y.-P. Cao, and '\n",
      "          'S.-H.\\n'\n",
      "          'Zhang, “Triplane meets gaussian splatting: Fast and generalizable '\n",
      "          'single-view 3d reconstruction with transformers,” in _Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[209] H. Ling, S. W. Kim, A. Torralba, S. Fidler, and K. Kreis, '\n",
      "          '“Align\\n'\n",
      "          'your gaussians: Text-to-4d with dynamic 3d gaussians and composed '\n",
      "          'diffusion models,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[210] J. Ren, L. Pan, J. Tang, C. Zhang, A. Cao, G. Zeng, and Z. '\n",
      "          'Liu,\\n'\n",
      "          '“Dreamgaussian4d: Generative 4d gaussian splatting,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2312.17142_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[211] Y. Yin, D. Xu, Z. Wang, Y. Zhao, and Y. Wei, “4dgen: '\n",
      "          'Grounded\\n'\n",
      "          '4d content generation with spatial-temporal consistency,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2312.17225_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[212] J. Zhang, Z. Tang, Y. Pang, X. Cheng, P. Jin, Y. Wei, W. Yu,\\n'\n",
      "          'M. Ning, and L. Yuan, “Repaint123: Fast and high-quality one\\n'\n",
      "          'image to 3d generation with progressive controllable 2d '\n",
      "          'repainting,” _arXiv preprint arXiv:2312.13271_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[213] Z. Pan, Z. Yang, X. Zhu, and L. Zhang, “Fast dynamic 3d\\n'\n",
      "          'object generation from a single-view video,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2401.08742_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[214] D. Xu, Y. Yuan, M. Mardani, S. Liu, J. Song, Z. Wang, and\\n'\n",
      "          'A. Vahdat, “Agg: Amortized generative 3d gaussians for single\\n'\n",
      "          'image to 3d,” _arXiv preprint arXiv:2401.04099_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[215] C. Yang, S. Li, J. Fang, R. Liang, L. Xie, X. Zhang, W. '\n",
      "          'Shen,\\n'\n",
      "          'and Q. Tian, “Gaussianobject: Just taking four images to get a\\n'\n",
      "          'high-quality 3d object with gaussian splatting,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2402.10259_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[216] F. Barthel, A. Beckmann, W. Morgenstern, A. Hilsmann, and\\n'\n",
      "          'P. Eisert, “Gaussian splatting decoder for 3d-aware generative\\n'\n",
      "          'adversarial networks,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit. Worksh._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[217] L. Jiang and L. Wang, “Brightdreamer: Generic 3d gaussian\\n'\n",
      "          'generative framework for fast text-to-3d synthesis,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2403.11273_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[218] W. Zhuo, F. Ma, H. Fan, and Y. Yang, “Vividdreamer: '\n",
      "          'Invariant\\n'\n",
      "          'score distillation for hyper-realistic text-to-3d generation,” in\\n'\n",
      "          '_Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[219] Z. Wu, C. Yu, Y. Jiang, C. Cao, F. Wang, and X. Bai, “Sc4d:\\n'\n",
      "          'Sparse-controlled video-to-4d generation and motion transfer,”\\n'\n",
      "          '_arXiv preprint arXiv:2404.03736_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[220] X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan,\\n'\n",
      "          'W. Ouyang, and T. He, “Gvgen: Text-to-3d generation with\\n'\n",
      "          'volumetric representation,” in _Proc. Eur. Conf. Comput. Vis._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[221] X. Yang and X. Wang, “Hash3d: Training-free acceleration for '\n",
      "          '3d\\n'\n",
      "          'generation,” _arXiv preprint arXiv:2404.06091_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[222] J. Kim, J. Koo, K. Yeo, and M. Sung, “Synctweedies: A '\n",
      "          'general\\n'\n",
      "          'generative framework based on synchronized diffusions,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2403.14370_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[223] Q. Feng, Z. Xing, Z. Wu, and Y.-G. Jiang, “Fdgaussian: Fast '\n",
      "          'gaussian splatting from single image via geometric-aware diffusion\\n'\n",
      "          'model,” _arXiv preprint arXiv:2403.10242_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[224] H. Li, H. Shi, W. Zhang, W. Wu, Y. Liao, L. Wang, L.-h.\\n'\n",
      "          'Lee, and P. Zhou, “Dreamscene: 3d gaussian-based text-to-3d\\n'\n",
      "          'scene generation via formation pattern sampling,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.03575_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[225] L. Melas-Kyriazi, I. Laina, C. Rupprecht, N. Neverova,\\n'\n",
      "          'A. Vedaldi, O. Gafni, and F. Kokkinos, “Im-3d: Iterative multiview '\n",
      "          'diffusion and reconstruction for high-quality 3d generation,” in '\n",
      "          '_Proc. ACM Int. Conf. Mach. Learn._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[226] B. Zhang, Y. Cheng, J. Yang, C. Wang, F. Zhao, Y. Tang, D. '\n",
      "          'Chen,\\n'\n",
      "          'and B. Guo, “Gaussiancube: Structuring gaussian splatting using\\n'\n",
      "          'optimal transport for 3d generative modeling,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2403.19655_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[227] Y.-C. Lee, Y.-T. Chen, A. Wang, T.-H. Liao, B. Y. Feng, and\\n'\n",
      "          'J.-B. Huang, “Vividdream: Generating 3d scene with ambient\\n'\n",
      "          'dynamics,” _arXiv preprint arXiv:2405.20334_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[228] J. Huang and H. Yu, “Point’n move: Interactive scene object '\n",
      "          'manipulation on gaussian splatting radiance fields,” _arXiv '\n",
      "          'preprint_\\n'\n",
      "          '_arXiv:2311.16737_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[229] K. Lan, H. Li, H. Shi, W. Wu, Y. Liao, L. Wang, and\\n'\n",
      "          'P. Zhou, “2d-guided 3d gaussian segmentation,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.16047_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[230] J. Zhuang, D. Kang, Y.-P. Cao, G. Li, L. Lin, and Y. Shan, '\n",
      "          '“Tipeditor: An accurate 3d editor following both text-prompts and\\n'\n",
      "          'image-prompts,” _arXiv preprint arXiv:2401.14828_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[231] B. Dou, T. Zhang, Y. Ma, Z. Wang, and Z. Yuan, '\n",
      "          '“Cosseggaussians: Compact and swift scene segmenting 3d gaussians,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2401.05925_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[232] X. Hu, Y. Wang, L. Fan, J. Fan, J. Peng, Z. Lei, Q. Li, and\\n'\n",
      "          'Z. Zhang, “Semantic anything in 3d gaussians,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2401.17857_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[233] F. Palandra, A. Sanchietti, D. Baieri, and E. Rodol`a, '\n",
      "          '“Gsedit:\\n'\n",
      "          'Efficient text-guided editing of 3d objects via gaussian '\n",
      "          'splatting,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.05154_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[234] Q. Gu, Z. Lv, D. Frost, S. Green, J. Straub, and C. Sweeney, '\n",
      "          '“Egolifter: Open-world 3d segmentation for egocentric perception,”\\n'\n",
      "          '_arXiv preprint arXiv:2403.18118_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[235] W. Lyu, X. Li, A. Kundu, Y.-H. Tsai, and M.-H. Yang, “Gaga:\\n'\n",
      "          'Group any gaussians via 3d-aware memory bank,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.07977_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[236] Z. Liu, H. Ouyang, Q. Wang, K. L. Cheng, J. Xiao, K. Zhu, N. '\n",
      "          'Xue,\\n'\n",
      "          'Y. Liu, Y. Shen, and Y. Cao, “Infusion: Inpainting 3d gaussians '\n",
      "          'via\\n'\n",
      "          'learning depth completion from diffusion prior,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.11613_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[237] D. Zhang, Z. Chen, Y.-J. Yuan, F.-L. Zhang, Z. He, S. Shan,\\n'\n",
      "          'and L. Gao, “Stylizedgs: Controllable stylization for 3d gaussian\\n'\n",
      "          'splatting,” _arXiv preprint arXiv:2404.05220_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[238] Q. Zhang, Y. Xu, C. Wang, H.-Y. Lee, G. Wetzstein, B. Zhou,\\n'\n",
      "          'and C. Yang, “3ditscene: Editing any scene via language-guided\\n'\n",
      "          'disentangled gaussian splatting,” _arXiv preprint '\n",
      "          'arXiv:2405.18424_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[239] J. Wu, J.-W. Bian, X. Li, G. Wang, I. Reid, P. Torr, and V. '\n",
      "          'A.\\n'\n",
      "          'Prisacariu, “Gaussctrl: Multi-view consistent text-driven 3d '\n",
      "          'gaussian splatting editing,” in _Proc. Eur. Conf. Comput. Vis._, '\n",
      "          '2024, pp.\\n'\n",
      "          '55–71.\\n'\n",
      "          '\\n'\n",
      "          '[240] Y. Wang, X. Yi, Z. Wu, N. Zhao, L. Chen, and H. Zhang, '\n",
      "          '“Viewconsistent 3d editing with gaussian splatting,” in _Proc. Eur. '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis._, 2024, pp. 404–420.\\n'\n",
      "          '\\n'\n",
      "          '[241] R. Jena, G. S. Iyer, S. Choudhary, B. Smith, P. Chaudhari,\\n'\n",
      "          'and J. Gee, “Splatarmor: Articulated gaussian splatting for '\n",
      "          'animatable humans from monocular rgb videos,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2311.10812_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[242] K. Ye, T. Shao, and K. Zhou, “Animatable 3d gaussians\\n'\n",
      "          'for high-fidelity synthesis of human motions,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2311.13404_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[243] A. Moreau, J. Song, H. Dhamo, R. Shaw, Y. Zhou, and E. '\n",
      "          'P´erezPellitero, “Human gaussian splatting: Real-time rendering of\\n'\n",
      "          'animatable avatars,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[244] M. Kocabas, J.-H. R. Chang, J. Gabriel, O. Tuzel, and A. '\n",
      "          'Ranjan,\\n'\n",
      "          '“Hugs: Human gaussian splats,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[245] R. Abdal, W. Yifan, Z. Shi, Y. Xu, R. Po, Z. Kuang, Q. Chen, '\n",
      "          'D.Y. Yeung, and G. Wetzstein, “Gaussian shell maps for efficient\\n'\n",
      "          '3d human generation,” in _Proc. IEEE Conf. Comput. Vis. Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[246] S. Zheng, B. Zhou, R. Shao, B. Liu, S. Zhang, L. Nie, and Y. '\n",
      "          'Liu,\\n'\n",
      "          '“Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 21,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 21\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'for real-time human novel view synthesis,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[247] L. Hu, H. Zhang, Y. Zhang, B. Zhou, B. Liu, S. Zhang, and L. '\n",
      "          'Nie,\\n'\n",
      "          '“Gaussianavatar: Towards realistic human avatar modeling from\\n'\n",
      "          'a single video via animatable 3d gaussians,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[248] H. Pang, H. Zhu, A. Kortylewski, C. Theobalt, and M. '\n",
      "          'Habermann, “Ash: Animatable gaussian splats for efficient and '\n",
      "          'photoreal human rendering,” in _Proc. IEEE Conf. Comput. Vis. '\n",
      "          'Pattern_\\n'\n",
      "          '_Recognit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[249] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang, '\n",
      "          '“3dgsavatar: Animatable avatars via deformable 3d gaussian '\n",
      "          'splatting,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[250] H. Jung, N. Brasch, J. Song, E. Perez-Pellitero, Y. Zhou, Z. '\n",
      "          'Li,\\n'\n",
      "          'N. Navab, and B. Busam, “Deformable 3d gaussian splatting\\n'\n",
      "          'for animatable human avatars,” _arXiv preprint arXiv:2312.15059_,\\n'\n",
      "          '2023.\\n'\n",
      "          '\\n'\n",
      "          '[251] M. Li, J. Tao, Z. Yang, and Y. Yang, “Human101: Training\\n'\n",
      "          '100+ fps human gaussians in 100s from 1 view,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.15258_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[252] M. Li, S. Yao, Z. Xie, K. Chen, and Y.-G. Jiang, '\n",
      "          '“Gaussianbody:\\n'\n",
      "          'Clothed human reconstruction via 3d gaussian splatting,” _arXiv_\\n'\n",
      "          '_preprint arXiv:2401.09720_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[253] J. Xiang, X. Gao, Y. Guo, and J. Zhang, “Flashavatar: '\n",
      "          'Highfidelity digital avatar rendering at 300fps,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2312.02214_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[254] Y. Chen, L. Wang, Q. Li, H. Xiao, S. Zhang, H. Yao, and\\n'\n",
      "          'Y. Liu, “Monogaussianavatar: Monocular gaussian point-based\\n'\n",
      "          'head avatar,” _arXiv preprint arXiv:2312.04558_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[255] Z. Zhao, Z. Bao, Q. Li, G. Qiu, and K. Liu, “Psavatar: A '\n",
      "          'pointbased morphable shape model for real-time head avatar '\n",
      "          'creation\\n'\n",
      "          'with 3d gaussian splatting,” _arXiv preprint arXiv:2401.12900_, '\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[256] A. Rivero, S. Athar, Z. Shu, and D. Samaras, “Rig3dgs: '\n",
      "          'Creating controllable portraits from casual monocular videos,” '\n",
      "          '_arXiv_\\n'\n",
      "          '_preprint arXiv:2402.03723_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[257] H. Luo, M. Ouyang, Z. Zhao, S. Jiang, L. Zhang, Q. Zhang,\\n'\n",
      "          'W. Yang, L. Xu, and J. Yu, “Gaussianhair: Hair modeling and '\n",
      "          'rendering with light-aware gaussians,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2402.10483_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[258] Y. Wang, Y. Long, S. H. Fan, and Q. Dou, “Neural rendering '\n",
      "          'for\\n'\n",
      "          'stereo 3d reconstruction of deformable tissues in robotic '\n",
      "          'surgery,”\\n'\n",
      "          'in _Proc. Int. Conf. Med. Image Comput. Comput. Assist. Interv._, '\n",
      "          '2022,\\n'\n",
      "          'pp. 431–441.\\n'\n",
      "          '\\n'\n",
      "          '[259] C. Yang, K. Wang, Y. Wang, X. Yang, and W. Shen, “Neural\\n'\n",
      "          'lerplane representations for fast 4d reconstruction of deformable\\n'\n",
      "          'tissues,” in _Proc. Int. Conf. Med. Image Comput. Comput. Assist._\\n'\n",
      "          '_Interv._, 2023, pp. 46–56.\\n'\n",
      "          '\\n'\n",
      "          '[260] R. Zha, X. Cheng, H. Li, M. Harandi, and Z. Ge, “Endosurf:\\n'\n",
      "          'Neural surface reconstruction of deformable tissues with stereo\\n'\n",
      "          'endoscope videos,” in _Proc. Int. Conf. Med. Image Comput. '\n",
      "          'Comput._\\n'\n",
      "          '_Assist. Interv._, 2023, pp. 13–23.\\n'\n",
      "          '\\n'\n",
      "          '[261] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, '\n",
      "          'J. J. Engel, R. Mur-Artal, C. Ren, S. Verma _et al._, “The replica '\n",
      "          'dataset: A\\n'\n",
      "          'digital replica of indoor spaces,” _arXiv preprint '\n",
      "          'arXiv:1906.05797_,\\n'\n",
      "          '2019.\\n'\n",
      "          '\\n'\n",
      "          '[262] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison, “imap: '\n",
      "          'Implicit\\n'\n",
      "          'mapping and positioning in real-time,” in _Proc. IEEE Int. Conf._\\n'\n",
      "          '_Comput. Vis._, 2021, pp. 6229–6238.\\n'\n",
      "          '\\n'\n",
      "          '[263] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, '\n",
      "          '“Voxfusion: Dense tracking and mapping with voxel-based neural '\n",
      "          'implicit representation,” in _IEEE International Symposium on '\n",
      "          'Mixed_\\n'\n",
      "          '_and Augmented Reality_, 2022, pp. 499–507.\\n'\n",
      "          '\\n'\n",
      "          '[264] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. '\n",
      "          'Oswald,\\n'\n",
      "          'and M. Pollefeys, “Nice-slam: Neural implicit scalable encoding\\n'\n",
      "          'for slam,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2022,\\n'\n",
      "          'pp. 12 786–12 796.\\n'\n",
      "          '\\n'\n",
      "          '[265] M. M. Johari, C. Carta, and F. Fleuret, “Eslam: Efficient '\n",
      "          'dense\\n'\n",
      "          'slam system based on hybrid representation of signed distance\\n'\n",
      "          'fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2023,\\n'\n",
      "          'pp. 17 408–17 419.\\n'\n",
      "          '\\n'\n",
      "          '[266] E. Sandstr¨om, Y. Li, L. Van Gool, and M. R. Oswald, '\n",
      "          '“Point-slam:\\n'\n",
      "          'Dense neural point cloud-based slam,” in _Proc. IEEE Int. Conf._\\n'\n",
      "          '_Comput. Vis._, 2023, pp. 18 433–18 444.\\n'\n",
      "          '\\n'\n",
      "          '[267] H. Wang, J. Wang, and L. Agapito, “Co-slam: Joint coordinate '\n",
      "          'and\\n'\n",
      "          'sparse parametric encodings for neural real-time slam,” in _Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 13 293–13 '\n",
      "          '302.\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '[268] Y. Hu, Y. Fang, Z. Ge, Z. Qu, Y. Zhu, A. Pradhana, and C. '\n",
      "          'Jiang,\\n'\n",
      "          '“A moving least squares material point method with displacement '\n",
      "          'discontinuity and two-way rigid body coupling,” _ACM_\\n'\n",
      "          '_Trans. Graph._, vol. 37, no. 4, pp. 1–14, 2018.\\n'\n",
      "          '\\n'\n",
      "          '[269] M. M¨uller, B. Heidelberger, M. Hennix, and J. Ratcliff, '\n",
      "          '“Position\\n'\n",
      "          'based dynamics,” _Journal of Visual Communication and Image Rep-_\\n'\n",
      "          '_resentation_, vol. 18, no. 2, pp. 109–118, 2007.\\n'\n",
      "          '\\n'\n",
      "          '[270] A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun, “Tanks and\\n'\n",
      "          'temples: Benchmarking large-scale scene reconstruction,” _ACM_\\n'\n",
      "          '_Trans. Graph._, vol. 36, no. 4, pp. 1–13, 2017.\\n'\n",
      "          '\\n'\n",
      "          '[271] T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely, '\n",
      "          '“Stereo\\n'\n",
      "          'magnification: learning view synthesis using multiplane images,”\\n'\n",
      "          '_ACM Trans. Graph._, vol. 37, no. 4, pp. 1–12, 2018.\\n'\n",
      "          '\\n'\n",
      "          '[272] P. Hedman, J. Philip, T. Price, J.-M. Frahm, G. Drettakis, '\n",
      "          'and\\n'\n",
      "          'G. Brostow, “Deep blending for free-viewpoint image-based '\n",
      "          'rendering,” _ACM Trans. Graph._, vol. 37, no. 6, pp. 1–15, 2018.\\n'\n",
      "          '\\n'\n",
      "          '[273] B. Mildenhall, P. P. Srinivasan, R. Ortiz-Cayon, N. K. '\n",
      "          'Kalantari,\\n'\n",
      "          'R. Ramamoorthi, R. Ng, and A. Kar, “Local light field fusion:\\n'\n",
      "          'Practical view synthesis with prescriptive sampling guidelines,”\\n'\n",
      "          '_ACM Trans. Graph._, vol. 38, no. 4, pp. 1–14, 2019.\\n'\n",
      "          '\\n'\n",
      "          '[274] A. Liu, R. Tucker, V. Jampani, A. Makadia, N. Snavely, and\\n'\n",
      "          'A. Kanazawa, “Infinite nature: Perpetual view generation of\\n'\n",
      "          'natural scenes from a single image,” in _Proc. IEEE Int. Conf._\\n'\n",
      "          '_Comput. Vis._, 2021, pp. 14 458–14 467.\\n'\n",
      "          '\\n'\n",
      "          '[275] J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. '\n",
      "          'Cremers,\\n'\n",
      "          '“A benchmark for the evaluation of rgb-d slam systems,” in _Proc._\\n'\n",
      "          '_IEEE/RSJ Int. Conf. Intell. Robot. Syst._, 2012, pp. 573–580.\\n'\n",
      "          '\\n'\n",
      "          '[276] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for '\n",
      "          'autonomous driving? the kitti vision benchmark suite,” in _Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2012, pp. 3354–3361.\\n'\n",
      "          '\\n'\n",
      "          '[277] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and\\n'\n",
      "          'M. Nießner, “Scannet: Richly-annotated 3d reconstructions of\\n'\n",
      "          'indoor scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2017, pp. 5828–5839.\\n'\n",
      "          '\\n'\n",
      "          '[278] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. '\n",
      "          'Patnaik,\\n'\n",
      "          'P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine _et al._, “Scalability '\n",
      "          'in\\n'\n",
      "          'perception for autonomous driving: Waymo open dataset,” in\\n'\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2020, pp. 2446–\\n'\n",
      "          '2454.\\n'\n",
      "          '\\n'\n",
      "          '[279] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. '\n",
      "          'Xu,\\n'\n",
      "          'A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A\\n'\n",
      "          'multimodal dataset for autonomous driving,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2020, pp. 11 621–11 631.\\n'\n",
      "          '\\n'\n",
      "          '[280] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison, “Rlbench:\\n'\n",
      "          'The robot learning benchmark & learning environment,” _IEEE_\\n'\n",
      "          '_Robotics and Automation Letters_, vol. 5, no. 2, pp. 3019–3026, '\n",
      "          '2020.\\n'\n",
      "          '\\n'\n",
      "          '[281] A. Mandlekar, D. Xu, J. Wong, S. Nasiriany, C. Wang, R. '\n",
      "          'Kulkarni,\\n'\n",
      "          'L. Fei-Fei, S. Savarese, Y. Zhu, and R. Mart´ın-Mart´ın, “What\\n'\n",
      "          'matters in learning from offline human demonstrations for robot\\n'\n",
      "          'manipulation,” in _Proc. Annu. Conf. Robot Learn._, 2022, pp. '\n",
      "          '1678–\\n'\n",
      "          '1690.\\n'\n",
      "          '\\n'\n",
      "          '[282] Z. Yan, C. Li, and G. H. Lee, “Nerf-ds: Neural radiance '\n",
      "          'fields\\n'\n",
      "          'for dynamic specular objects,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2023, pp. 8285–8295.\\n'\n",
      "          '\\n'\n",
      "          '[283] K. Kania, K. M. Yi, M. Kowalski, T. Trzci´nski, and A. '\n",
      "          'Tagliasacchi,\\n'\n",
      "          '“Conerf: Controllable neural radiance fields,” in _Proc. IEEE '\n",
      "          'Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2022, pp. 18 623–18 632.\\n'\n",
      "          '\\n'\n",
      "          '[284] A. Mirzaei, T. Aumentado-Armstrong, K. G. Derpanis, J. '\n",
      "          'Kelly,\\n'\n",
      "          'M. A. Brubaker, I. Gilitschenski, and A. Levinshtein, “Spin-nerf:\\n'\n",
      "          'Multiview segmentation and perceptual inpainting with neural\\n'\n",
      "          'radiance fields,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2023, pp. 20 669–20 679.\\n'\n",
      "          '\\n'\n",
      "          '[285] R. Shao, Z. Zheng, H. Tu, B. Liu, H. Zhang, and Y. Liu, '\n",
      "          '“Tensor4d:\\n'\n",
      "          'Efficient neural 4d decomposition for high-fidelity dynamic '\n",
      "          'reconstruction and rendering,” in _Proc. IEEE Conf. Comput. Vis._\\n'\n",
      "          '_Pattern Recognit._, 2023, pp. 16 632–16 642.\\n'\n",
      "          '\\n'\n",
      "          '[286] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. '\n",
      "          'Yang,\\n'\n",
      "          'J. Wang, C. Qian _et al._, “Omniobject3d: Large-vocabulary 3d\\n'\n",
      "          'object dataset for realistic perception, reconstruction and '\n",
      "          'generation,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2023, pp.\\n'\n",
      "          '803–814.\\n'\n",
      "          '\\n'\n",
      "          '[287] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. '\n",
      "          'VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi,\\n'\n",
      "          '“Objaverse: A universe of annotated 3d objects,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 13 142–13 153.\\n'\n",
      "          '\\n'\n",
      "          '[288] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. '\n",
      "          'Pons-Moll,\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []},\n",
      " {'graphics': [],\n",
      "  'images': [],\n",
      "  'metadata': {'author': '',\n",
      "               'creationDate': 'D:20250310005050Z',\n",
      "               'creator': 'LaTeX with hyperref',\n",
      "               'encryption': None,\n",
      "               'file_path': 'data/raw_papers/A survey on 3DGS.pdf',\n",
      "               'format': 'PDF 1.5',\n",
      "               'keywords': '',\n",
      "               'modDate': 'D:20250310005050Z',\n",
      "               'page': 22,\n",
      "               'page_count': 22,\n",
      "               'producer': 'pdfTeX-1.40.25',\n",
      "               'subject': '',\n",
      "               'title': '',\n",
      "               'trapped': ''},\n",
      "  'tables': [],\n",
      "  'text': 'IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 22\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '“Video based reconstruction of 3d people models,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2018, pp. 8387–8397.\\n'\n",
      "          '\\n'\n",
      "          '[289] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. '\n",
      "          'Black,\\n'\n",
      "          '“Capture, learning, and synthesis of 3d speaking styles,” in '\n",
      "          '_Proc._\\n'\n",
      "          '_IEEE Conf. Comput. Vis. Pattern Recognit._, 2019, pp. 10 101–10 '\n",
      "          '111.\\n'\n",
      "          '\\n'\n",
      "          '[290] Z. Zheng, T. Yu, Y. Wei, Q. Dai, and Y. Liu, “Deephuman: 3d\\n'\n",
      "          'human reconstruction from a single image,” in _Proc. IEEE Int._\\n'\n",
      "          '_Conf. Comput. Vis._, 2019, pp. 7739–7749.\\n'\n",
      "          '\\n'\n",
      "          '[291] T. Yu, Z. Zheng, K. Guo, P. Liu, Q. Dai, and Y. Liu, '\n",
      "          '“Function4d:\\n'\n",
      "          'Real-time human volumetric capture from very sparse consumer\\n'\n",
      "          'rgbd sensors,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2021, pp. 5746–5756.\\n'\n",
      "          '\\n'\n",
      "          '[292] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. '\n",
      "          'Zhou,\\n'\n",
      "          '“Neural body: Implicit neural representations with structured\\n'\n",
      "          'latent codes for novel view synthesis of dynamic humans,” in\\n'\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2021, pp. 9054–\\n'\n",
      "          '9063.\\n'\n",
      "          '\\n'\n",
      "          '[293] E. Ramon, G. Triginer, J. Escur, A. Pumarola, J. Garcia, X. '\n",
      "          'Giro-i\\n'\n",
      "          'Nieto, and F. Moreno-Noguer, “H3d-net: Few-shot high-fidelity\\n'\n",
      "          '3d head reconstruction,” in _Proc. IEEE Int. Conf. Comput. Vis._,\\n'\n",
      "          '2021, pp. 5620–5629.\\n'\n",
      "          '\\n'\n",
      "          '[294] Z. Su, T. Yu, Y. Wang, and Y. Liu, “Deepcloth: Neural '\n",
      "          'garment\\n'\n",
      "          'representation for shape and style editing,” _IEEE Trans. Pattern_\\n'\n",
      "          '_Anal. Mach. Intell._, vol. 45, no. 2, pp. 1581–1593, 2022.\\n'\n",
      "          '\\n'\n",
      "          '[295] M. Allan, J. Mcleod, C. Wang, J. C. Rosenthal, Z. Hu, N. '\n",
      "          'Gard,\\n'\n",
      "          'P. Eisert, K. X. Fu, T. Zeffiro, W. Xia _et al._, “Stereo '\n",
      "          'correspondence\\n'\n",
      "          'and reconstruction of endoscopic data challenge,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2101.01133_, 2021.\\n'\n",
      "          '\\n'\n",
      "          '[296] Y. Cai, J. Wang, A. Yuille, Z. Zhou, and A. Wang, '\n",
      "          '“Structureaware sparse-view x-ray 3d reconstruction,” in _Proc. '\n",
      "          'IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2024, pp. 11 174–11 183.\\n'\n",
      "          '\\n'\n",
      "          '[297] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. '\n",
      "          'Dai,\\n'\n",
      "          'and D. Lin, “Bungeenerf: Progressive neural radiance field for\\n'\n",
      "          'extreme multi-scale scene rendering,” in _Proc. Eur. Conf. '\n",
      "          'Comput._\\n'\n",
      "          '_Vis._ Springer, 2022, pp. 106–122.\\n'\n",
      "          '\\n'\n",
      "          '[298] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. '\n",
      "          'P.\\n'\n",
      "          'Srinivasan, J. T. Barron, and H. Kretzschmar, “Block-nerf: '\n",
      "          'Scalable\\n'\n",
      "          'large scene neural view synthesis,” in _Proc. IEEE Conf. Comput._\\n'\n",
      "          '_Vis. Pattern Recognit._, 2022, pp. 8248–8258.\\n'\n",
      "          '\\n'\n",
      "          '[299] G. Yang, F. Xue, Q. Zhang, K. Xie, C.-W. Fu, and H. Huang, '\n",
      "          '“Urbanbis: a large-scale benchmark for fine-grained urban building\\n'\n",
      "          'instance segmentation,” in _Proc. ACM Spec. Interest Group '\n",
      "          'Comput._\\n'\n",
      "          '_Graph. Interact. Tech._, 2023, pp. 1–11.\\n'\n",
      "          '\\n'\n",
      "          '[300] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, '\n",
      "          '“Image\\n'\n",
      "          'quality assessment: from error visibility to structural '\n",
      "          'similarity,”\\n'\n",
      "          '_IEEE Trans. Image Process._, vol. 13, no. 4, pp. 600–612, 2004.\\n'\n",
      "          '\\n'\n",
      "          '[301] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,\\n'\n",
      "          '“The unreasonable effectiveness of deep features as a perceptual\\n'\n",
      "          'metric,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, '\n",
      "          '2018,\\n'\n",
      "          'pp. 586–595.\\n'\n",
      "          '\\n'\n",
      "          '[302] J. Fang, T. Yi, X. Wang, L. Xie, X. Zhang, W. Liu, M. '\n",
      "          'Nießner, and\\n'\n",
      "          'Q. Tian, “Fast dynamic radiance fields with time-aware neural\\n'\n",
      "          'voxels,” in _SIGGRAPH Asia_, 2022, pp. 1–9.\\n'\n",
      "          '\\n'\n",
      "          '[303] A. Cao and J. Johnson, “Hexplane: A fast representation for '\n",
      "          'dynamic scenes,” in _Proc. IEEE Conf. Comput. Vis. Pattern '\n",
      "          'Recognit._,\\n'\n",
      "          '2023, pp. 130–141.\\n'\n",
      "          '\\n'\n",
      "          '[304] F. Wang, Z. Chen, G. Wang, Y. Song, and H. Liu, “Masked '\n",
      "          'spacetime hash encoding for efficient dynamic scene '\n",
      "          'reconstruction,”\\n'\n",
      "          'in _Proc. Adv. Neural Inf. Process. Syst._, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[305] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and\\n'\n",
      "          'I. Kemelmacher-Shlizerman, “Humannerf: Free-viewpoint rendering of '\n",
      "          'moving people from monocular video,” in _Proc. IEEE_\\n'\n",
      "          '_Conf. Comput. Vis. Pattern Recognit._, 2022, pp. 16 210–16 220.\\n'\n",
      "          '\\n'\n",
      "          '[306] C. Geng, S. Peng, Z. Xu, H. Bao, and X. Zhou, “Learning '\n",
      "          'neural\\n'\n",
      "          'volumetric representations of dynamic humans in minutes,” in\\n'\n",
      "          '_Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2023, pp. 8759–\\n'\n",
      "          '8770.\\n'\n",
      "          '\\n'\n",
      "          '[307] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and\\n'\n",
      "          'H. Bao, “Animatable neural radiance fields for modeling dynamic '\n",
      "          'human bodies,” in _Proc. IEEE Int. Conf. Comput. Vis._, 2021,\\n'\n",
      "          'pp. 14 314–14 323.\\n'\n",
      "          '\\n'\n",
      "          '[308] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, “pixelnerf: Neural\\n'\n",
      "          'radiance fields from one or few images,” in _Proc. IEEE Conf._\\n'\n",
      "          '_Comput. Vis. Pattern Recognit._, 2021, pp. 4578–4587.\\n'\n",
      "          '\\n'\n",
      "          '[309] Y. Kwon, D. Kim, D. Ceylan, and H. Fuchs, “Neural human\\n'\n",
      "          'performer: Learning generalizable radiance fields for human\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          '\\n'\n",
      "          'performance rendering,” in _Proc. Adv. Neural Inf. Process. '\n",
      "          'Syst._,\\n'\n",
      "          '2021, pp. 24 741–24 752.\\n'\n",
      "          '\\n'\n",
      "          '[310] J. Wang, Z. Zhang, Q. Zhang, J. Li, J. Sun, M. Sun, J. He, '\n",
      "          'and R. Xu,\\n'\n",
      "          '“Query-based semantic gaussian field for scene representation in\\n'\n",
      "          'reinforcement learning,” _arXiv preprint arXiv:2406.02370_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[311] Y. Qu, S. Dai, X. Li, J. Lin, L. Cao, S. Zhang, and R. Ji, '\n",
      "          '“Goi: Find\\n'\n",
      "          '3d gaussians of interest with an optimizable open-vocabulary\\n'\n",
      "          'semantic-space hyperplane,” _arXiv preprint arXiv:2405.17596_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[312] Y. Ji, H. Zhu, J. Tang, W. Liu, Z. Zhang, Y. Xie, L. Ma, and\\n'\n",
      "          'X. Tan, “Fastlgs: Speeding up language embedded gaussians with\\n'\n",
      "          'feature grid mapping,” _arXiv preprint arXiv:2406.01916_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[313] G. Liao, J. Li, Z. Bao, X. Ye, J. Wang, Q. Li, and K. Liu,\\n'\n",
      "          '“Clip-gs: Clip-informed gaussian splatting for real-time and\\n'\n",
      "          'view-consistent 3d semantic understanding,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2404.14249_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[314] S. Choi, H. Song, J. Kim, T. Kim, and H. Do, '\n",
      "          '“Click-gaussian:\\n'\n",
      "          'Interactive segmentation to any 3d gaussians,” _arXiv preprint_\\n'\n",
      "          '_arXiv:2407.11793_, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[315] S. Ji, G. Wu, J. Fang, J. Cen, T. Yi, W. Liu, Q. Tian, and X. '\n",
      "          'Wang,\\n'\n",
      "          '“Segment any 4d gaussians,” _arXiv preprint arXiv:2407.04504_,\\n'\n",
      "          '2024.\\n'\n",
      "          '\\n'\n",
      "          '[316] A. Gu´edon and V. Lepetit, “Sugar: Surface-aligned gaussian\\n'\n",
      "          'splatting for efficient 3d mesh reconstruction and high-quality\\n'\n",
      "          'mesh rendering,” in _Proc. IEEE Conf. Comput. Vis. Pattern Recog-_\\n'\n",
      "          '_nit._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[317] T. Liu, G. Wang, S. Hu, L. Shen, X. Ye, Y. Zang, Z. Cao, W. '\n",
      "          'Li,\\n'\n",
      "          'and Z. Liu, “Fast generalizable gaussian splatting reconstruction\\n'\n",
      "          'from multi-view stereo,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[318] Y. Li, X. Fu, S. Zhao, R. Jin, and S. K. Zhou, “Sparse-view\\n'\n",
      "          'ct reconstruction with 3d gaussian volumetric representation,”\\n'\n",
      "          '_arXiv preprint arXiv:2312.15676_, 2023.\\n'\n",
      "          '\\n'\n",
      "          '[319] Y. Cai, Y. Liang, J. Wang, A. Wang, Y. Zhang, X. Yang, Z. '\n",
      "          'Zhou,\\n'\n",
      "          'and A. Yuille, “Radiative gaussian splatting for efficient x-ray\\n'\n",
      "          'novel view synthesis,” in _Proc. Eur. Conf. Comput. Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '[320] J. Chang, Y. Xu, Y. Li, Y. Chen, and X. Han, “Gaussreg: Fast '\n",
      "          '3d\\n'\n",
      "          'registration with gaussian splatting,” in _Proc. Eur. Conf. '\n",
      "          'Comput._\\n'\n",
      "          '_Vis._, 2024.\\n'\n",
      "          '\\n'\n",
      "          '\\n',\n",
      "  'toc_items': [],\n",
      "  'words': []}]\n"
     ]
    }
   ],
   "source": [
    "chunks_images = pymupdf4llm.to_markdown(doc='data/raw_papers/A survey on 3DGS.pdf',\n",
    "                                 page_chunks=True, write_images=True, image_path='output_images')\n",
    "\n",
    "pprint.pprint(chunks_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
