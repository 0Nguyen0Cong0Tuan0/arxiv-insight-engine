{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Validation Notebook\n",
    "Tests all components of the Multi-Agent LangGraph System.\n",
    "Date: 11:11 AM +07, November 01, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: d:\\Learn\\AIE Project\\arxiv-insight-engine\n",
      "src/ exists: True\n"
     ]
    }
   ],
   "source": [
    "# --- Setup Project Path ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    path = Path.cwd()\n",
    "    while path != path.parent:\n",
    "        if (path / \"src\").exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    return Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"src/ exists: {(PROJECT_ROOT / 'src').exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid text for BM25. Falling back to vector retriever only.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\nguye\\anaconda3\\envs\\arxiv-insight\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import os\n",
    "from src.agents.graph import app, AgentState\n",
    "from src.agents.tools.hybrid_retriever import EnsembleRetriever\n",
    "from src.agents.tools.summarizer import Summarizer\n",
    "from src.agents.tools.image_captioner import ImageCaptioner\n",
    "from src.agents.nodes.retriever import retrieve\n",
    "from src.agents.nodes.summarizer import summarize\n",
    "from src.agents.nodes.visual_analyzer import analyze_figures\n",
    "from src.agents.nodes.synthesizer import synthesize\n",
    "from src.agents.nodes.fact_checker import fact_check\n",
    "from src.stores.feedback_store import store_feedback\n",
    "from langchain_core.messages import HumanMessage\n",
    "import uuid\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid text for BM25. Falling back to vector retriever only.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1.1 Hybrid Retriever ---\n",
    "hybrid_retriever = EnsembleRetriever.from_qdrant()\n",
    "\n",
    "# Pass the user query\n",
    "query = \"what is machine learning?\"\n",
    "results = hybrid_retriever.retrieve(query, k=10)\n",
    "\n",
    "# Process results\n",
    "if results:\n",
    "    for doc in results:\n",
    "        print(f\"Content: {doc.page_content[:100]}... Type: {doc.metadata.get('type', 'unknown')}\")\n",
    "else:\n",
    "    print(\"No results returned. Check Qdrant data or retriever setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'_id': '6fae486d-e968-41d6-acb6-c0aef723580d', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': '98874f8b-0af2-4a55-9fb0-683eb98bf363', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': 'f100e488-e798-43e8-bb38-8e1747b5cc6e', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': '0cb13cf3-9b53-4b4c-a1f9-df3eede22c71', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': '4ff3b431-c22b-46bc-87b4-a2bb2023edb6', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': 'bf78cb7d-0ecd-4092-a524-9163a36ac7df', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': '81583444-0f8f-4579-8bec-593f4a3f3ce1', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': 'a3ad5297-1ae2-4a6e-889b-544d5b878bb8', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': '8f69f66c-21eb-4f9d-973b-6a047d44efcb', '_collection_name': 'arxiv_multimodal'}, page_content=''),\n",
       " Document(metadata={'_id': 'd086134b-614a-4ed0-8b54-84bdc5cdff5b', '_collection_name': 'arxiv_multimodal'}, page_content='')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: This is a long text about LLMs and their scaling laws. This is aLong text about LLMs and their scali...\n"
     ]
    }
   ],
   "source": [
    "# --- 1.2 Summarizer ---\n",
    "summarizer = Summarizer()\n",
    "sample_text = \"This is a long text about LLMs and their scaling laws...\" * 10\n",
    "summaries = summarizer.summarize_texts([sample_text])\n",
    "print(f\"Summary: {summaries[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: Error: Incorrect image source. Must be a valid URL starting with `http://` or `https://`, a valid path to an image file, or a base64 encoded string. Got iVBORw0KG.... Failed with Invalid base64-encoded string: number of data characters (9) cannot be 1 more than a multiple of 4\n"
     ]
    }
   ],
   "source": [
    "# --- 1.3 Image Captioner ---\n",
    "captioner = ImageCaptioner()\n",
    "# Mock base64 (use a real one from your data if available)\n",
    "mock_b64 = \"data:image/png;base64,iVBORw0KG...\"  # Truncated for brevity\n",
    "captions = captioner.caption_images([mock_b64])\n",
    "print(f\"Caption: {captions[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'PointStruct' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 1.4 Feedback Store ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mstore_feedback\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest query\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCorrected: use HyDE method\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFeedback stored successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Learn\\AIE Project\\arxiv-insight-engine\\src\\stores\\feedback_store.py:32\u001b[39m, in \u001b[36mstore_feedback\u001b[39m\u001b[34m(query, correction)\u001b[39m\n\u001b[32m     26\u001b[39m vector = embed_text(query)\n\u001b[32m     27\u001b[39m points = PointStruct(\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mid\u001b[39m=\u001b[38;5;28mstr\u001b[39m(uuid4()),\n\u001b[32m     29\u001b[39m     vector=vector,\n\u001b[32m     30\u001b[39m     payload={\u001b[33m\"\u001b[39m\u001b[33mcorrection\u001b[39m\u001b[33m\"\u001b[39m: correction}\n\u001b[32m     31\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mQDRANT_FEEDBACK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\nguye\\anaconda3\\envs\\arxiv-insight\\Lib\\site-packages\\qdrant_client\\qdrant_client.py:1604\u001b[39m, in \u001b[36mQdrantClient.upsert\u001b[39m\u001b[34m(self, collection_name, points, wait, ordering, shard_key_selector, **kwargs)\u001b[39m\n\u001b[32m   1574\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1575\u001b[39m \u001b[33;03mUpdate or insert a new point into the collection.\u001b[39;00m\n\u001b[32m   1576\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1598\u001b[39m \u001b[33;03m    Operation Result(UpdateResult)\u001b[39;00m\n\u001b[32m   1599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1600\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) == \u001b[32m0\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1602\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1603\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(points, types.Batch)\n\u001b[32m-> \u001b[39m\u001b[32m1604\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m > \u001b[32m0\u001b[39m\n\u001b[32m   1605\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(points[\u001b[32m0\u001b[39m], grpc.PointStruct)\n\u001b[32m   1606\u001b[39m ):\n\u001b[32m   1607\u001b[39m     \u001b[38;5;66;03m# gRPC structures won't support local inference feature, so we deprecated it\u001b[39;00m\n\u001b[32m   1608\u001b[39m     show_warning_once(\n\u001b[32m   1609\u001b[39m         message=\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1610\u001b[39m \u001b[33m    Usage of `grpc.PointStruct` is deprecated. Please use `models.PointStruct` instead.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1614\u001b[39m         stacklevel=\u001b[32m4\u001b[39m,\n\u001b[32m   1615\u001b[39m     )\n\u001b[32m   1617\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cloud_inference \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inference_inspector.inspect(points):\n",
      "\u001b[31mTypeError\u001b[39m: object of type 'PointStruct' has no len()"
     ]
    }
   ],
   "source": [
    "# --- 1.4 Feedback Store ---\n",
    "store_feedback(\"test query\", \"Corrected: use HyDE method\")\n",
    "print(\"Feedback stored successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.1 Retriever Node ---\n",
    "initial_state = {\"query\": \"scaling laws in LLMs\", \"messages\": [HumanMessage(content=\"Start\")]}\n",
    "retrieved_state = retrieve(initial_state)\n",
    "print(f\"Retrieved {len(retrieved_state['retrieved_chunks'])} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.2 Summarizer Node ---\n",
    "state_with_chunks = {**initial_state, \"retrieved_chunks\": docs}\n",
    "summarized_state = summarize(state_with_chunks)\n",
    "print(f\"Summaries: {summarized_state['summaries'][:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.3 Visual Analyzer Node ---\n",
    "visual_state = analyze_figures(state_with_chunks)\n",
    "print(f\"Figure Insights: {visual_state['figure_insights'][:1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.4 Synthesizer Node ---\n",
    "synth_state = synthesize({\n",
    "    \"query\": \"scaling laws\",\n",
    "    \"summaries\": summarized_state['summaries'],\n",
    "    \"figure_insights\": visual_state['figure_insights']\n",
    "})\n",
    "print(f\"Synthesis: {synth_state['synthesis'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2.5 Fact Checker Node ---\n",
    "fact_state = fact_check({\n",
    "    \"synthesis\": synth_state['synthesis'],\n",
    "    \"retrieved_chunks\": docs\n",
    "})\n",
    "print(f\"Verified: {fact_state['verified']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Full Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.1 Run Full Pipeline ---\n",
    "query = \"What are the latest advancements in LLM scaling laws?\"\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "result = app.invoke({\"query\": query, \"messages\": []}, config)\n",
    "\n",
    "print(\"Final State:\")\n",
    "for key, value in result.items():\n",
    "    if key == \"messages\":\n",
    "        print(f\"{key}: {[m.content for m in value]}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value[:100]}...\" if isinstance(value, str) else f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LangSmith Tracing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4.1 Enable LangSmith (set env vars first) ---\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"your-langsmith-key\"\n",
    "\n",
    "# Rerun with tracing\n",
    "result_with_trace = app.invoke({\"query\": query, \"messages\": []}, config)\n",
    "print(\"Check traces at https://smith.langchain.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(app.get_graph().draw_png())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
