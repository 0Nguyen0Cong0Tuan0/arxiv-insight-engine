{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412fa7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: d:\\Learn\\AIE Project\\arxiv-insight-engine\n",
      "src/ exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root():\n",
    "    path = Path.cwd()\n",
    "    while path != path.parent:\n",
    "        if (path / \"src\").exists():\n",
    "            return path\n",
    "        path = path.parent\n",
    "    return Path.cwd()  # fallback\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"src/ exists: {(PROJECT_ROOT / 'src').exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "761df887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.ingest.pipeline:Starting pipeline...\n",
      "INFO:httpx:HTTP Request: GET http://localhost:6333/collections/arxiv_multimodal/exists \"HTTP/1.1 200 OK\"\n",
      "INFO:arxiv:Requesting page (first: True, try: 0): https://export.arxiv.org/api/query?search_query=LLM+agents&id_list=&sortBy=submittedDate&sortOrder=descending&start=0&max_results=100\n",
      "INFO:arxiv:Got first page: 100 of 94990 total results\n",
      "WARNING:src.ingest.pipeline:No chunks were extracted from the PDFs. Nothing to upsert.\n",
      "INFO:src.ingest.pipeline:Pipeline complete!\n"
     ]
    }
   ],
   "source": [
    "from src.ingest.pipeline import run_pipeline\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83ba72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pikepdf._core:pikepdf C++ to Python logger bridge initialized\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "INFO:unstructured_inference:Reading PDF for file: ..\\..\\data\\raw_papers\\2510.26790v1.pdf ...\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "WARNING:pdfminer.pdfinterp:Cannot set gray non-stroke color because /'H1' is an invalid float value\n",
      "INFO:unstructured_inference:Loading the Table agent ...\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "INFO:unstructured_inference:Loading the table structure model ...\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "INFO:timm.models._hub:[timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (fc.weight, fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "Device set to use cpu\n",
      "c:\\Users\\nguye\\anaconda3\\envs\\arxiv-insight\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_0', type=<ChunkType.TEXT: 'text'>, content='5', summary='Gistify is a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow and the ability to produce potentially large code patches.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_1', type=<ChunkType.TEXT: 'text'>, content='2025', summary='Large language models (LLMs) are increasingly being used in code-related tasks. Yet, the evaluation toolkit for assessing such capabilities has lagged behind. Evidence shows that widely-adopted benchmarks such as SWE-bench do not require full reasoning over the whole execution.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_2', type=<ChunkType.TEXT: 'text'>, content='2', summary='Gistify was inspired by a common practice of how developers navigate and understand unfamiliar repositories. Gistify formalizes this practice by requiring an (agentic) coding model to extract the gist of a given command. It produces a single, self-contained, minimal, and executable gistified file.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_3', type=<ChunkType.TEXT: 'text'>, content='0', summary='Gistify is a way to measure codebase-level understanding. Given a codebase and a command of entrypoint, the goal is to generate a minimal, self-contained gistified code file.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_4', type=<ChunkType.TEXT: 'text'>, content='2', summary='Gistify provides insight into the ability of models to reason at the codebase level. It requires only the repository and an entrypoint, without issued logs or pull requests. This allows automatic construction of challenging tasks for any arbitrary repositories.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_6', type=<ChunkType.TEXT: 'text'>, content='5', summary='When given a codebase and a command as input, the coding agent must generate a single gistified file that reproduces the runtime behavior of the original codebase under the given command. Gistify must satisfy the following requirements.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_7', type=<ChunkType.TEXT: 'text'>, content='2', summary='All models are prompted to generate a gistified file for the entrypoint. We can programmatically verify whether the expected behavior is preserved when the ground-truth test is run. Here, we focus on comparing outputs of test commands.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_8', type=<ChunkType.TEXT: 'text'>, content=':', summary='Execution Fidelity is a binary metric where 1 means the gistified file runs successfully and produces the same output as the original codebase. Line Execution Rate measures minimality by calculating the fraction of lines that are actually executed under the given command.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_9', type=<ChunkType.TEXT: 'text'>, content='v', summary='Line Existence Rate measures the proportion of code in the gistified file that is directly preserved from the original codebase. A 100% existence rate indicates full fidelity to theOriginal codebase without hallucination.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_10', type=<ChunkType.TEXT: 'text'>, content='arXiv', summary='We conduct experiments using three widely adopted open-sourced frameworks. SWE-Agent and GitHub Copilot provide a rich scaffolding to LLM-based agents. We also experiment with Mini-SWE- Agent, a lightweight framework where LLMs only have access to a bash terminal to solve the task.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_11', type=<ChunkType.TEXT: 'text'>, content='i', summary='SWE- Bench is an open-source, cross-platform test bench. It is based on SWE-bench and the GitHub repositories pylint, flask, scikit-learn, seaborn, and debug-gym. We evaluate over 25 tests for each of the 5 repositories.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_12', type=<ChunkType.TEXT: 'text'>, content='X', summary='We report results for our main evaluation protocol, where the model does not have access to execution tools. Examples of gistified files are in Appendix B. Among the models evaluated, Claude-4 tends to perform best; however, performance drops sharply on the hard subsets.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_13', type=<ChunkType.TEXT: 'text'>, content='r', summary='GPT-5-mini and Claude-3.7 models are strong bash users. GPT models achieve higher Line Existence, whereas Claude models achieve high Line Execution.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_14', type=<ChunkType.TEXT: 'text'>, content='a', summary='Execution tools are not a silver bullet. We observed a sharp decrease in performance for the GPT-5 model when evaluated on SWE-Agent without execution tools. A detailled discussion can be found in Appendix B.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_15', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a frog with a leaf in its mouth', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_16', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='Table 2 shows that each model tends to fail for different reasons. Error cases are categorized into four groups. See Appendix B.4 for detailed examples of each error case.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_17', type=<ChunkType.TEXT: 'text'>, content='Hyunji Lee∗1, Minseon Kim2, Chinmay Singh2, Matheus Pereira2,', summary='Missing Test Function errors occur when the generated gistified file does not contain the function implemen- tation for the test specified in the given command. This can happen when the model strips out the content of the test and executes it outside of the pytest wrapper. The results indicate this is the most common cause of error for the best performing model, Claude-4.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_18', type=<ChunkType.TEXT: 'text'>, content='Atharv Sonwane3, Isadora White4, Elias Stengel-Eskin5, Mohit Bansal1, Zhengyan Shi2,', summary='We observe that models frequently modify the test function, despite being provided with explicit instructions to copy without modification. To measure such modifications, we define the Test F1 Score as the line-level overlap between the test code of the original file and the gistified version. High Test F 1 Score indicates that the model has successfully identified and copied the correct test function.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_19', type=<ChunkType.TEXT: 'text'>, content='Alessandro Sordoni2, Marc-Alexandre Côté2, Xingdi Yuan2, Lucas Caccia∗2', summary='In this section, we analyze how different strategies and tools affect performance on the Gistify task. For all experiments, we evaluate 50 test instances drawn from the pylint codebase, a setting where the model generally exhibited modest performance. We use SWE-Agent paired with Claude-Sonnet-4.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_20', type=<ChunkType.TEXT: 'text'>, content='∗Equal contribution 1University of North Carolina at Chapel Hill 2Microsoft Research 3Cornell University 4University of California San Diego 5University of Texas at Austin', summary='In this section, we analyze how different strategies and sources of information affect model performance. We first begin with the simplest approach: modifying the prompt to provide explicit task guidance. We then move to more explicit approaches that rely on additional tools. Detailed descriptions of prompts and tools, along with examples, are provided in Appendix C.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_21', type=<ChunkType.TEXT: 'text'>, content='hyunjil@cs.unc.edu debug-gym@microsoft.com', summary='Execution-Based Tools In Section 3.2, we saw that enabling execution tools resulted in small but consistent gains overall. In this section, we examine whether having unrestricted access to a bash terminal is really necessary to observe these gains, or whether simply having access to execution logs of the generated file is enough. The results are surprising: Having access to fewer tools actually increases performance.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_22', type=<ChunkType.TEXT: 'text'>, content='https://microsoft.github.io/debug-gym', summary='In this section, we investigate what properties make a given test hard to Gistify. We hypothesize that tests generating a longer and more complex execution trace would entail a harder task for the coding LLM. We investigate how two axes to measure a runtime execution’s difficulty affect performance.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_23', type=<ChunkType.TEXT: 'text'>, content='As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.', summary='We use the same configuration as prior analysis as Claude-4 with 50 tests sampled from the pylint codebase. In Figure 2a, we see a clear correlation between the difficulty of a given Gistify task, and how complex the execution traces are, according to both metrics considered. On this subset, performance drops to 21%, as compared to 43%, the baseline weighted performance average following the', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_24', type=<ChunkType.TEXT: 'text'>, content='1 Introduction', summary='In this section, we experiment over how models perform in a static setup. As such static coding LLMs do not have tools, they cannot search or view files dynamically. As they cannot iterate over multiple steps, they have to output everything at once. This suggests that selecting files dynamically over multiple iterations is more effective.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_25', type=<ChunkType.TEXT: 'text'>, content='Large language models (LLMs) are increasingly being used in code-related tasks, powering applications in debugging (Yuan et al., 2025) and agentic code generation (Yang et al., 2024; Liang et al., 2025). Thus, the ability to handle isolated snippets and reasoning across entire codebases, including complex file and module relationships, is becoming increasingly essential. Yet, the evaluation toolkit for assessing such capabilities has lagged behind. Recent evidence shows that widely-adopted repository-level benchmarks such as SWE-bench (Jimenez et al., 2024) and RepoBench (Liu et al., 2023b) still do not require full reasoning over the whole execution and could be solved through heuristic shortcuts or retrieval of localized patches (Aleithan et al., 2024; Liang et al., 2025). Moreover, because many of these datasets rely on GitHub issues or pull requests for construction, they are not easily generalizable to arbitrary repositories. At the same time, coding agents are increasingly deployed in large, real-world codebases, highlighting the need for automatically constructed, broadly applicable, and more challenging repository-level evaluation.', summary='Previous work has introduced a variety of benchmarks to evaluate LLMs on codebase-level code understanding. These generally fall into three categories: question answering, code synthesis, and mapping natural language specifications to the entire codebase. Our work tackles a more complex setting, where models must reason over full execution traces and examine multiple files.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_26', type=<ChunkType.TEXT: 'text'>, content='To fill this gap, we introduce the Gistify task, which is deliberately inspired by a common practice of how developers navigate and understand unfamiliar repositories. Rather than reading files in isolation, they start from a concrete execution point such as test command or entry script often mentioned in READMEs. Then, they iteratively reason over the runtime behavior such as identifying dependencies, following control paths to uncover the codebase’s structure and functionality. Gistify formalizes this practice by requiring an (agentic) coding model to extract the gist of a given command, i.e. to generate a single, self-contained, minimal, and executable gistified file that faithfully reproduces the runtime behavior of a given command as when using the original full codebase (Figure 1). In addition to serving as a challenging coding task, such gistified repositories might give human coders a better understanding of a specific functionality of a given codebase, or even a way to export the single functionality of interest without inheriting heavy dependencies.', summary='There are also works that isolates (or sandboxes) functionalities from the codebase, to simplify dependencies while preserving executability. This sandboxing step is similar to Gistify in that it tries to construct a simplified file that has the feature extracted. However, the sandboxing is done programmatically: relevant code snippets are extracted by leveraging the (static) call graph. In contrast, our work focuses on generating a simplifiedfile using an LLM, thereby', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_27', type=<ChunkType.TEXT: 'text'>, content='To perform well in Gistify, an agent should generate a single gistified file that satisfies four key requirements: it should be self-contained, including all necessary components from the codebase so that it can be executed independently; it should ensure execution fidelity, producing the same outputs as the original codebase under the given command; it should satisfy minimality, retaining only the essential code required for execution without redundant or extraneous lines; and it should guarantee faithful preservation, avoiding hallucinated or fabricated code and relying solely on content from the original codebase. To assess model performance, we introduce evaluation metrics that align with these requirements, providing a systematic', summary='Recent work on autonomous agents for codebase-level code understanding has focused on improving code navigation, reasoning, and generation. Approaches leverage structural information of code for function-call graphs, module-dependency graphs, and hierarchical code structures. Another line of work integrate multi-step reasoning and state update policies to enable more effective planning.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_28', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='microsoft windows logo', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_29', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='Various works have introduced benchmarks to evaluate LLMs’ ability to reason over code execution at runtime. In this work, we extend prior approaches by going beyond reasoning over execution traces to also reformulate programs. We further show that this capability serves as a useful tool at inference time, helping models better structure and complete execution-driven tasks.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_30', type=<ChunkType.TEXT: 'text'>, content='compact.py', summary='In this paper, we introduced the Gistify task in which a coding LLM extracts a specific funtionality of a codebase into a single, self-contained file. The file could be leveraged in other downstream tasks, such as code refactoring or debugging.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_31', type=<ChunkType.TEXT: 'text'>, content='from http.cookies import Morsel', summary='Coding LLMs increasingly being deployed in real-world software development. State-of-the-art LLMs still face challenges on the Gistify task, especially when faced with long, complex execution traces.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_32', type=<ChunkType.TEXT: 'text'>, content='test_requests. py', summary='Anthropic. Claude sonnet 3.7. Hybrid reasoning model. Swe-bench+: Enhanced coding benchmark for llms. Ai pair programming in your terminal.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_33', type=<ChunkType.TEXT: 'text'>, content='from requests.compact import Morsel from adapters import HTTPAdapter', summary='Do swe-agents solve multi-file issues like humans? a deep dive into swe- bench verified, January 2025. Corecodebench: A configurable multi-scenario repository-level benchmark. CRUXEval: A benchmark for code reasoning, understanding and execution.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_34', type=<ChunkType.TEXT: 'text'>, content='class TestMorsel: morsel = Morsel()', summary='Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve real-world GitHub issues? Infibench: Evaluating the question-answering capabilities of code large language models.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_35', type=<ChunkType.TEXT: 'text'>, content='def test_cookie(): s=TestMorsel() smount(}iTTPAdapter(0, 0))', summary='Codecgraph is an open-source framework for repository-level code documentation generation. The project is based on the llm-powered Repoagent project.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_36', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a red arrow pointing up in the air', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_37', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a red object with a red ribbon', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_38', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a red letter t on a white background', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_39', type=<ChunkType.TEXT: 'text'>, content='auth.py', summary='Gittaskbench: A benchmark for code agents solving real-world tasks through code repository leveraging. Microsoft. Github copilot in vs code. 2025. URL https://code.visualstudio.com/docs/copilot/ overview.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_40', type=<ChunkType.TEXT: 'text'>, content='istified_file. G be from http.cookies import Morsel def _basic_auth(username): class BaseAdapter: . class HTTPAdapter(BaseAdapter): class HTTPAdapter(BaseAdapter): def __ init__(self): class TestMorse!: morsel = Morsel() self.auth() SS ra G ist ify def test_cookie(): s=TestMorsel() s.mount(\"http://\", HTTPAdapter(0, 0))', summary='Aims to evaluate large language models and agents for machine learning tasks on repository-level code. Openhands: An open platform for ai software developers as generalist agents.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_41', type=<ChunkType.TEXT: 'text'>, content='def _basic_auth(username):', summary='Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1–12, 2024.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_42', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='there is a blue object with a blue arrow on it', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_43', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='there is a blue toothbrush sitting on a counter top', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_44', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a blue arrow on a white background', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_45', type=<ChunkType.TEXT: 'text'>, content='-', summary='Execution fidelity measures whether the generated gistified file reproduces the same functional behavior as the original codebase under the given command. A rate of 100% indicates that the file is concise and contains primarily necessary lines that are executed.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_46', type=<ChunkType.TEXT: 'text'>, content='adapters. py from auth import _basic_auth', summary='The line existence rate measures the proportion of lines in the gistified file that are directly preserved from the original codebase. We classify each line of code into three categories: executable, potentially executable, and non-executable.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_47', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a cell phone with a green arrow pointing to the bottom', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_48', type=<ChunkType.TEXT: 'text'>, content='BaseAdapter: auth(self): er iaend)', summary='Code that is functionally identical may differ in formatting. To address this, we normalize each code block before performing line-wise matching. These normalizations ensure robustness by making the comparison focus on the code’s underlying structure and functionality rather than superficial formatting differences.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_49', type=<ChunkType.TEXT: 'text'>, content='Command', summary='We evaluate experiments with three agentic frameworks: mini-SWE-Agent, SWE- Agent, and Copilot. All experiments are run in the default Gistify setup, where the model is restricted from executing any commands (e.g., python, pytest)', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_50', type=<ChunkType.TEXT: 'text'>, content='pytest test_requests.py::test_cookie', summary='For each repository, we extract all available test cases, including parameterized ones. For experimental test runs, we group tests3 that share the same base structure but differ only in parameterization. In the main experiments, we used 25 test instances for each of the five codebases.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_51', type=<ChunkType.TEXT: 'text'>, content='Figure 1: The Gistify task: given a codebase and a command of entrypoint, the goal is to generate a', summary='We observe that when specific command-line parameters are provided, models often adapt the generated gistified file to those parameters. We group test cases based on the parameters provided to the command. See Appendix A.5 for more details.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_52', type=<ChunkType.TEXT: 'text'>, content='minimal, self-contained gistified code file that faithfully reproduces the original runtime behavior using code from the given codebase.', summary='Produce one file only: ‘{working dir}/concise.py’ (no extra files, no analysis, no commentary) Inline internal dependencies. Copy every function, class, or top-level code that is executed when running {problem statement}. Do not use ‘import’ statements for modules defined in {working dir}.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_53', type=<ChunkType.TEXT: 'text'>, content='way to measure codebase-level understanding. Gistify requires agents to follow the execution path through the codebase without bypassing modules, i.e., understanding how relevant objects are modified along the way, and identifying which classes or functions can be simplified or removed. Since even moderately sized codebases exceed the context window of current LLMs, success also requires effective search capabilities.', summary='We categorize errors into four types:Import Error Figure 8 shows an example of Import Error. This occurs when the model incorrectly imports the original repository (e.g., import requests) instead of inlining the required modules into the gistified file.File Creation Failure This error arises when theModel fails to generate the gistify file. This can happen in two ways: (1) the model exceeds the maximum step limit or (2) theModel completes within the time limit.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_54', type=<ChunkType.TEXT: 'text'>, content='The advantages that Gistify brings are multiple: first, it provides direct insight into the ability of models to reason at the codebase level with an understanding of runtime execution, rather than on isolated code snippets. Second, it is lightweight and broadly applicable: it requires only the repository and an entrypoint, without issued logs or pull requests. This allows automatic construction of challenging tasks for any arbitrary repositories, including private ones. Finally, gistified files themselves are valuable outputs: by compressing a specific feature of a large codebase into a minimal file, they can be applied to various downstream tasks, including automated debugging or error localization.', summary='Missing Test Function occurs when the generated gistified file does not contain the modules for specified test in the given command. Conceptually, this corresponds to a 0% line existence rate for the test function. We also observe an interesting behavior of GPT-5 where it tends to insert __name__ even though it is not provided in the original codebase.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_55', type=<ChunkType.TEXT: 'text'>, content='We conduct experiments across a variety of frameworks (mini-SWE-agent, SWE-agent, and Copilot) and models (GPT-5-mini, GPT-5, Claude-3.7-Sonnet, and Claude-Sonnet-4) and uncover several interesting findings. First, even widely used, high-performing frameworks and models struggle to create a successful gistified file, especially when execution traces are long and have high coverage on the repositories. Second, faithfully reproducing the test function in the generated file is a strong indicator of gistified performance, as it serves as the starting step for reasoning about execution traces. Third, enabling execution tools yields small but consistent performance gains, and additionally providing global code context and runtime information further boosts performance. Finally, agentic models benefit from dynamically deciding what to read and refine their reasoning through multi-step trajectories, outperforming static approaches.', summary='# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses /gpl -2.0.0/gpl.html# For details, please visit: http://pylint.com/Pylint/blob/main/LICENSE.py. # Copyright (c) 2018 Pylint -dev/ pylint-dev.py is a fork of the Pyleint project.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_0', type=<ChunkType.TEXT: 'text'>, content='2 Gistify', summary='Figure 6: Example of a successful gistified file. The code correctly handles all parameters, achieving 100% line–existence rate, a test F1 score of 100, and an execution rate of 65.5%.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_1', type=<ChunkType.TEXT: 'text'>, content='2.1 Task Definition', summary='This error refers to failures that occur during pytest execution, such as syntax errors or fixture-related issues. We explicitly separate those cases by first verifying the presence of the required test functions and running pytest only when they exist.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_2', type=<ChunkType.TEXT: 'text'>, content='As shown in Figure 1, when given a codebase and a command as input, the coding agent must generate a single gistified file that reproduces the runtime behavior of the original codebase under the given command. Specifically, the gistified file must satisfy the following requirements.', summary='import os from contextlib import contextmanager from pathlib import Path def discover_package_path(path, _search_paths): if isinstance(path , Path): p = path else: p = Path(path) if p.exists() and p.is_dir(): return p raise ValueError(\"Cannot\\xa0determine\\xa0package\\xa0path\") @contextmanager def augmented_sys_path(\\'project/tests\\', \\'data\\', \\'proJECT_ROOT_DIR\\') def test_discover_ package_path_source_root_as_parent(parent): parent = pth.parent new.append(str(parent), str(parent) new.path[:] = new yield finally: sys.path [:] = original @pytest.mark.parametrize( \"py_mod_base_name\", (\"__init__\"), ids=(\"explicit -namespace\"), )', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_3', type=<ChunkType.TEXT: 'text'>, content='Self-Contained: All necessary components from the given codebase must be included so that the gistified file can be executed standalone, i.e. without relying on the codebase. The model must identify all relevant modules and dependencies, demonstrating understanding of inter-file relationships.', summary='Table 5 shows the list of available tools in Github Copilot. B.3 Tools Available in GitHub Copilot: Gistify! Codebase-Level Understanding via Runtime Execution via Pytest.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_4', type=<ChunkType.TEXT: 'text'>, content='Execution Fidelity: Executing the gistified file must replicate the original codebase’s runtime behavior, ensuring the model captures the dynamic execution, not just static code patterns.', summary='GPT-5 shows a notably higher Average Pytest Pass Rate, indicating that among the ones they successfully generate, they tend to pass all pytest. For the Test F1 Score, Claude-4 shows the highest performance.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_6', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='We experiment with two variants of the prompt, Reading and Tracing. RepoGraph (Ouyang et al., 2024) is a plug-in module designed to help LLMs leverage the codebase-level structure. Tracing constructs a graph where each node represents a line of code.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_7', type=<ChunkType.TEXT: 'text'>, content='Minimalism: Only the code essential to reproducing the runtime behavior should be preserved, with unused functions and objects pruned. This requires fine-grained understanding of the code to identify which lines are actually executed and essential for the task.', summary=' copilot_getNotebook returns the list of Notebook cells with id, types, line ranges, language, execution info, and output mime types. edit_notebook_file Edit an existing Notebook file in the workspace. apply_patch Edit text files using a special diff/patch format. run_ notebook_cell Run a code cell in a notebook file and return the output.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_8', type=<ChunkType.TEXT: 'text'>, content='Grounded Preservation: No hallucinated code may be introduced. All content must be derived directly from the original codebase. This ensures the task evaluates the model’s understanding of the codebase, rather than its ability to generate arbitrary code that happens to satisfy the command.', summary='Read the contents of a file. open_simple_browser Preview or open a URL in VS Code’s Simple Browser. fetch_webpage Fetch main content from a webpage for summarization or analysis. think Think deeply about a request and log structured reasoning (no execution)', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_9', type=<ChunkType.TEXT: 'text'>, content='2.2 Evaluation Protocol', summary='Table 6: Average Pytest Pass Rate and Test F1 Score of different models using SWE-Agent. Table 5: Available tools and their descriptions. We note that many tools available to the agent are never used.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_10', type=<ChunkType.TEXT: 'text'>, content='There are two inputs to a Gistify task: i) a docker image containing the target codebase, for consistent evaluation; ii) an entrypoint, such as a pytest command on one of the tests in the codebase. Test cases are existing entrypoints one can easily leverage, but broadly, any command that the user would want to use to run a functionality of the existing codebase is allowed.', summary='The test TestGetNetrcAuth.test_works is converted from a pytest unit test into a standalone script. The Bash tool is a basic utility that allows the model to invoke any necessary Bash commands. The Edit and Execute tool is designed specifically for working with the gistified file.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_11', type=<ChunkType.TEXT: 'text'>, content='All models are prompted to generate a gistified file for the entrypoint. We can programmatically verify whether the expected behavior is preserved when the ground-truth test is run within this setup. Here, we focus on comparing outputs of test commands. Once the model generates the gistified file, to ensure that execution for evaluation is based on the original test, we integrate the test code from the original codebase to the gistified file and execute it. This ensures that the model does not cheat by modifying the test.', summary='Figure 17 shows an example of the behavior observed when adding the execution tool. Common patterns we observe are: (1) the model first runs the provided command to identify which files are accessed; and (2) the test runs the command to see which files have been accessed.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_0', type=<ChunkType.TEXT: 'text'>, content='2.3 Metrics', summary='Table 7: Analysis of tool usage during the Gistify task. Avg. tool usage view search execute GPT-5-mini 10.8 71.9 9.8 1.7 16.6 G PT-5 18.5 72.4 8.3 3.3 16.1 17.3 67.5 10.1 4.5 17.9 Claude-Sonnet-4 19.3 74.6 2.1 11.8 11.5 2.7.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_1', type=<ChunkType.TEXT: 'text'>, content='Once a gistified file is generated, we evaluate it using the given execution command. The evaluation considers three dimensions, aligned with the task requirements, to provide a comprehensive measure of a model’s ability to reason over an entire codebase and understand its execution behavior. See Appendix A.1 for more details.', summary='Table 7 shows the statistics on tool usage across models using SWE-bench. We group various tools into four categories: view, search, execute, and other. For all models, we compute usage rates both with and without execution enabled.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_2', type=<ChunkType.TEXT: 'text'>, content='Execution Fidelity is a binary metric where 1 means the gistified file runs successfully and produces the same output as the original codebase when executed under the given command; otherwise, it is 0. Failures include cases where the file is not runnable or yields different outputs. The comparison checks for tests pass/fail consistency and stdout/stderr matching.', summary='The ‘pytest’ command for the test currently being debugged in the given repo. The ‘repograph’ function searches in the mentioned repository with a specific function or class, and returns the def and ref relations.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_3', type=<ChunkType.TEXT: 'text'>, content='Formally, let c denote the given command, C a given codebase, and G a gistified file. Define runs(c,C) as an indicator of whether c executes without crashing when running over C, and out(c,C) returns the set of outputs and error traces from running c with C. Then, execution fidelity is defined as', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_4', type=<ChunkType.TEXT: 'text'>, content='1[runs(c, G) A (out(e, G) = out(e,C))], (1)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_5', type=<ChunkType.TEXT: 'text'>, content='where 1[·] is the indicator function.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_6', type=<ChunkType.TEXT: 'text'>, content='Line Execution Rate measures minimality by calculating the fraction of lines in the gistified file that are actually executed under the given command. A 100% execution rate means all lines are essential, indicating a focused and concise file. This metric is only computed for files that run successfully, since the execution trace is required to determine which lines are run.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_7', type=<ChunkType.TEXT: 'text'>, content='Formally, let Lexec(G) be a list of executable lines (i.e., no comments) in G. Then, the Line Execution rate is defined as', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_8', type=<ChunkType.TEXT: 'text'>, content='1', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_9', type=<ChunkType.TEXT: 'text'>, content='|Lexec(G)| X ℓ∈Lexec(G) 1[ℓ is executed]. (2)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_10', type=<ChunkType.TEXT: 'text'>, content='Line Existence Rate measures the proportion of code in the gistified file that is directly preserved from the original codebase. Specifically, lines of code are grouped into blocks (classes, functions, or top-level units), and matches are computed block by block while respecting the code hierarchy. This helps avoiding false matches from common lines appearing in unrelated parts of the codebase. To ensure robustness, we normalize across common variations such as indentation, multi-line statements, and imports. A 100% existence rate indicates full fidelity to the original codebase without hallucination.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_12', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_13', type=<ChunkType.TEXT: 'text'>, content='Formally, let BG and BC be the sets of blocks in the gistified file and the original codebase, respectively. For a block b, let L(b) represent its set of lines. Then, the existence rate is defined as', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_14', type=<ChunkType.TEXT: 'text'>, content='P 1 b∈BG |L(b)| X b∈BG X ℓ∈L(b) 1{ℓ ∈ LC(b)}, (3)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_15', type=<ChunkType.TEXT: 'text'>, content='where 1{ℓ ∈ LC(b)} = 0, if no matching block exists in BC.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_0', type=<ChunkType.TEXT: 'text'>, content='3 Experiments', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_1', type=<ChunkType.TEXT: 'text'>, content='3.1 Setting', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_2', type=<ChunkType.TEXT: 'text'>, content='We conduct experiments using three widely adopted open-sourced frameworks. SWE-Agent (Yang et al., 2024) and GitHub Copilot (Microsoft, 2025) provide a rich scaffolding to LLM-based agents, enabling them to autonomously perform software engineering tasks. This includes a set of tools for creating and editing code files, navigating repositories, and executing tests. These frameworks also offer the LLM controllable cache management, and LLMs follow the standard tool-calling format. We also experiment with Mini-SWE- Agent (Yang et al., 2024), a lightweight framework where LLMs only have access to a bash terminal to solve the task. Commands are parsed from the agent output and executed directly. As the task objective is for the model to use reasoning over the execution flow rather than the ability of tool usage, for the agentic models, we exclude the execution tools (“python”, “pytest”) in the default setting where execution is disabled.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_3', type=<ChunkType.TEXT: 'text'>, content='Our evaluation spans four leading LLM variants: GPT-5 (OpenAI, 2025a), GPT-5-mini (OpenAI, 2025b), Claude-3.7-Sonnet (Anthropic, 2025a), and Claude-Sonnet-4 (Anthropic, 2025b), offering different cost / performance tradeoffs. For ease or reading, we will refer to the last two models as Claude-3.7 and Claude-4. We use a 128K token limit for all models. All experiments run are capped at 50 steps, after which whatever is generated at this moment in the gistifed file is submitted for evaluation.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_4', type=<ChunkType.TEXT: 'text'>, content='On the data side, we experiment with widely used GitHub repositories which are present in SWE- Bench (requests, pylint, flask, scikit-learn, seaborn). We also explore an additional repository, debug-gym (Yuan et al., 2025)1. This library is relatively new and importantly does not overlap with SWE-Bench. We extract and filter test sets for each repository. Namely, we remove tests whose execution is dependent on the test’s file location. For the main experiment, we evaluate over 25 tests for each of the 5 repositories. More details regarding the evaluation setup and prompt can be found in the Appendix A.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_0', type=<ChunkType.TEXT: 'text'>, content='3.2 Results', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_1', type=<ChunkType.TEXT: 'text'>, content='We begin by giving an overview of the main results presented in Table 1. We report results for our main evaluation protocol, where the model does not have access to execution tools (e.g. “python” and “pytest” commands), as well as the alternative. Examples of gistified files are in Appendix B.1.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_2', type=<ChunkType.TEXT: 'text'>, content='Strong models and frameworks still struggle with Gistify task. Across models and execution frameworks, performance remains limited: even the strongest model with strong framework (Copilot with Claude-4) achieves 58.7% average Execution Fidelity, a binary success/fail metric, indicating that reliably producing a correct gistified file is still challenging. Among the models evaluated, Claude-4 tends to perform best; however, performance drops sharply on the hard subsets (Section 4.2), suggesting that the benchmark can scale in difficulty and will remain a meaningful target as future models strengthen and require more challenging evaluations.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_3', type=<ChunkType.TEXT: 'text'>, content='Different model families exhibit distinct strengths. Claude-4 achieves the highest Line Existence scores, indicating that it most faithfully extracts relevant code from the original codebase. In contrast, GPT-5 produces the most concise outputs, with a substantially higher Line Execution rate than other models. We', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_4', type=<ChunkType.TEXT: 'text'>, content='1We provide a link to all the GitHub repositories used in this work in Table 4.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_6', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_7', type=<ChunkType.TEXT: 'text'>, content='Table 1: Average Performance over three agentic frameworks with four models. We evaluated over 25 tests over 5 repositories. Execution Fidelity is shown as w/o exec, and w execution tools. Line Existence and Execution are average across the two settings for clarity.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_8', type=<ChunkType.TABLE: 'table'>, content='Framework Model Execution Fidelity Line Existence Line Execution (wo exec / w. exec) GPT-5-mini 17.1 / 24.0 44.9 61.2 mini-SWE-agent GPT-5 Claude-3.7 51.0 / 54.0 38.7 / 43.3 56.8 66.0 83.1 69.2 Claude-4 54.0 / 55.3 67.0 75.7 GPT-5-mini 30.9 / 45.3 47.9 74.8 SWE-agent GPT-5 Claude-3.7 30.7 / 46.0 40.7 / 46.0 48.3 66.8 81.7 69.9 Claude-4 56.7 / 57.3 66.3 72.9 GPT-5-mini 58.0 / 55.3 62.4 77.8 Copilot GPT-5 Claude-3.7 58.7 / 60.7 43.3 / 56.0 66.9 63.0 81.4 74.4 Claude-4 58.7 / 61.3 69.6 80.3', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Framework</th><th>Model</th><th>(wo</th><th>Execution Fidelity exec / w. exec)</th><th>Line Existence</th><th>Line</th><th>Execution</th></tr></thead><tbody><tr><td rowspan=\"4\">a mini SWE-agent</td><td>GPT-5-mini</td><td></td><td>17.1 / 24.0</td><td>44.9</td><td></td><td>61.2</td></tr><tr><td>GPT-5</td><td></td><td>51.0 / 54.0</td><td>56.8</td><td></td><td>83.1</td></tr><tr><td>Oy, de-3.7</td><td></td><td>38.7 / 43.3</td><td>66.0</td><td></td><td>69.2</td></tr><tr><td>Claude-4</td><td></td><td>54.0 / 55.3</td><td>67.0</td><td></td><td>75.7</td></tr><tr><td rowspan=\"4\">SWE. SWE-agent</td><td>GPT-5-mini</td><td></td><td>30.9 / 45.3</td><td>47.9</td><td></td><td>74.8</td></tr><tr><td>GPT-5</td><td></td><td>30.7 / 46.0</td><td>48.3</td><td></td><td>81.7</td></tr><tr><td>Claude-3.7</td><td></td><td>40.7 / 46.0</td><td>66.8</td><td></td><td></td></tr><tr><td>Claude-4</td><td></td><td>56.7 / 57.3</td><td>66.3</td><td></td><td></td></tr><tr><td rowspan=\"4\">Copilot “opie</td><td>GPT-5-mini</td><td></td><td>58.0 / 55.3</td><td>62.4</td><td></td><td>77.8</td></tr><tr><td>GPT-5</td><td></td><td>58.7 / 60.7</td><td>66.9</td><td></td><td>81.4</td></tr><tr><td>Claude-3.7</td><td></td><td>43.3 / 56.0</td><td>63.0</td><td></td><td>TAA</td></tr><tr><td>Claude-4</td><td></td><td>58.7 / 61.3</td><td>69.6</td><td></td><td></td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_9', type=<ChunkType.TEXT: 'text'>, content='observe a similar trend for GPT-5-mini and Claude-3.7: in general, GPT models achieve higher Line Existence, whereas Claude models achieve higher Line Execution.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_10', type=<ChunkType.TEXT: 'text'>, content='Small(er) models perform well with scaffolding. We note that GPT-5-mini’s performance varies significantly across different evaluation settings, from 17% in a bash-only setup to 58% when provided with a large inventory of tools from the Copilot framework (see Appendix B.3 for a full list). We note that this performance increase is also reflected in the quality of the generated gist, where we see a notable increase in line existence and line execution.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_11', type=<ChunkType.TEXT: 'text'>, content='Frontier models (GPT-5 / Claude-4) are strong bash users. When looking at performance on mini-swe-agent, where the models only have access to a bash terminal to solve the task, both models perform relatively well, solving over half of the tasks. Importantly, this is not the case for smaller and previous-generation models.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_12', type=<ChunkType.TEXT: 'text'>, content='Execution tools are not a silver bullet. Overall, when comparing performance with and without execution in Table 1, we note that in most cases we observe only a small performance gain. We expected that current coding LLMs could better leverage execution tools: indeed, using tools specifically for runtime execution analysis, such as a debugger, could significantly help solving a gistify task. However, we are not seeing this behavior emerge, even from frontier models. We observed a sharp decrease in performance for the GPT-5 model when evaluated on SWE-Agent without execution tools. We performed a visual inspection and noticed formatting issues when rewriting the input test function. A detailled discussion can be found in Appendix B.2.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_0', type=<ChunkType.TEXT: 'text'>, content='3.3 Error Analysis Over Execution Failure', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_1', type=<ChunkType.TEXT: 'text'>, content='We proceed with an analysis of the underlying failure causes, in order to understand which aspect of the Gistify task different models struggle with. Table 2 shows that each model tends to fail for different reasons. See Appendix B.4 for detailed examples of each error case.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_2', type=<ChunkType.TEXT: 'text'>, content='Import Error occurs when the model incorrectly imports the original codebase (e.g., import requests) instead of inlining the required modules into the gistified file. We note that this error occurs even as coding LLMs are explicitly prompted not to import the specific packages in question. Perhaps surprisingly, the best performing model, Claude-4, commits this seemingly innocuous error the most out of all four models.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_3', type=<ChunkType.TEXT: 'text'>, content='File Creation Failure errors arise when the model fails to generate the gistified file. This can happen in two ways: the model exceeds the maximum step limit, or the model terminates the task without any file being generated.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_6', type=<ChunkType.TEXT: 'text'>, content='Table 2: Average error rates (%) of different failure reasons when running SWE-agent across models. Error cases are categorized into four groups. The numbers in parentheses indicate the number of errors for each category.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_7', type=<ChunkType.TABLE: 'table'>, content='Models Import Error File Creation Failure Missing Test Function Pytest Runtime Error GPT-5-mini 2.1 (2) 11.3 (11) 76.3 (72) 10.3 (10) GPT-5 5.2 (4) 10.4 (8) 77.9 (60) 6.5 (5) Claude-Sonnet-3.7 20.0 (10) 20.0 (10) 2.0 (1) 58.0 (29) Claude-Sonnet-4 32.5 (13) 10.0 (4) 7.5 (3) 50.0 (20)', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Models</th><th>Import Error</th><th>File Creation Failure</th><th>Test Function</th><th>Pytest Runtime Error</th></tr></thead><tbody><tr><td>GPT-5-mini</td><td>2.1 (2)</td><td>11.3 (11)</td><td></td><td>10.3 (10)</td></tr><tr><td>GPT-5</td><td>5.2 (4)</td><td>10.4 (8)</td><td></td><td>6.5 (5)</td></tr><tr><td>Claude-Sonnet-3.7</td><td>20.0 (10)</td><td>20.0 (10)</td><td></td><td>58.0 (29)</td></tr><tr><td>Claude-Sonnet-4</td><td>32.5 (13)</td><td>10.0 (4)</td><td></td><td>50.0 (20)</td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_8', type=<ChunkType.TEXT: 'text'>, content='Missing Test Function errors occur when the generated gistified file does not contain the function implemen- tation for the test specified in the given command, or implements the test in a different structure. This can happen when the model strips out the content of the test and executes it outside of the pytest wrapper, under e.g. if __name__ == __main__:. Claude models tend to avoid this mistake, while this is the main source of error for GPT-5 models, specifically under the SWE-agent framework. Importantly, we observe that this error does not happen at random, but rather alongside other execution errors; we attempted to add the missing test function, and it in most cases the test fails to run, i.e. it results in a runtime error. This aligns with the analysis in the next section, showing a strong correlation between the task’s success and the fidelity between the original and the generated tests.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_9', type=<ChunkType.TEXT: 'text'>, content='Pytest Runtime Error occurs when the execution of the generated file fails, either due to a runtime error or because the gistified output does not match the output from the original codebase. The results indicate this is the most common cause of error for the best performing model, Claude-4.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_0', type=<ChunkType.TEXT: 'text'>, content='3.4 Importance of Faithfully Preserving the Test Function', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_1', type=<ChunkType.TEXT: 'text'>, content='We observe that models frequently modify the test function, despite being provided with explicit instructions to copy without modification, except for unavoidable adjustments (e.g., removing imports). Again, to ensure consistent evaluation, we replace the test function in the gistified file with the original version before evaluation.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_2', type=<ChunkType.TEXT: 'text'>, content='To measure such modifications, we define the Test F1 Score as the line-level overlap between the test code of the original file and the gistified version. High Test F1 Score indicates that the model has successfully identified and copied the correct test function to the gistified file. We observe a strong correlation between Test F1 Score and execution fidelity (correlation=0.76, p=0.01); test instances with higher F1 scores are substantially more likely to produce a successful gistified file. We hypothesize that this arises because in the Gistify task, models often reason backwards from the test file, thereby if the model fails from identifying or copying the test function, the subsequent reasoning process is highly likely to fail.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_3', type=<ChunkType.TEXT: 'text'>, content='To better understand the impact of the first step—searching, viewing, and copying the test function—we conduct an ablation study where we remove potential failure at this stage. Specifically, we explicitly provide the correct test function body and signature in the prompt, so the model no longer needs to locate or copy it. This isolates the effect of errors in identifying the test function. In this setting, we observe that Test F1 Score improves highly from the base Gistify 68.4 to 85.3, along with execution fidelity (from 42.0% to 60.0%). This suggests that accurately handling the test function is a critical first step to do the Gistify task successfully. Detailed results are in Appendix B.5.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_4', type=<ChunkType.TEXT: 'text'>, content='4 Analysis', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_5', type=<ChunkType.TEXT: 'text'>, content='In this section, we analyze how different strategies and tools affect performance on the Gistify task, identify factors that contribute to its difficulty, and experiment with the use of a static coding LLM to gain a deeper understanding of the task. For all experiments, we evaluate 50 test instances drawn from the pylint codebase, a setting where the model generally exhibited modest performance. We use SWE-Agent paired with Claude-Sonnet-4.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_7', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_8', type=<ChunkType.TEXT: 'text'>, content='Table 3: Analysis of the effect of different strategies and tools (global information, execution) on the Gistify', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_9', type=<ChunkType.TEXT: 'text'>, content='task. We evaluate SWE-Agent with Claude 4 using 50 test instances from the pylint codebase. Max Steps Reached (%) indicates the percentage of runs that terminated because the maximum step limit was reached.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_10', type=<ChunkType.TABLE: 'table'>, content='Ablation Type Execution Fidelity Line Existence Base Gistify 42.0 65.0 58.3 14.6 Prompted Strategies Tracing Reading 48.0 50.0 75.4 77.6 62.8 62.6 0.0 3.9 Global Info (Tool) RepoGraph Tracing 52.0 56.0 76.1 75.1 60.1 65.1 6.0 0.0 Execution (Tool) Bash Edit And Execute 52.0 56.0 73.1 74.3 64.2 64.2 16.0 10.0', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Ablation</th><th>Type</th><th>| Execution Fidelity</th><th>Line</th><th>Existence</th><th>Line Execution</th><th>| Max Steps Reached (%</th></tr></thead><tbody><tr><td>Base</td><td>GISTIFY</td><td>42.0</td><td>65.0</td><td></td><td>58.3</td><td>| 14.6</td></tr><tr><td>Prompted Strateries</td><td>ewan</td><td>| 00</td><td>6</td><td></td><td>ao</td><td>| 38</td></tr><tr><td>cioatnw cay</td><td>Mpeaph</td><td>| 8</td><td></td><td></td><td>mm</td><td></td></tr><tr><td rowspan=\"2\">Execution (Tool)</td><td>And Execute</td><td>60</td><td>ts</td><td></td><td>1</td><td>| 100</td></tr><tr><td>Edit</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_11', type=<ChunkType.TEXT: 'text'>, content='Line Execution Max Steps Reached (%)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_0', type=<ChunkType.TEXT: 'text'>, content='4.1 Effect of Various Strategies and Tools', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_1', type=<ChunkType.TEXT: 'text'>, content='In this section, we analyze how different strategies and sources of information affect model performance. We begin with the simplest approach, modifying the prompt to guide the model (Prompt-Based Guidance), and then move to more explicit approaches that rely on additional tools: providing global context (Global Information via Tools) or feedback from code execution (Execution-Based Tools). Detailed descriptions of prompts and tools, along with examples, are provided in the Appendix C.1.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_2', type=<ChunkType.TEXT: 'text'>, content='Prompt-Based Guidance We first begin with the simplest approach: modifying the prompt to provide explicit task guidance. We experiment in two settings. In the former, we prompt the model to perform step-by-step reasoning, by first predicting the execution traces and then going over them, adding relevant code snippets along the way (tracing). In the latter, a similar approach is used, with explicit instructions on how to recursively determine the execution traces: starting from the test, identify the relevant components and read the files where they are defined, and repeat until the end (reading). As shown in Table 3, we observe that adding such strategies tends to enhance overall metrics, giving both better execution fidelity and more faithful code extractions, as measured by line existence.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_3', type=<ChunkType.TEXT: 'text'>, content='Global Information via Tools Building on the above observation, we next assess the effect of explicitly providing global context through external tools, rather than predicting it. We examine two tools: (1) RepoGraph (Ouyang et al., 2024), which constructs a graph of the codebase where each node represents a line of code and edges capture connections between lines, enabling graph-based search over the entire codebase; and (2) a Tracing tool that exposes gold execution traces obtained from running the given test command. Results in Table 3 show that both tools improve performance, with the Tracing tool yielding the largest gains. This finding suggests that access to the global context, especially the gold tracing information, substantially strengthens the model’s ability to perform runtime reasoning, as it can easily identify which file to look at.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_4', type=<ChunkType.TEXT: 'text'>, content='Execution-Based Tools In Section 3.2, we saw that enabling execution tools resulted in small but consistent gains overall. In this section, we examine whether having unrestricted access to a bash terminal is really necessary to observe these gains, or whether simply having access to execution logs of the generated file is enough. For this experiment we compare Bash access with a simple method that executes and prints the output of the gistified file whenever it is edited (Edit And Execute). No other execution tools are available to the agent, including runtime information about the ground truth test. The results are surprising: having access to fewer tools actually increases performance. Indeed, we note that when give access to a full set of bash commands, the coding LLM tends to explore more tools, increasing the overall trajectory length, and potentially reaching the maximum step limit.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_0', type=<ChunkType.TEXT: 'text'>, content='4.2 Tests with High Coverage are Harder to Gistify', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_1', type=<ChunkType.TEXT: 'text'>, content='In this section, we investigate what properties makes a given test hard to Gistify. We hypothesize that tests generating a longer and more complex execution trace would entail a harder task for the coding LLM. To this end, we investigate how two axes to measure a runtime execution’s difficulty affect performance: the length of trace, as measured by the number of function calls executed, and the number of unique files touched by', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_3', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_5', type=<ChunkType.TEXT: 'text'>, content='Performance according to Exec. Trace Difficulty', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_6', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a graph showing the number of files covered by a file', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_7', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a chart of the number of different types of lines in a bar chart', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_8', type=<ChunkType.TEXT: 'text'>, content='(a) Difficulty of the Gistify task is measured as a function of the execution trace difficulty of the underlying test.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_9', type=<ChunkType.TEXT: 'text'>, content='(b) Performance of a static coding LLM and various agentic coding LLMs (mini-SWE-Agent, SWE-Agnet, Copilot).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_10', type=<ChunkType.TEXT: 'text'>, content='the tracing procedure. While these metrics correlate with one another, they will differ when, for example, a function is looped over many times or when the location of the relevant functions is in a single file versus across multiple files.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_11', type=<ChunkType.TEXT: 'text'>, content='For this experiment, we use again the same configuration as prior analysis, namely Claude-4 with 50 tests sampled from the pylint codebase. In Figure 2a, we see a clear correlation between the difficulty of a given Gistify task, and how complex the execution traces are, according to both metrics considered. We leverage this insight to create a Gistify-hard subset, where we select the 30 most difficult examples according to each. We end up with 57 unique datapoints (30 from pylint, 28 from sklearn, 6 from seaborn). On this subset, performance drops to 21%, as compared to 43%, the baseline weighted performance average following the same distribution over repositories. Overall, this selection criteria offers a promising direction for designing challenging evaluation scenarios with Gistify.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_0', type=<ChunkType.TEXT: 'text'>, content='4.3 Static Coding LLM', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_1', type=<ChunkType.TEXT: 'text'>, content='In this section, we experiment over how models perform in a static setup, where they have no access to tools and cannot iterate on the generated solution. As such static coding LLMs do not have tools, they cannot search or view files dynamically. Thereby, to measure a possible upper bound for non-agentic approaches, we provide as input all files that were accessed during the original program execution (gold files). Also, as they cannot iterate over multiple steps, they have to output everything at once and are therefore restricted by the context window of the LLM. Since solving the Gistify task involves touching multiple files, we observe in many cases that the inputs exceed the model’s maximum sequence length. Thus, we sample a subset of test examples where the combined content fits within the 128K token limit of the LLM. As shown in Figure 2b, agentic models outperform static ones even when the latter receive all relevant files. This suggests that selecting files dynamically over multiple iterations is more effective than providing everything at once, which can overwhelm the model2. However, interestingly, the static coding LLM setup achieves the highest Line Existence score. This is likely because the model can copy lines directly from input, yet it performs worse on Line Execution and Execution Fidelity, suggesting that models do not have a good understanding of the codebase, often copying lines that are incomplete or incorrect.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_4', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_6', type=<ChunkType.TEXT: 'text'>, content='5 Related Works', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_7', type=<ChunkType.TEXT: 'text'>, content='5.1 Codebase-level Understanding Benchmark', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_8', type=<ChunkType.TEXT: 'text'>, content='Previous work has introduced a variety of benchmarks to evaluate LLMs on codebase-level code understanding. These generally fall into three categories: question answering, code synthesis, and mapping natural language specifications to the entire codebase. Several benchmarks introduce codebase-level question-answering (Strich et al., 2024; Li et al., 2024b; Sahu et al., 2024; Chen et al., 2025; Hu et al., 2024; Fu et al., 2025). In these settings, the model must correctly answer questions that require an understanding of the codebase. The questions are drawn from various sources, including real-world GitHub issues and queries resembling those asked of tools like Copilot. Another line of work evaluates whether models can synthesize code by leveraging information distributed across multiple files in the codebase (Zhang et al., 2023; Liu et al., 2023b; Ding et al., 2023; Li et al., 2024a; Yu et al., 2024). These benchmarks include tasks such as retrieval-augmented completion, cross-file refactoring, and more specialized settings such as sketch-based coding or codebase evolution. Moreover, there is a line of benchmark that maps natural language specifications to entire code repositories, leveraging hierarchical or multi-stage representations to capture inter-file relationships and maintain consistency across a codebase (Tang et al., 2023; Zan et al., 2024; Ni et al., 2025). Our work tackles a more complex setting, where models must reason over full execution traces and examine multiple files, making the task challenging, and even widely used agentic models struggle alongside static ones.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_9', type=<ChunkType.TEXT: 'text'>, content='There are also works that isolates (or sandboxes) functionalities from the codebase, to simplify dependencies while preserving executability (Xie et al., 2025b; Jain et al., 2024). This sandboxing step is similar to Gistify in that it tries to construct a simplified file that has the feature extracted. However, the sandboxing step is done programmatically: relevant code snippets are extracted by leveraging the (static) call graph. In contrast, our work focuses on generating a simplified file using an LLM, thereby evaluating the model’s ability to reason about both code dependencies and runtime behavior. Notably, while prior work acknowledges cases in which static programmatic sandboxing fails (e.g., when functions have large dependency slices) and discards those examples, we consider them informative because they require reasoning about more complex runtime behavior. We further observe that these instances also present challenging examples for the Gistify task.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_0', type=<ChunkType.TEXT: 'text'>, content='5.2 Methods for Codebase-Level Understanding', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_1', type=<ChunkType.TEXT: 'text'>, content='Recent work on autonomous agents for codebase-level code understanding has focused on improving code navigation, reasoning, and generation through structured representations and planning. Approaches leverage structural information of code for function-call graphs, module-dependency graphs, and hierarchical code structures to provide models with core components of repositories (Wang et al., 2025; Liu et al., 2024). Another line of work integrate multi-step reasoning and state update policies to enable more effective planning over complex tasks (Bairi et al., 2024; Gautam et al., 2025). Additional methods combine various agents with multiple tools to streamline codebase-level exploration and task solving (Luo et al., 2024; Zhang et al., 2023; Shrivastava et al., 2023; Wang et al., 2024; Yang et al., 2024; Tang et al., 2023; aider, 2025; Microsoft, 2025; cursor, 2025).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_2', type=<ChunkType.TEXT: 'text'>, content='5.3 Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_3', type=<ChunkType.TEXT: 'text'>, content='Various works have introduced benchmarks to evaluate LLMs’ ability to reason over code execution at runtime (Gu et al., 2024; Chen et al., 2024; Xie et al., 2025a; Beger & Dutta, 2025; Hu et al., 2025). These benchmarks typically test whether models can predict execution traces or intermediate states such as variable values, control flow, or data dependencies—given code and inputs, or alternatively, infer inputs from code and outputs. Some benchmarks further extend this paradigm by leveraging execution traces to construct new problems through program composition, thereby varying complexity in a principled way. Beyond evaluation, execution traces have also been incorporated into training pipelines to strengthen models’ runtime reasoning abilities (Liu et al., 2023a; Ding et al., 2024). By augmenting pre-training and fine-tuning with execution states, paths, and coverage signals, these methods help models capture program dynamics and generalize to execution-aware tasks. At inference time, several frameworks leverage runtime feedback to iteratively guide models in debugging or completing partial programs, thereby improving performance on execution-driven', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_7', type=<ChunkType.TEXT: 'text'>, content='tasks (Zhong et al., 2024; Xue et al., 2024). In this work, we extend prior approaches by going beyond reasoning over execution traces to also reformulate programs; the model not only tracks execution but also identifies how to compress and organize code into a concise, coherent file. We further show that this capability serves as a useful tool at inference time, helping models better structure and complete execution-driven tasks.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_11_0', type=<ChunkType.TEXT: 'text'>, content='6 Discussion and Conclusion', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_11_1', type=<ChunkType.TEXT: 'text'>, content='In this paper, we introduced the Gistify task in which a coding LLM extracts a specific funtionality of a codebase into a single, self-contained file. Beyond serving as a standalone evaluation task that is easily applicable to arbitrary repositories and execution commands, the gistified file itself also opens several promising directions for research and practical applications. Large codebases often overwhelm automated agents due to their complex dependencies, and they especially struggle when tasked with fixing bugs that span multiple files (Ganhotra, 2025). In such scenarios, a gistified file would greatly reduce this challenge, and enable a more efficient reasoning about the codebase without navigating through unrelated code. In other words, this file could be leveraged in other downstream tasks, such as code refactoring or debugging, or even as a way to extract and share a minimal implementation of a specific codebase functionality.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_11_2', type=<ChunkType.TEXT: 'text'>, content='In summary, with coding LLMs increasingly being deployed in real-world software development, the need for automatically constructing evaluation setups that require codebase-level understanding of arbitrary repositories is growing. Through extensive experiments across a range of models and frameworks, we found that state-of-the-art LLMs still face challenges on the Gistify task, especially when faced with long, complex execution traces. Our analysis shows that incorporating global code context or execution-aware tools improves performance, and agentic coding LLM tend to handle the task more effectively by reasoning about which files to inspect using various tools. Beyond serving as a benchmark, the gistified files themselves are valuable artifacts. They distill the essential functionality of complex systems into a compact, executable form, making them easier to inspect and understand. Such files could support a range of practical applications, including debugging, refactoring, and code review, which we leave for future work.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_11_4', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_0', type=<ChunkType.TEXT: 'text'>, content='References', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_1', type=<ChunkType.TEXT: 'text'>, content='aider. Ai pair programming in your terminal. 2025. URL https://github.com/Aider-AI/aider?tab= readme-ov-file.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_2', type=<ChunkType.TEXT: 'text'>, content='Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. Swe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_3', type=<ChunkType.TEXT: 'text'>, content='Anthropic. Claude sonnet 3.7. https://www.anthropic.com/news/claude-3-7-sonnet, 2025a. Hybrid reasoning model; accessed: 2025-09-25.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_4', type=<ChunkType.TEXT: 'text'>, content='Anthropic. Claude sonnet 4. https://www.anthropic.com/claude/sonnet, 2025b. Improved version over Sonnet 3.7; accessed: 2025-09-25.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_5', type=<ChunkType.TEXT: 'text'>, content='Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Balasubramanyan Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675–698, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_6', type=<ChunkType.TEXT: 'text'>, content='Claas Beger and Saikat Dutta. Coconut: Structural code understanding does not fall out of a tree. In 2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pp. 128–136. IEEE, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_7', type=<ChunkType.TEXT: 'text'>, content='Jialiang Chen, Kaifa Zhao, Jie Liu, Chao Peng, Jierui Liu, Hang Zhu, Pengfei Gao, Ping Yang, and Shuiguang Deng. Coreqa: uncovering potentials of language models in code repository question answering. arXiv preprint arXiv:2501.03447, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_8', type=<ChunkType.TEXT: 'text'>, content='Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, and Xin Xia. Reasoning runtime behavior of a program with llm: How far are we? arXiv preprint arXiv:2403.16437, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_9', type=<ChunkType.TEXT: 'text'>, content='cursor. cursor. 2025. URL https://cursor.com/.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_10', type=<ChunkType.TEXT: 'text'>, content='Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36:46701–46723, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_11', type=<ChunkType.TEXT: 'text'>, content='Yangruibo Ding, Benjamin Steenhoek, Kexin Pei, Gail Kaiser, Wei Le, and Baishakhi Ray. Traced: Execution- aware pre-training for source code. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1–12, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_12', type=<ChunkType.TEXT: 'text'>, content='Lingyue Fu, Hao Guan, Bolun Zhang, Haowei Yuan, Yaoming Zhu, Jun Xu, Zongyu Wang, Lin Qiu, Xunliang Cai, Xuezhi Cao, et al. Corecodebench: A configurable multi-scenario repository-level benchmark. arXiv preprint arXiv:2507.05281, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_13', type=<ChunkType.TEXT: 'text'>, content='Jatin Ganhotra. Do swe-agents solve multi-file issues like humans? a deep dive into swe- bench verified, January 2025. URL https://jatinganhotra.dev/blog/swe-agents/2025/01/05/ swe-bench-mutliple-files/. Blog post.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_14', type=<ChunkType.TEXT: 'text'>, content='Dhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, and Roshanak Zilouchian Moghaddam. Refac- torbench: Evaluating stateful reasoning in language agents through code. arXiv preprint arXiv:2503.07832, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_15', type=<ChunkType.TEXT: 'text'>, content='Alex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CRUXEval: A benchmark for code reasoning, understanding and execution. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 16568–16621. PMLR, 21–27 Jul 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_16', type=<ChunkType.TEXT: 'text'>, content='Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, and Cuiyun Gao. Coderepoqa: A large-scale benchmark for software engineering question answering. arXiv preprint arXiv:2412.14764, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_18', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_20', type=<ChunkType.TEXT: 'text'>, content='Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, and Kaidi Xu. Dynacode: A dynamic complexity-aware code benchmark for evaluating large language models in code generation. arXiv preprint arXiv:2503.10452, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_21', type=<ChunkType.TEXT: 'text'>, content='Naman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. R2e: Turning any github repository into a programming agent environment. In ICML, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_22', type=<ChunkType.TEXT: 'text'>, content='Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_23', type=<ChunkType.TEXT: 'text'>, content='Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_24', type=<ChunkType.TEXT: 'text'>, content='Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. arXiv preprint arXiv:2404.00599, 2024a.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_25', type=<ChunkType.TEXT: 'text'>, content='Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, and Hongxia Yang. Infibench: Evaluating the question-answering capabilities of code large language models. Advances in Neural Information Processing Systems, 37:128668–128698, 2024b.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_26', type=<ChunkType.TEXT: 'text'>, content='Shanchao Liang, Spandan Garg, and Roshanak Zilouchian Moghaddam. The swe-bench illusion: When state-of-the-art llms remember instead of reason. arXiv preprint arXiv:2506.12286, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_27', type=<ChunkType.TEXT: 'text'>, content='Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, and Nan Duan. Code execution with pre-trained language models. In Anna Rogers, Jordan Boyd- Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 4984–4999, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.308. URL https://aclanthology.org/2023.findings-acl.308/.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_28', type=<ChunkType.TEXT: 'text'>, content='Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto- completion systems. arXiv preprint arXiv:2306.03091, 2023b.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_29', type=<ChunkType.TEXT: 'text'>, content='Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, and Wenmeng Zhou. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv preprint arXiv:2408.03910, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_30', type=<ChunkType.TEXT: 'text'>, content='Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. Repoagent: An llm-powered open-source framework for repository-level code documentation generation. arXiv preprint arXiv:2402.16667, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_0', type=<ChunkType.TEXT: 'text'>, content='Microsoft. Github copilot in vs code. 2025. URL https://code.visualstudio.com/docs/copilot/ overview.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_1', type=<ChunkType.TEXT: 'text'>, content='Ziyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun, Hongzhang Liu, et al. Gittaskbench: A benchmark for code agents solving real-world tasks through code repository leveraging. arXiv preprint arXiv:2508.18993, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_2', type=<ChunkType.TEXT: 'text'>, content='OpenAI. Gpt-5 technical overview. https://platform.openai.com/docs, 2025a. Accessed: 2025-09-25.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_3', type=<ChunkType.TEXT: 'text'>, content='OpenAI. Gpt-5 mini. https://platform.openai.com/docs/models/gpt-5-mini, 2025b. Compact variant of GPT-5; accessed: 2025-09-25.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_4', type=<ChunkType.TEXT: 'text'>, content='Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph. arXiv preprint arXiv:2410.14684, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_5', type=<ChunkType.TEXT: 'text'>, content='Surya Prakash Sahu, Madhurima Mandal, Shikhar Bharadwaj, Aditya Kanade, Petros Maniatis, and Shirish Shevade. Codequeries: A dataset of semantic queries over code. In Proceedings of the 17th Innovations in Software Engineering Conference, pp. 1–11, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_7', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_9', type=<ChunkType.TEXT: 'text'>, content='Disha Shrivastava, Denis Kocetkov, Harm De Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion: Training code models to understand your repository. arXiv preprint arXiv:2306.10998, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_10', type=<ChunkType.TEXT: 'text'>, content='Jan Strich, Florian Schneider, Irina Nikishina, and Chris Biemann. On improving repository-level code QA for large language models. In Xiyan Fu and Eve Fleisig (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 209–244, Bangkok, Thailand, August 2024. Association for Computational Linguistics. ISBN 979-8-89176-097-4. doi: 10.18653/v1/2024.acl-srw.28.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_11', type=<ChunkType.TEXT: 'text'>, content='Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, et al. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. arXiv preprint arXiv:2311.09835, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_12', type=<ChunkType.TEXT: 'text'>, content='Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du, et al. Repomaster: Autonomous exploration and understanding of github repositories for complex task solving. arXiv preprint arXiv:2505.21577, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_13', type=<ChunkType.TEXT: 'text'>, content='Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_14', type=<ChunkType.TEXT: 'text'>, content='Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, and Xiangyu Zhang. Core: Benchmarking llms code reasoning capabilities through static analysis tasks. arXiv preprint arXiv:2507.05269, 2025a.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_15', type=<ChunkType.TEXT: 'text'>, content='Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, and Carolyn Rose. Repost: Scalable repository-level coding environment construction with sandbox testing. Conference on Language Modeling, 2025b.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_16', type=<ChunkType.TEXT: 'text'>, content='Zhipeng Xue, Zhipeng Gao, Shaohua Wang, Xing Hu, Xin Xia, and Shanping Li. Selfpico: Self-guided partial code execution with llms. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 1389–1401, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_17', type=<ChunkType.TEXT: 'text'>, content='John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik R Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_18', type=<ChunkType.TEXT: 'text'>, content='Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1–12, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_19', type=<ChunkType.TEXT: 'text'>, content='Xingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, Lucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, et al. debug-gym: A text-based environment for interactive debugging. arXiv preprint arXiv:2503.21557, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_20', type=<ChunkType.TEXT: 'text'>, content='Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, et al. Codes: Natural language to code repository via multi-layer sketch. arXiv preprint arXiv:2403.16443, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_21', type=<ChunkType.TEXT: 'text'>, content='Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_22', type=<ChunkType.TEXT: 'text'>, content='Li Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger via verifying runtime execution step-by-step. arXiv preprint arXiv:2402.16906, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_24', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_0', type=<ChunkType.TEXT: 'text'>, content='A Experimental Setting', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_1', type=<ChunkType.TEXT: 'text'>, content='A.1 Metrics', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_2', type=<ChunkType.TEXT: 'text'>, content='Execution Fidelity Execution fidelity measures whether the generated gistified file reproduces the same functional behavior as the original codebase under the given command. This includes producing the same number of test passes or failures, as well as consistent outputs and error handling. If the file’s behavior matches the original codebase, it is assigned 100%; otherwise, it receives 0%.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_3', type=<ChunkType.TEXT: 'text'>, content='Line Execution Rate The line execution rate measures the proportion of lines in the gistified file that are actually executed when running it under the given command. We first analyze the gistified file to identify which lines are executable (e.g., imports, function or class definitions) versus not-executable (e.g., comments). Using a tracing function, we then determine which of the executable lines are touched during execution. The line execution rate is computed as the fraction of executable lines that are executed. A rate of 100% indicates that the gistified file is concise and contains primarily necessary lines that are executed, while 0% indicates that non of the executable lines were touched. When calculating line execution rate, we exclude the tests where the self-containment is 0% as the goal of line execution rate is to evaluate the model’s ability to construct a concise, executable file, not to penalize failures in generating runnable code.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_4', type=<ChunkType.TEXT: 'text'>, content='We classify each line of code into three categories: executable, potentially executable, and non-executable. Executable lines include imports and functional code that can be directly run. Potentially executable lines are those that may or may not be executed during a run, such as the except block of a try-except statement or placeholders for classes and function definitions. Non-executable lines, such as comments, are those that have no effect on execution. To calculate the line execution rate, we first classify each line in the gistified file and then consider only the executable lines. Non-executable lines are ignored since their presence or absence does not affect execution outcomes, and potentially executable lines are excluded because they are often ambiguous (e.g., placeholders) and cannot be reliably judged as necessary or removable.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_5', type=<ChunkType.TEXT: 'text'>, content='Line Existence Rate The line existence rate measures the proportion of lines in the gistified file that are directly preserved from the original codebase. We first parse both the gistified file and the original codebase into blocks, where each block corresponds to a class or function. Within classes, functions are nested under their parent class, forming a hierarchy. Lines outside of any block (e.g., top-level statements) are treated as standalone units.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_6', type=<ChunkType.TEXT: 'text'>, content='For each block in the gistified file, we locate the corresponding block in the original codebase using its name and hierarchical position. If a matching block exists, we compare the two line by line to determine which lines are preserved; whether the lines in the gistified block appear in the corresponding original block. If no match is found, all lines in that block are treated as non-existent. For lines outside any block, existence is determined by direct comparison with top-level lines in the original codebase.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_7', type=<ChunkType.TEXT: 'text'>, content='An existence rate of 100% indicates perfect preservation of the original code without hallucinated content.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_8', type=<ChunkType.TEXT: 'text'>, content='Normalization for Line-wise Code Matching When checking the existence of a code line within a file, as our objective is to determine semantic equivalence rather than strict syntactical identity, we do normalization; code that is functionally identical may differ in formatting, such as multiline statements, indentations, or space, which can hinder direct line-wise comparison. To address this, we normalize each code block before performing line-wise matching. Specifically, we parse the code into an Abstract Syntax Tree (AST) and ignore comments; split combined import statements into individual imports; merge statements that span multiple lines into a single line; remove inline comments (e.g., for i in range(5): # comment); and eliminate indentation and redundant spaces. These normalizations ensure robustness by making the comparison focus on the code’s underlying structure and functionality rather than superficial formatting differences.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_0', type=<ChunkType.TEXT: 'text'>, content='A.2 Framework', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_1', type=<ChunkType.TEXT: 'text'>, content='We evaluate experiments with three agentic frameworks: mini-SWE-Agent (Yang et al., 2024), SWE- Agent (Yang et al., 2024), and Copilot (Microsoft, 2025). Unless otherwise noted, all experiments are run in the default Gistify setup, where the model is restricted from executing any commands (e.g., python, pytest).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_3', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_4', type=<ChunkType.TEXT: 'text'>, content='SWE-Agent and Copilot Agent enable LLMs to interact with a codebase through a suite of tools, including bash commands. These tools support capabilities such as viewing, searching, editing, and creating files or directories. In addition, Copilot Agent extends this functionality with browser integration, explicit reasoning, and API usage. mini-SWE-agent is a simplified variant of SWE-Agent that only supports bash commands. Despite its minimal design, it achieves strong performance on the SWE-Bench Verified benchmark (Jimenez et al., 2023). For both mini-SWE-Agent and SWE-Agent, we set the maximum number of steps to 50 and run them in the same Docker environment, using the current version of the repositories.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_5', type=<ChunkType.TEXT: 'text'>, content='A.3 Experimental Test Set Construction', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_6', type=<ChunkType.TEXT: 'text'>, content='Table 4: Details of the GitHub repositories used as the test set.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_7', type=<ChunkType.TABLE: 'table'>, content='Repository URL License flask https://github.com/pallets/flask BSD 3-Clause requests https://github.com/psf/requests Apache-2.0 pylint https://github.com/pylint-dev/pylint GPL 2.0 scikit-learn https://github.com/scikit-learn/scikit-learn BSD 3-Clause seaborn https://github.com/mwaskom/seaborn BSD 3-Clause debug-gym https://github.com/microsoft/debug-gym MIT', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Repository</th><th>URL</th><th>License</th></tr></thead><tbody><tr><td>flask</td><td>https: //github.com/pallets/flask</td><td>BSD 3-Clause</td></tr><tr><td>requests</td><td>https: //github.com/psf/requests</td><td>Apache-2.0</td></tr><tr><td>pylint</td><td>https: //github.com/pylint-dev/pylint</td><td>GPL 2.0</td></tr><tr><td>scikit-learn</td><td>| https://github.com/scikit-learn/scikit-learn</td><td>BSD 3-Clause</td></tr><tr><td>seaborn</td><td>https: //github.com/mwaskom/seaborn</td><td>BSD 3-Clause</td></tr><tr><td>debug-gym</td><td>https: //github.com/microsoft/debug-gym</td><td>MIT</td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_8', type=<ChunkType.TEXT: 'text'>, content='Table 4 summarizes the repositories used in our evaluation. For each repository, we begin by extracting all available test cases, including parameterized ones. For experimental test runs, we group tests3 that share the same base structure but differ only in parameterization, treating them as a single test. During evaluation, however, we execute all parameterized instances and measure how many are passed, thereby assessing execution fidelity. Finally, we filter out environment-dependent tests, such as those requiring relative file paths or fixed module locations. In the main experiments, we used 25 test instances for each of the five codebases, and the analysis was conducted using 50 test instances from the pylint codebase.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_0', type=<ChunkType.TEXT: 'text'>, content='A.4 Prompt for Gistify', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_1', type=<ChunkType.TEXT: 'text'>, content='Figure 3 shows the prompt used in the main experiments.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_2', type=<ChunkType.TEXT: 'text'>, content='A.5 Providing specific parameters to commands tends to make models generate parameter-specific gistified files', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_3', type=<ChunkType.TEXT: 'text'>, content='We observe that when specific command-line parameters are provided, models often adapt the generated gistified file to those parameters rather than producing a fully general solution. Examples of this parameter- specific behavior are shown in Figures 4 and 5. Accordingly, in our experiments, we group test cases based on the parameters provided to the command.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_4', type=<ChunkType.TEXT: 'text'>, content='B Results', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_5', type=<ChunkType.TEXT: 'text'>, content='B.1 Example of gistified file', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_6', type=<ChunkType.TEXT: 'text'>, content='Figure 6 and Figure 7 show two gistified files on the same test case with different models; each model succeed or fail in generating a gistified file with execution fidelity of 100% and 0%, respectively. In the successful case (Figure 6), the generated file handles both parameters correctly, achieving a 100% line existence rate, a 65.5% execution rate, and a test F1 score of 100. In contrast, the failed case (Figure 7) cannot execute due to a missing import pytest statement. Moreover, the hallucinated test function yields a test F1 score of 0, and the file shows a much lower line existence rate of 28%.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_7', type=<ChunkType.TEXT: 'text'>, content='3We adopt this grouping design as we observe that models often overfit to specific values when parameters are provided. See Appendix A.5 for more details.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_10', type=<ChunkType.TEXT: 'text'>, content='Prompt for Gistify', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_11', type=<ChunkType.TEXT: 'text'>, content='I’ve uploaded a python code repository in the directory {working dir}.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_12', type=<ChunkType.TEXT: 'text'>, content='There is an original test invocation (the command that reproduces behavior we want to preserve): problem statement Your job: create a single file named ‘concise.py’ saved at {working dir} that is **self-contained**, **minimal**, and **executable**, and when used in place of the original test run reproduces the same runtime behavior and outputs. Follow the instructions below when creating the file.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_13', type=<ChunkType.TEXT: 'text'>, content='OUTPUT - Produce one file only: ‘{working dir}/concise.py’. - The assistant must return only the contents of ‘concise.py’ (no extra files, no analysis, no commentary). HIGH-LEVEL RULES for creating ‘concise.py’ 1. Inline internal dependencies * Copy into ‘concise.py’ every function, class, or top-level code from the files inside {working dir} that is executed when running {problem statement}. * Do not use ‘import’ statements for modules defined in {working dir}. 2. Remove unexecuted lines * When copying lines in ‘concise.py’, keep only the lines that is actually executed when running {problem statement}. * Delete unused functions, classes, variables, if-else, imports, and unreachable branches. * Ensure the file remains syntactically correct and minimal after removal. 3. Preserve original source lines * Do not rewrite or reformat lines unless necessary to keep the files valid. * Do not arbitrary generate new lines that do not exist in the original {working dir} files. * You may adjust indentation, remove empty ‘else’‘ blocks, or adapt ‘try-except’ structures only when required to preserve correctness. 4. Keep external imports * Leave imports to external libraries, frameworks, or standard runtime libraries unchanged. * Only remove or inline dependencies that come from {working dir}. 5. No shortcuts or cheating * Do not stub, fake, or monkey-patch external modules. * Do not reimplement or newly add third-party libraries. * Do not hard-code outputs * Do not replace test logic with simplified equivalents 6. Preserve test behavior * The test function much remain unchanged, except for import adjustments needed to reference inlined code. * The output, exceptions, or exit codes must match the original run of {problem statement}. 7. Do not execute the code * Do not run or simulate the program (e.g., with ‘pytest’, ‘python’, or any other tools)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_14', type=<ChunkType.TEXT: 'text'>, content='Figure 3: Base Prompt Template for Gistify Task.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_0', type=<ChunkType.TEXT: 'text'>, content='B.2 Error analysis over execution failure', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_1', type=<ChunkType.TEXT: 'text'>, content='We categorize errors into four types:', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_2', type=<ChunkType.TEXT: 'text'>, content='Import Error Figure 8 shows an example of Import Error. This occurs when the model incorrectly imports the original repository (e.g., import requests) instead of inlining the required modules into the gistified file.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_3', type=<ChunkType.TEXT: 'text'>, content='File Creation Failure This error arises when the model fails to generate the gistified file. This can happen in two ways: (1) the model exceeds the maximum step limit or (2) the model completes within the time limit but still fails to generate the new file using the tool.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_6', type=<ChunkType.TEXT: 'text'>, content='@pytest.mark.parametrize(', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_7', type=<ChunkType.TEXT: 'text'>, content='\"value ,␣expected\", ( (\"application/xml\", (\"application/xml\", {})), ( \"application/json␣;␣charset=utf -8\", (\"application/json\", {\"charset\": \"utf -8\"}), ), (\"text/plain\", (\"text/plain\", {})), ... def test__parse_content_type_header(value , expected): assert _parse_content_type_header(value) == expected (a) Original Test Case def test__parse_content_type_header(): \"\"\"Test␣for␣the␣_parse_content_type_header␣function␣with␣application/json␣and␣charset=utf -8\"\"\" value = \"application/json␣;␣charset=utf -8\" expected = (\"application/json\", {\"charset\": \"utf -8\"}) assert _parse_content_type_header(value) == expected', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_8', type=<ChunkType.TEXT: 'text'>, content=')', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_9', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_10', type=<ChunkType.TEXT: 'text'>, content='Figure 4: Example of a model generating a parameter-specific gistified file when given a command that includes a parameter.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_11', type=<ChunkType.TEXT: 'text'>, content='@pytest.mark.parametrize(', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_12', type=<ChunkType.TEXT: 'text'>, content='\"url,␣expected\", ( (\"http://192.168.0.1:5000/\", True), ... (\"http://google.com:5000/v1.0/\", False), ), ) def test_should_bypass_proxies_no_proxy(url, expected , monkeypatch): \"\"\"Tests␣for␣function␣should_bypass_proxies␣to␣check␣if␣proxy ␣␣␣␣can␣be␣bypassed␣or␣not␣using␣the␣’no_proxy’␣argument ␣␣␣␣\"\"\" no_proxy = \"192.168.0.0/24 ,127.0.0.1 , localhost.localdomain ,172.16.1.1\" # Test ’no_proxy’ argument assert should_bypass_proxies(url, no_proxy=no_proxy) == expected', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_13', type=<ChunkType.TEXT: 'text'>, content='(a) Original Test Case', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_14', type=<ChunkType.TEXT: 'text'>, content='def test_should_bypass_proxies_no_proxy(url, expected , monkeypatch): \"\"\"Tests␣for␣function␣should_bypass_proxies␣to␣check␣if␣proxy ␣␣␣␣can␣be␣bypassed␣or␣not␣using␣the␣’no_proxy’␣argument ␣␣␣␣\"\"\" no_proxy = \"192.168.0.0/24 ,127.0.0.1 , localhost.localdomain ,172.16.1.1\" # Test ’no_proxy’ argument assert should_bypass_proxies(url, no_proxy=no_proxy) == expected', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_15', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_16', type=<ChunkType.TEXT: 'text'>, content='Figure 5: Example of a model generating a parameter-specific gistified file when given a command that includes a parameter.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_17', type=<ChunkType.TEXT: 'text'>, content='Missing Test Function This occurs when the generated gistified file does not contain the modules for specified test in the given command. It typically arises when the model fails to locate or copy the modules necessary for the test into the gistified file. Conceptually, this corresponds to a 0% line existence rate for the test function. Since the presence of the modules for the given test case is essential for validation, we classify this as an error.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_18', type=<ChunkType.TEXT: 'text'>, content='We also observe an interesting behavior of GPT-5 where it tends to insert __name__ == \"__main__\" even though it is not provided in the original codebase and even though it is explicitly mentioned that we will test on the provided command and expect the same output. They often remove the test function but move the lines in the test function under the \"__main__\" guard (e.g., Figure 10). We hypothesize that this may be because they are more familiar with codebases following this pattern. We also observe cases where the model', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_20', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_21', type=<ChunkType.TEXT: 'text'>, content='# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl -2.0.html', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_22', type=<ChunkType.TEXT: 'text'>, content='# For details: https://github.com/pylint -dev/pylint/blob/main/LICENSE # Copyright (c) https://github.com/pylint -dev/pylint/blob/main/CONTRIBUTORS.txt from __future__ import annotations import os from collections.abc import Sequence from typing import Any import pytest def discover_package_path(modulepath: str, source_roots: Sequence[str]) -> str: \"\"\"Discover␣package␣path␣from␣one␣its␣modules␣and␣source␣roots.\"\"\" dirname = os.path.realpath(os.path.expanduser(modulepath)) if not os.path.isdir(dirname): dirname = os.path.dirname(dirname) # Look for a source root that contains the module directory for source_root in source_roots: source_root = os.path.realpath(os.path.expanduser(source_root)) if os.path.commonpath([source_root , dirname]) in [dirname , source_root]: return source_root # Fall back to legacy discovery by looking for __init__.py upwards as # it’s the only way given that source root was not found or was not provided while True: if not os.path.exists(os.path.join(dirname , \"__init__.py\")): return dirname old_dirname = dirname dirname = os.path.dirname(dirname) if old_dirname == dirname: return os.getcwd() @pytest.mark.parametrize( \"py_mod_base_name\", (\"__init__\", \"impl\"), ids=(\"explicit -namespace\", \"implicit -namespace\"), ) def test_discover_package_path_source_root_as_parent( py_mod_base_name: str, tmp_path: Any, ) -> None: \"\"\"Test␣discover_package_path␣when␣source␣root␣is␣a␣parent␣of␣the␣module.\"\"\" # Create this temporary structure: # /tmp_path/ # project/ # my-package/ # __init__.py project_dir = tmp_path / \"project\" package_dir = project_dir / \"mypackage\" package_dir.mkdir(parents=True) (package_dir / f\"{py_mod_base_name}.py\").touch() # Test with project_dir as source root (parent of package) result = discover_package_path(str(package_dir), [str(project_dir)]) assert result == str(project_dir)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_23', type=<ChunkType.TEXT: 'text'>, content='Figure 6: Example of a successful gistified file: the code correctly handles all parameters, achieving 100% line–existence rate, a test F1 score of 100, and an execution rate of 65.5%.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_24', type=<ChunkType.TEXT: 'text'>, content='attempts to “cheat” the task by injecting a mock, in-memory version of the original codebase package to satisfy import dependencies, rather than copying the necessary code inline (e.g., Figure 12).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_0', type=<ChunkType.TEXT: 'text'>, content='Pytest Runtime Error', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_1', type=<ChunkType.TEXT: 'text'>, content='This error refers to failures that occur during pytest execution, such as syntax errors or fixture-related issues (e.g., Figure 9). Although the absence of test functions is also one of pytest failures, we explicitly separate those cases by first verifying the presence of the required test functions and running pytest only when they exist.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_3', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_5', type=<ChunkType.TEXT: 'text'>, content='import os import sys from contextlib import contextmanager from pathlib import Path def discover_package_path(path , _search_paths): \"\"\" ␣␣␣␣Minimal␣version:␣accept␣a␣filesystem␣path␣(string␣or␣Path)␣and␣return␣a␣Path ␣␣␣␣to␣that␣directory.␣This␣matches␣the␣usage␣in␣the␣specific␣test␣where␣a ␣␣␣␣path␣to␣the␣tests␣data␣directory␣is␣passed. ␣␣␣␣\"\"\" if isinstance(path , Path): p = path else: p = Path(path) if p.exists() and p.is_dir(): return p raise ValueError(\"Cannot␣determine␣package␣path\") @contextmanager def augmented_sys_path(paths): \"\"\" ␣␣␣␣Minimal␣context␣manager:␣set␣sys.path␣to␣the␣parent␣directories␣of␣the ␣␣␣␣provided␣paths␣(in␣order),␣restoring␣the␣original␣sys.path␣on␣exit. ␣␣␣␣\"\"\" original = list(sys.path) try: new = [] for p in paths: pth = Path(p) # Ensure we use the parent directory (the \"source root\" in the test) parent = pth.parent new.append(str(parent)) sys.path[:] = new yield finally: sys.path[:] = original @pytest.mark.parametrize( \"py_mod_base_name\", (\"__init__\"), ids=(\"explicit -namespace\"), ) def test_discover_package_path_source_root_as_parent(): # This test’s behavior is preserved: it uses discover_package_path on the # tests data directory and ensures augmented_sys_path sets sys.path to its # parent (the project/tests directory). TEST_DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"tests\", \"data\")) PROJECT_ROOT_DIR = os.path.abspath(os.path.join(TEST_DATA_DIR , \"..\")) with augmented_sys_path([discover_package_path(TEST_DATA_DIR , [])]): assert sys.path == [PROJECT_ROOT_DIR]', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_6', type=<ChunkType.TEXT: 'text'>, content='Figure 7: Example of failed gistified file: the code fails to import pytest. The model hallucinates the function test_discover_package_path_source_root_as_parent(), resulting in a test F1 score of 0 and a low line–existence rate of 28.0%', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_0', type=<ChunkType.TEXT: 'text'>, content='B.3 Tools Available in GitHub Copilot', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_1', type=<ChunkType.TEXT: 'text'>, content='Table 5 shows the list of available tools in Github Copilot.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_2', type=<ChunkType.TEXT: 'text'>, content='B.4 Change Test', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_3', type=<ChunkType.TEXT: 'text'>, content='even high performing models and frameworks (especially GPT-5 and GPT-5-mini) seems to modify test codes even though explicitly mentioned not to. We observed three common modification: (1) removing the test function but move the lines in the test function under the \"__main__\" guard (e.g., Figure 10), (2) adding the \"__main__\" guard even though unnecessary (e.g., Figure 11), and (3) mocking a minimal in-memory package to bypass missing dependencies and force the test to run (e.g., Figure 12).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_6', type=<ChunkType.TEXT: 'text'>, content='@click.option(\"--all-methods\", is_flag=True , help=\"Show␣HEAD␣and␣OPTIONS␣methods.\") @with_appcontext def routes_command(sort , all_methods): \"\"\"Show␣all␣registered␣routes␣with␣endpoints␣and␣methods.\"\"\" from flask import current_app rules = list(current_app.url_map.iter_rules()) if not rules: click.echo(\"No␣routes␣were␣registered.\") return', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_7', type=<ChunkType.TEXT: 'text'>, content='Figure 8: Example of an Import Error: the gistified file imports from the original repository (e.g., from flask import current_app).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_8', type=<ChunkType.TEXT: 'text'>, content='T = t.TypeVar(\"T\")', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_9', type=<ChunkType.TEXT: 'text'>, content='class ConfigAttribute(t.Generic[T]): \"\"\"Makes␣an␣attribute␣forward␣to␣the␣config\"\"\" def __init__( self , name: str, get_converter: t.Callable[[t.Any], T] | None = None ) -> None: self.__name__ = name self.get_converter = get_converter (a) Original Test Case class ConfigAttribute: def __init__( self , name: str, get_converter: t.Callable[[t.Any], T] | None = None ) -> None: self.__name__ = name', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_10', type=<ChunkType.TEXT: 'text'>, content='self.get_converter = get_converter', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_11', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_12', type=<ChunkType.TEXT: 'text'>, content='Figure 9: Example of an Pytest Runtime Error: gistified file fails with error message E TypeError: ’ConfigAttribute’ is not subscriptable type', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_13', type=<ChunkType.TEXT: 'text'>, content='B.5 Additional Metrics', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_14', type=<ChunkType.TEXT: 'text'>, content='Table 6 shows the result of additional evaluation metrics, including the Average Pytest Pass Rate, which is defined as the average test pass rate over cases with at least one successful run, and the Test F1 Score, which quantifies the line-wise F1 existence between the test functions in the original codebase and those in the gistified fie.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_15', type=<ChunkType.TEXT: 'text'>, content='GPT-5 shows a notably higher Average Pytest Pass Rate, indicating that among the ones they successfully generate, they tend to pass all pytest. For the Test F1 Score, Claude-4 shows the highest performance, aliging with the trend discussed in Section 3.4.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_0', type=<ChunkType.TEXT: 'text'>, content='C Analysis', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_1', type=<ChunkType.TEXT: 'text'>, content='C.1 Effect of various strategies and tools', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_2', type=<ChunkType.TEXT: 'text'>, content='Prompt-Based Guidance We experiment with two variants of the prompt, Reading and Tracing, where, on top of the base prompt (Figure 3), we add specific instructions of How to Operate to encourage reasoning using a particular strategy. The addition prompt detail of Reading is in Figure 14, and for Tracing is in Figure 15.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_3', type=<ChunkType.TEXT: 'text'>, content='Global Information via Tools We experiment with two tools that provide global information: RepoGraph and Tracing. Details of the information provided to the model about each tool are shown in Figure 16.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_4', type=<ChunkType.TEXT: 'text'>, content='RepoGraph (Ouyang et al., 2024) is a plug-in module designed to help LLMs leverage the codebase-level structure. It parses code at the line level, extracts relationships, and constructs a graph where each node represents a line of code and each edge encodes dependencies between code definitions and their references.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_6', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_7', type=<ChunkType.TABLE: 'table'>, content='Tool Description copilot_getNotebookSummary Returns the list of Notebook cells with id, types, line ranges, language, execution info, and output mime types. Useful for getting cell IDs, execution order, and outputs. edit_notebook_file Edit an existing Notebook file in the workspace. Supports inserting, deleting, or editing cells while preserving whitespace and indentation. apply_patch Edit text files using a special diff/patch format. Do not use for Jupyter notebooks. semantic_search Run a natural language search for relevant code or documentation comments in the workspace. create_directory Create a new directory structure in the workspace (like mkdir -p). create_file Create a new file with specified content. Automatically creates directories if they do not exist. file_search Search for files in the workspace by glob pattern (e.g., **/*.js). Returns matching paths only. test_search For a source file, find the corresponding test file, and vice versa. grep_search Fast text or regex search in the workspace. Useful for exact string or regex queries. run_notebook_cell Run a code cell in a notebook file and return the output. Avoid running Markdown cells. read_notebook_cell_output Retrieve the latest output for a notebook cell, even if not run in the current session. get_search_view_results Returns results from the search view. github_repo Search a GitHub repository for relevant code snippets. Use only for external repos, not local workspaces. insert_edit_into_file Insert or edit code in an existing file using minimal hints, avoiding duplication of unchanged code. install_extension Install an extension in VS Code. Used only during workspace creation. list_dir List the contents of a directory (folders and files). create_new_jupyter_notebook Generate a new Jupyter Notebook (.ipynb) in VS Code. create_new_workspace Set up a complete new project (scaffolding, dependencies, config, boilerplate). get_project_setup_info Provides project setup information for a VS Code workspace after workspace creation. read_file Read the contents of a file. Supports offsets and limits for large files. open_simple_browser Preview or open a URL in VS Code’s Simple Browser. test_failure Include test failure information in the prompt. think Think deeply about a request and log structured reasoning (no execution). Useful for planning, debugging, and brainstorming. get_vscode_api Retrieve comprehensive VS Code API documentation and references for exten- sion development. run_vscode_command Run a VS Code command by ID with arguments. Used mainly in workspace creation. fetch_webpage Fetch main content from a webpage for summarization or analysis.', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>copilot__getNotebookSummary</th><th>Returns the list of Notebook cells with id, types, line ranges, language, execution info, and output mime types. Useful for getting cell IDs, execution order, and outputs.</th></tr></thead><tbody><tr><td>edit_ notebook _file</td><td>Edit an existing Notebook file in the workspace. Supports inserting, deleting, or editing cells while preserving whitespace and indentation.</td></tr><tr><td>apply_patch</td><td>Edit text files using a special diff/patch format. notebooks. Do not use for Jupyter</td></tr><tr><td>semantic_search</td><td>Run a natural the workspace. anguage search for relevant code or documentation comments in</td></tr><tr><td>create_ directory</td><td>Create a new directory structure in the workspace (like mkdir -p).</td></tr><tr><td>create_file</td><td>Create a new file with specified content. Automatically creates directories if they do not exist.</td></tr><tr><td>file_ search</td><td>Search for files in the workspace by glob pattern (e.g., **/*.js). Returns matching paths only.</td></tr><tr><td>test__search</td><td>For a source file, find the corresponding test file, and vice versa.</td></tr><tr><td>grep_search</td><td>Fast text or regex search in the workspace. Useful for exact string or regex queries.</td></tr><tr><td>run_notebook_cell</td><td>Run a code cell in a notebook file and return the output. Markdown cells. Avoid running</td></tr><tr><td>read_notebook_cell_ output</td><td>Retrieve the latest output for a notebook cell, even if not run in the current session.</td></tr><tr><td>get_search_view_results</td><td>Returns results from the search view.</td></tr><tr><td>github_repo</td><td>Search a GitHub repository for relevant code snippets. Use only for external repos, not local workspaces.</td></tr><tr><td>insert__edit_into_file</td><td>Insert or edit code in an existing file using minimal hints, avoiding duplication of unchanged code.</td></tr><tr><td>install_ extension</td><td>Install an extension in VS Code. Used only during workspace creation.</td></tr><tr><td>list__dir</td><td>List the contents of a directory (folders and files).</td></tr><tr><td>create_new__jupyter_notebook</td><td>Generate a new Jupyter Notebook (.ipynb) in VS Code.</td></tr><tr><td>create_new__workspace</td><td>Set up a complete new project (scaffolding, dependencies, config, boilerplate).</td></tr><tr><td>get__project_setup_ info</td><td>Provides project setup information for a VS Code workspace after workspace creation.</td></tr><tr><td>read_file</td><td>Read the contents of a file. Supports offsets and limits for large files.</td></tr><tr><td>open_simple_ browser</td><td>Preview or open a URL in VS Code’s Simple Browser.</td></tr><tr><td>test__failure</td><td>Include test failure information in the prompt.</td></tr><tr><td>think</td><td>Think deeply about a request and log structured reasoning (no execution). Useful for planning, debugging, and brainstorming.</td></tr><tr><td>get_vscode_api</td><td>Retrieve comprehensive VS Code API documentation and references for exten- sion development.</td></tr><tr><td>tun_vscode_command</td><td>Run a VS Code command by ID with arguments. Used mainly in workspace creation.</td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_8', type=<ChunkType.TEXT: 'text'>, content='Table 5: Available tools and their descriptions. We note that many tools available to the agent are never used. Table 6: Average Pytest Pass Rate and Test F1 Score of different models using SWE-Agent on the main table (Table 1) test dataset.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_9', type=<ChunkType.TABLE: 'table'>, content='Models Execution Fidelity Average Pytest Pass Rate Test F1 Score GPT-5-mini 30.9 49.2 47.9 GPT-5 30.7 88.8 45.0 Claude-3.7 40.7 61.9 55.9 Claude-4 56.7 72.2 60.0', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Models</th><th>Execution Fidelity</th><th>| Average</th><th>Pytest Pass</th><th>Rate</th><th>Test Fy Scoré</th></tr></thead><tbody><tr><td>GPT-5-mini</td><td>30.9</td><td></td><td>49.2</td><td></td><td></td></tr><tr><td>GPT-5</td><td>30.7</td><td></td><td>88.8</td><td></td><td></td></tr><tr><td>Claude-3.7</td><td>40.7</td><td></td><td>61.9</td><td></td><td></td></tr><tr><td>Claude-4</td><td>56.7</td><td></td><td>72.2</td><td></td><td></td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_11', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_12', type=<ChunkType.TEXT: 'text'>, content='class TestGetNetrcAuth:', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_13', type=<ChunkType.TEXT: 'text'>, content='def test_works(self , tmp_path , monkeypatch): netrc_path = tmp_path / \".netrc\" monkeypatch.setenv(\"NETRC\", str(netrc_path)) with open(netrc_path , \"w\") as f: f.write(\"machine␣example.com␣login␣aaaa␣password␣bbbb\\\\n\") auth = get_netrc_auth(\"http://example.com/thing\") assert auth == (\"aaaa\", \"bbbb\") (a) Original Test Case if __name__ == \"__main__\": # Reproduce tests/test_utils.py::TestGetNetrcAuth::test_works with tempfile.TemporaryDirectory() as tmpdir: netrc_path = os.path.join(tmpdir , \".netrc\") os.environ[\"NETRC\"] = netrc_path with open(netrc_path , \"w\") as f: f.write(\"machine␣example.com␣login␣aaaa␣password␣bbbb\\\\n\") auth = get_netrc_auth(\"http://example.com/thing\")', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_14', type=<ChunkType.TEXT: 'text'>, content='(a) Original Test Case', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_15', type=<ChunkType.TEXT: 'text'>, content='assert auth == (\"aaaa\", \"bbbb\")', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_0', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_1', type=<ChunkType.TEXT: 'text'>, content='Figure 10: Test Modification Case 1: The test TestGetNetrcAuth.test_works is converted from a pytest unit test into a standalone script.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_2', type=<ChunkType.TEXT: 'text'>, content='# Test class and method - preserved unchanged class TestArgparseOptionsProviderMixin: \"\"\"Tests␣for␣the␣argparse␣implementation␣of␣OptionsProviderMixIn. ␣␣␣␣The␣logger␣checker␣is␣used␣as␣an␣example␣checker␣for␣this␣implementation. ␣␣␣␣\"\"\" @staticmethod def test_logger_without_options() -> None: \"\"\"Check␣that␣we␣raise␣messages␣when␣we␣do␣not␣supply␣any␣options.\"\"\" with pytest.raises(SystemExit) as ex: Run([LOGGING_TEST]) assert ex.value.code == 2 # Main execution for pytest if __name__ == \"__main__\": test = TestArgparseOptionsProviderMixin() test.test_logger_without_options()', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_3', type=<ChunkType.TEXT: 'text'>, content='Figure 11: Test Modification Case 2: Adding unnecessary \"__main__\" guard', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_4', type=<ChunkType.TEXT: 'text'>, content='Thereby, when given a specific module, it returns the relationship with other modules as represented within the constructed graph.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_5', type=<ChunkType.TEXT: 'text'>, content='Tracing is a tool that uses the tracer provided from the sys module to execute a command and track which components of the codebase are accessed. When the model uses the tool with a specific command, the tool provides the model with the files and functions touched when running the command, in the order in which they are encountered.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_6', type=<ChunkType.TEXT: 'text'>, content='Execution-Based Tools We experiment with two execution-based tools: the Bash tool and the Edit and Execute tool.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_7', type=<ChunkType.TEXT: 'text'>, content='The Bash tool is a basic utility that allows the model to invoke any necessary Bash commands. In contrast, the Edit and Execute tool is designed specifically for working with the gistified file: it enables the model to create or modify the gistified file and optionally execute it to verify changes.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_8', type=<ChunkType.TEXT: 'text'>, content='The primary difference between the two tools is their scope of execution. The Bash tool can run commands on both the original codebase and the gistified file, whereas the Edit and Execute tool is restricted to executing only the gistified file.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_9', type=<ChunkType.TEXT: 'text'>, content='We include an example of the behavior observed when adding the execution tool in Figure 17. Common patterns we observe are: (1) the model first runs the provided command to identify which files are accessed', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_11', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_12', type=<ChunkType.TEXT: 'text'>, content='# Create a minimal in-memory ’requests’ package with required submodules. requests_mod = types.ModuleType(’requests’) requests_mod.__path__ = [] compat_mod = types.ModuleType(’requests.compat’) structures_mod = types.ModuleType(’requests.structures’) # Populate compat with only what’s needed by this test suite import paths. compat_mod.Mapping = Mapping compat_mod.MutableMapping = MutableMapping compat_mod.urljoin = urljoin # Populate structures with the classes. structures_mod.CaseInsensitiveDict = CaseInsensitiveDict structures_mod.LookupDict = LookupDict # Wire the package hierarchy and register in sys.modules. requests_mod.compat = compat_mod requests_mod.structures = structures_mod sys.modules[’requests’] = requests_mod sys.modules[’requests.compat’] = compat_mod sys.modules[’requests.structures’] = structures_mod if __name__ == ’__main__’: import pytest', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_13', type=<ChunkType.TEXT: 'text'>, content='raise SystemExit(pytest.main([’-q’, ’tests/test_structures.py::TestCaseInsensitiveDict::test_list’]))', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_14', type=<ChunkType.TEXT: 'text'>, content='Figure 12: Test Modification Case 3: Manually mocking a minimal in-memory package to bypass missing dependencies and force the test to run.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_15', type=<ChunkType.TEXT: 'text'>, content='@pytest.mark.parametrize( \"value ,␣expected\", ( (’foo=\"is␣a␣fish\",␣bar=\"as␣well\"’, {\"foo\": \"is␣a␣fish\", \"bar\": \"as␣well\"}), (\"key_without_value\", {\"key_without_value\": None}), ), ) def test_parse_dict_header(value , expected): assert parse_dict_header(value) == expected', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_16', type=<ChunkType.TEXT: 'text'>, content='(a) Original Test Case', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_17', type=<ChunkType.TEXT: 'text'>, content='assert parse_dict_header(’foo=\"is␣a␣fish\",␣bar=\"as␣well\"’) == {\"foo\": \"is␣a␣fish\", \"bar\": \"as␣well\"} assert parse_dict_header(\"key_without_value\") == {\"key_without_value\": None}', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_0', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_1', type=<ChunkType.TEXT: 'text'>, content='Figure 13: The test function test_parse_dict_header is simplified: in the original, @pytest.mark.parametrize to feed multiple input/expected pairs into one function; in the gistified version, this is replaced with two direct assert statements, one per case. it used', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_2', type=<ChunkType.TEXT: 'text'>, content='Table 7: Analysis of tool usage during the Gistify task', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_3', type=<ChunkType.TABLE: 'table'>, content='Models Avg. tool usage view search execute GPT-5-mini 10.8 71.9 9.8 1.7 16.6 GPT-5 18.5 72.4 8.3 3.3 16.1 17.3 67.5 10.1 4.5 17.9 Claude-Sonnet-4 19.3 74.6 2.1 11.8 11.5', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Models</th><th>Avg.</th><th>tool usage</th><th>| view</th><th>search</th><th>execute</th><th>other</th></tr></thead><tbody><tr><td>GPT-5-mini</td><td></td><td>10.8</td><td>71.9</td><td>9.8</td><td>L7</td><td>16.6</td></tr><tr><td>GPT-5</td><td></td><td>18.5</td><td>72.4</td><td>8.3</td><td>3.3</td><td>16.1</td></tr><tr><td>Slaude-Sonnet-3.7</td><td></td><td>17.3</td><td>67.5</td><td>10.1</td><td>4.5</td><td>17.9</td></tr><tr><td>Claude-Sonnet-4</td><td></td><td>19.3</td><td>74.6</td><td>2.1</td><td>11.8</td><td>11.5</td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_4', type=<ChunkType.TEXT: 'text'>, content='Claude-Sonnet-3.7', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_5', type=<ChunkType.TEXT: 'text'>, content='other', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_6', type=<ChunkType.TEXT: 'text'>, content='and to gather execution feedback; (2) after creating a file, it iteratively executes it to verify that the generated gistified file behaves as expected; and (3) it repeatedly compares the outputs of the gistified file and the original codebase under the given command. We also observe that, due to this iterative checking process, enabling the execution tool often leads the model to terminate because it reaches the maximum step limit.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_8', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_9', type=<ChunkType.TEXT: 'text'>, content='Behavior Reading', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_10', type=<ChunkType.TEXT: 'text'>, content='How to Operate:', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_11', type=<ChunkType.TEXT: 'text'>, content='1. Examine the test file and the test function used for {problem statement}', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_12', type=<ChunkType.TEXT: 'text'>, content='2. Identify which module used by these functions are defined in {working dir}', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_13', type=<ChunkType.TEXT: 'text'>, content='3. Copy and inline the code from those modules into ‘concise.py’', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_14', type=<ChunkType.TEXT: 'text'>, content='4. Check these modules for any internal functions or classes and inline them as needed.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_15', type=<ChunkType.TEXT: 'text'>, content='5. Repeat this process recursively until all internal dependencies are inlined.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_16', type=<ChunkType.TEXT: 'text'>, content='6. Do not forget to copy and paste external imports.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_17', type=<ChunkType.TEXT: 'text'>, content='Figure 14: Prompt for Reading strategy.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_18', type=<ChunkType.TEXT: 'text'>, content='Trace Reasoning', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_19', type=<ChunkType.TEXT: 'text'>, content='How to Operate:', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_20', type=<ChunkType.TEXT: 'text'>, content='1. Predict the execution traces.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_21', type=<ChunkType.TEXT: 'text'>, content='2. Follow the traces and inline (copy) only the necessary executed lines into ‘concise.py’', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_22', type=<ChunkType.TEXT: 'text'>, content='3. Repeat until all traces are fully handled.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_23', type=<ChunkType.TEXT: 'text'>, content='Figure 15: Prompt for Tracing strategy.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_24', type=<ChunkType.TEXT: 'text'>, content='C.2 Tool Usage Rates', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_25', type=<ChunkType.TEXT: 'text'>, content='Table 7 shows the statistics on tool usage across models using SWE-bench. We group various tools into four categories: view, search, execute, and other, which includes all remaining tools. For all models, we compute usage rates both with and without execution enabled, and then average across the two settings.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_26', type=<ChunkType.TEXT: 'text'>, content='Among all models, Claude-4 exhibits the highest average tool usage for each test cases, followed by GPT-5, Claude-3.7, and GPT-5-mini. In terms of specific functionality, Claude-4 shows the highest rate of both view and execute tool usage, while Claude-3.7 shows the highest usage of the search tool. To generate a high-quality gistified file, a model must effectively view relevant files and copy only the necessary content. The strong performance of Claude-4 on line existence may be related to its high usage of the view tool. Also, the execution tool tends to support correctness verification of the generated file, which would lead to high execution fidelity.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_0', type=<ChunkType.TEXT: 'text'>, content='Tool Information', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_1', type=<ChunkType.TEXT: 'text'>, content='* search repo (repograph): searches in the mentioned repository with a specific function or class, and returns the def and ref relations for the search term.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_2', type=<ChunkType.TEXT: 'text'>, content='* trace info (tracing): returns the trace log when running the ‘pytest’ command for the test currently being debugged in the given repo', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_3', type=<ChunkType.TEXT: 'text'>, content='Figure 16: Details of the information provided to the model about each tool.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_6', type=<ChunkType.TEXT: 'text'>, content='{ \"role\": \"assistant\", \"tool_calls\": { \"arguments\": \"{\\\\\"command\\\\\":\\\\\"cd␣/app␣&&␣python␣-m␣pytest␣ concise.py::TestDeprecationOptions::test_new_names␣-v\\\\\"}\", \"name\": \"bash\" } }, { \"role\": \"tool\", \"content\": [ { \"type\": \"text\", \"text\": \"OBSERVATION:␣test␣session␣starts␣...AssertionError\\\\n...pylint:␣error:␣unrecognized␣ arguments:␣/app/regrtest_data/empty.py\\\\n␣FAILED␣ concise.py::TestDeprecationOptions::test_new_names␣...\" } }, ... (fix concise.py based on observation) ... { \"role\": \"assistant\", \"tool_calls\": { \"arguments\": \"{\\\\\"command\\\\\":\\\\\"cd␣/app␣&&␣python␣-m␣pytest␣ tests/config/test_argparse_config.py::TestDeprecationOptions::test_new_names␣-v␣-s\\\\\"}\", \"name\": \"bash\" } }, ... { \"role\": \"assistant\", \"content\": \"Excellent!␣The␣messages␣are␣identical.␣Let␣me␣verify␣my␣implementation␣removes␣all␣ unused␣imports␣and␣only␣includes␣what’s␣needed:\"', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_7', type=<ChunkType.TEXT: 'text'>, content='}', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_8', type=<ChunkType.TEXT: 'text'>, content='Figure 17: Trajectory when including execution tool. Models often iteratively execute the given codebase or generated gistified file to ensure that it operates expectedly.', summary='', caption=None, image_paths=None, metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# from src.ingest.parser.multimodal_parser import parse_pdf\n",
    "# from src.ingest.tools.summarizer import Summarizer\n",
    "# from src.ingest.tools.image_captioner import ImageCaptioner\n",
    "# from src.ingest.models.document import DocumentChunk, ChunkType\n",
    "# from uuid import uuid4\n",
    "\n",
    "# from config import TextCategory\n",
    "\n",
    "# pdf_path = \"../../data/raw_papers/2510.26790v1.pdf\"\n",
    "# paper_id = str(uuid4())\n",
    "# elements, texts, tables = parse_pdf(pdf_path)\n",
    "\n",
    "# summarizer = Summarizer()\n",
    "# captioner = ImageCaptioner()\n",
    "\n",
    "# # Extract images\n",
    "# all_images_b64 = captioner.extract_base64_images(elements)\n",
    "# image_captions = captioner.caption_images(all_images_b64)\n",
    "\n",
    "# # Summarize texts and tables\n",
    "# raw_texts = [getattr(t, \"text\", \"\").strip() for t in texts if getattr(t, \"text\", \"\").strip()]\n",
    "# raw_htmls = [getattr(t.metadata, \"text_as_html\", \"\").strip() for t in tables if hasattr(t.metadata, \"text_as_html\")]\n",
    "\n",
    "# text_summaries = summarizer.summarize_texts(raw_texts)\n",
    "# table_summaries = summarizer.summarize_texts(raw_htmls)\n",
    "\n",
    "# # Build chunks\n",
    "# chunks = []\n",
    "# img_idx = 0\n",
    "\n",
    "# TEXT_CATEGORIES = {\n",
    "#     TextCategory.NARRATIVE_TEXT,\n",
    "#     TextCategory.TITLE,\n",
    "#     TextCategory.TEXT,\n",
    "#     TextCategory.LIST_ITEM,\n",
    "#     TextCategory.ABSTRACT,\n",
    "#     TextCategory.UNCATEGORIZED,\n",
    "#     TextCategory.FIGURECAPTION,\n",
    "#     TextCategory.FORMULA,\n",
    "#     TextCategory.CODESNIPPET\n",
    "# }\n",
    "\n",
    "# for i, composite_elem in enumerate(elements):\n",
    "#     try:\n",
    "#         orig_elements = composite_elem.metadata.orig_elements\n",
    "#         if not isinstance(orig_elements, list):\n",
    "#             continue\n",
    "#     except AttributeError:\n",
    "#         continue\n",
    "\n",
    "#     for j, elem in enumerate(orig_elements):\n",
    "#         chunk_id = f\"{paper_id}_{i}_{j}\" \n",
    "        \n",
    "#         category = getattr(elem, \"category\", \"\")\n",
    "\n",
    "#         if category in TEXT_CATEGORIES:\n",
    "#             chunks.append(DocumentChunk(\n",
    "#                 paper_id=paper_id, chunk_id=chunk_id, type=ChunkType.TEXT,\n",
    "#                 content=getattr(elem, \"text\", \"\"), \n",
    "#                 summary=text_summaries.pop(0) if text_summaries else \"\"\n",
    "#             ))\n",
    "        \n",
    "#         elif category == \"Table\":\n",
    "#             chunks.append(DocumentChunk(\n",
    "#                 paper_id=paper_id, chunk_id=chunk_id, type=ChunkType.TABLE,\n",
    "#                 content=getattr(elem, \"text\", \"\"), \n",
    "#                 summary=table_summaries.pop(0) if table_summaries else \"\",\n",
    "#                 metadata={\"html\": getattr(elem.metadata, \"text_as_html\", \"\") or \"\"}\n",
    "#             ))\n",
    "\n",
    "#         elif category == \"Image\":\n",
    "#             if img_idx < len(image_captions):\n",
    "#                 chunks.append(DocumentChunk(\n",
    "#                     paper_id=paper_id, chunk_id=chunk_id, type=ChunkType.FIGURE,\n",
    "#                     content=\"\",\n",
    "#                     caption=image_captions[img_idx],\n",
    "#                     image_path=f\"{paper_id}_fig_{img_idx}.png\"\n",
    "#                 ))\n",
    "#                 img_idx += 1\n",
    "\n",
    "# print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f81eaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_0', type=<ChunkType.TEXT: 'text'>, content='5', summary='Gistify is a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow and the ability to produce potentially large code patches.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_1', type=<ChunkType.TEXT: 'text'>, content='2025', summary='Large language models (LLMs) are increasingly being used in code-related tasks. Yet, the evaluation toolkit for assessing such capabilities has lagged behind. Evidence shows that widely-adopted benchmarks such as SWE-bench do not require full reasoning over the whole execution.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_2', type=<ChunkType.TEXT: 'text'>, content='2', summary='Gistify was inspired by a common practice of how developers navigate and understand unfamiliar repositories. Gistify formalizes this practice by requiring an (agentic) coding model to extract the gist of a given command. It produces a single, self-contained, minimal, and executable gistified file.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_3', type=<ChunkType.TEXT: 'text'>, content='0', summary='Gistify is a way to measure codebase-level understanding. Given a codebase and a command of entrypoint, the goal is to generate a minimal, self-contained gistified code file.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_4', type=<ChunkType.TEXT: 'text'>, content='2', summary='Gistify provides insight into the ability of models to reason at the codebase level. It requires only the repository and an entrypoint, without issued logs or pull requests. This allows automatic construction of challenging tasks for any arbitrary repositories.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_6', type=<ChunkType.TEXT: 'text'>, content='5', summary='When given a codebase and a command as input, the coding agent must generate a single gistified file that reproduces the runtime behavior of the original codebase under the given command. Gistify must satisfy the following requirements.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_7', type=<ChunkType.TEXT: 'text'>, content='2', summary='All models are prompted to generate a gistified file for the entrypoint. We can programmatically verify whether the expected behavior is preserved when the ground-truth test is run. Here, we focus on comparing outputs of test commands.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_8', type=<ChunkType.TEXT: 'text'>, content=':', summary='Execution Fidelity is a binary metric where 1 means the gistified file runs successfully and produces the same output as the original codebase. Line Execution Rate measures minimality by calculating the fraction of lines that are actually executed under the given command.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_9', type=<ChunkType.TEXT: 'text'>, content='v', summary='Line Existence Rate measures the proportion of code in the gistified file that is directly preserved from the original codebase. A 100% existence rate indicates full fidelity to theOriginal codebase without hallucination.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_10', type=<ChunkType.TEXT: 'text'>, content='arXiv', summary='We conduct experiments using three widely adopted open-sourced frameworks. SWE-Agent and GitHub Copilot provide a rich scaffolding to LLM-based agents. We also experiment with Mini-SWE- Agent, a lightweight framework where LLMs only have access to a bash terminal to solve the task.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_11', type=<ChunkType.TEXT: 'text'>, content='i', summary='SWE- Bench is an open-source, cross-platform test bench. It is based on SWE-bench and the GitHub repositories pylint, flask, scikit-learn, seaborn, and debug-gym. We evaluate over 25 tests for each of the 5 repositories.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_12', type=<ChunkType.TEXT: 'text'>, content='X', summary='We report results for our main evaluation protocol, where the model does not have access to execution tools. Examples of gistified files are in Appendix B. Among the models evaluated, Claude-4 tends to perform best; however, performance drops sharply on the hard subsets.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_13', type=<ChunkType.TEXT: 'text'>, content='r', summary='GPT-5-mini and Claude-3.7 models are strong bash users. GPT models achieve higher Line Existence, whereas Claude models achieve high Line Execution.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_14', type=<ChunkType.TEXT: 'text'>, content='a', summary='Execution tools are not a silver bullet. We observed a sharp decrease in performance for the GPT-5 model when evaluated on SWE-Agent without execution tools. A detailled discussion can be found in Appendix B.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_15', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a frog with a leaf in its mouth', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_16', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='Table 2 shows that each model tends to fail for different reasons. Error cases are categorized into four groups. See Appendix B.4 for detailed examples of each error case.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_17', type=<ChunkType.TEXT: 'text'>, content='Hyunji Lee∗1, Minseon Kim2, Chinmay Singh2, Matheus Pereira2,', summary='Missing Test Function errors occur when the generated gistified file does not contain the function implemen- tation for the test specified in the given command. This can happen when the model strips out the content of the test and executes it outside of the pytest wrapper. The results indicate this is the most common cause of error for the best performing model, Claude-4.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_18', type=<ChunkType.TEXT: 'text'>, content='Atharv Sonwane3, Isadora White4, Elias Stengel-Eskin5, Mohit Bansal1, Zhengyan Shi2,', summary='We observe that models frequently modify the test function, despite being provided with explicit instructions to copy without modification. To measure such modifications, we define the Test F1 Score as the line-level overlap between the test code of the original file and the gistified version. High Test F 1 Score indicates that the model has successfully identified and copied the correct test function.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_19', type=<ChunkType.TEXT: 'text'>, content='Alessandro Sordoni2, Marc-Alexandre Côté2, Xingdi Yuan2, Lucas Caccia∗2', summary='In this section, we analyze how different strategies and tools affect performance on the Gistify task. For all experiments, we evaluate 50 test instances drawn from the pylint codebase, a setting where the model generally exhibited modest performance. We use SWE-Agent paired with Claude-Sonnet-4.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_20', type=<ChunkType.TEXT: 'text'>, content='∗Equal contribution 1University of North Carolina at Chapel Hill 2Microsoft Research 3Cornell University 4University of California San Diego 5University of Texas at Austin', summary='In this section, we analyze how different strategies and sources of information affect model performance. We first begin with the simplest approach: modifying the prompt to provide explicit task guidance. We then move to more explicit approaches that rely on additional tools. Detailed descriptions of prompts and tools, along with examples, are provided in Appendix C.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_21', type=<ChunkType.TEXT: 'text'>, content='hyunjil@cs.unc.edu debug-gym@microsoft.com', summary='Execution-Based Tools In Section 3.2, we saw that enabling execution tools resulted in small but consistent gains overall. In this section, we examine whether having unrestricted access to a bash terminal is really necessary to observe these gains, or whether simply having access to execution logs of the generated file is enough. The results are surprising: Having access to fewer tools actually increases performance.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_22', type=<ChunkType.TEXT: 'text'>, content='https://microsoft.github.io/debug-gym', summary='In this section, we investigate what properties make a given test hard to Gistify. We hypothesize that tests generating a longer and more complex execution trace would entail a harder task for the coding LLM. We investigate how two axes to measure a runtime execution’s difficulty affect performance.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_23', type=<ChunkType.TEXT: 'text'>, content='As coding agents are increasingly deployed in large codebases, the need to automatically design challenging, codebase-level evaluation is central. We propose Gistify, a task where a coding LLM must create a single, minimal, self-contained file that can reproduce a specific functionality of a codebase. The coding LLM is given full access to a codebase along with a specific entrypoint (e.g., a python command), and the generated file must replicate the output of the same command ran under the full codebase, while containing only the essential components necessary to execute the provided command. Success on Gistify requires both structural understanding of the codebase, accurate modeling of its execution flow as well as the ability to produce potentially large code patches. Our findings show that current state-of-the-art models struggle to reliably solve Gistify tasks, especially ones with long executions traces.', summary='We use the same configuration as prior analysis as Claude-4 with 50 tests sampled from the pylint codebase. In Figure 2a, we see a clear correlation between the difficulty of a given Gistify task, and how complex the execution traces are, according to both metrics considered. On this subset, performance drops to 21%, as compared to 43%, the baseline weighted performance average following the', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_24', type=<ChunkType.TEXT: 'text'>, content='1 Introduction', summary='In this section, we experiment over how models perform in a static setup. As such static coding LLMs do not have tools, they cannot search or view files dynamically. As they cannot iterate over multiple steps, they have to output everything at once. This suggests that selecting files dynamically over multiple iterations is more effective.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_25', type=<ChunkType.TEXT: 'text'>, content='Large language models (LLMs) are increasingly being used in code-related tasks, powering applications in debugging (Yuan et al., 2025) and agentic code generation (Yang et al., 2024; Liang et al., 2025). Thus, the ability to handle isolated snippets and reasoning across entire codebases, including complex file and module relationships, is becoming increasingly essential. Yet, the evaluation toolkit for assessing such capabilities has lagged behind. Recent evidence shows that widely-adopted repository-level benchmarks such as SWE-bench (Jimenez et al., 2024) and RepoBench (Liu et al., 2023b) still do not require full reasoning over the whole execution and could be solved through heuristic shortcuts or retrieval of localized patches (Aleithan et al., 2024; Liang et al., 2025). Moreover, because many of these datasets rely on GitHub issues or pull requests for construction, they are not easily generalizable to arbitrary repositories. At the same time, coding agents are increasingly deployed in large, real-world codebases, highlighting the need for automatically constructed, broadly applicable, and more challenging repository-level evaluation.', summary='Previous work has introduced a variety of benchmarks to evaluate LLMs on codebase-level code understanding. These generally fall into three categories: question answering, code synthesis, and mapping natural language specifications to the entire codebase. Our work tackles a more complex setting, where models must reason over full execution traces and examine multiple files.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_26', type=<ChunkType.TEXT: 'text'>, content='To fill this gap, we introduce the Gistify task, which is deliberately inspired by a common practice of how developers navigate and understand unfamiliar repositories. Rather than reading files in isolation, they start from a concrete execution point such as test command or entry script often mentioned in READMEs. Then, they iteratively reason over the runtime behavior such as identifying dependencies, following control paths to uncover the codebase’s structure and functionality. Gistify formalizes this practice by requiring an (agentic) coding model to extract the gist of a given command, i.e. to generate a single, self-contained, minimal, and executable gistified file that faithfully reproduces the runtime behavior of a given command as when using the original full codebase (Figure 1). In addition to serving as a challenging coding task, such gistified repositories might give human coders a better understanding of a specific functionality of a given codebase, or even a way to export the single functionality of interest without inheriting heavy dependencies.', summary='There are also works that isolates (or sandboxes) functionalities from the codebase, to simplify dependencies while preserving executability. This sandboxing step is similar to Gistify in that it tries to construct a simplified file that has the feature extracted. However, the sandboxing is done programmatically: relevant code snippets are extracted by leveraging the (static) call graph. In contrast, our work focuses on generating a simplifiedfile using an LLM, thereby', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_27', type=<ChunkType.TEXT: 'text'>, content='To perform well in Gistify, an agent should generate a single gistified file that satisfies four key requirements: it should be self-contained, including all necessary components from the codebase so that it can be executed independently; it should ensure execution fidelity, producing the same outputs as the original codebase under the given command; it should satisfy minimality, retaining only the essential code required for execution without redundant or extraneous lines; and it should guarantee faithful preservation, avoiding hallucinated or fabricated code and relying solely on content from the original codebase. To assess model performance, we introduce evaluation metrics that align with these requirements, providing a systematic', summary='Recent work on autonomous agents for codebase-level code understanding has focused on improving code navigation, reasoning, and generation. Approaches leverage structural information of code for function-call graphs, module-dependency graphs, and hierarchical code structures. Another line of work integrate multi-step reasoning and state update policies to enable more effective planning.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_28', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='microsoft windows logo', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_29', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='Various works have introduced benchmarks to evaluate LLMs’ ability to reason over code execution at runtime. In this work, we extend prior approaches by going beyond reasoning over execution traces to also reformulate programs. We further show that this capability serves as a useful tool at inference time, helping models better structure and complete execution-driven tasks.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_30', type=<ChunkType.TEXT: 'text'>, content='compact.py', summary='In this paper, we introduced the Gistify task in which a coding LLM extracts a specific funtionality of a codebase into a single, self-contained file. The file could be leveraged in other downstream tasks, such as code refactoring or debugging.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_31', type=<ChunkType.TEXT: 'text'>, content='from http.cookies import Morsel', summary='Coding LLMs increasingly being deployed in real-world software development. State-of-the-art LLMs still face challenges on the Gistify task, especially when faced with long, complex execution traces.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_32', type=<ChunkType.TEXT: 'text'>, content='test_requests. py', summary='Anthropic. Claude sonnet 3.7. Hybrid reasoning model. Swe-bench+: Enhanced coding benchmark for llms. Ai pair programming in your terminal.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_33', type=<ChunkType.TEXT: 'text'>, content='from requests.compact import Morsel from adapters import HTTPAdapter', summary='Do swe-agents solve multi-file issues like humans? a deep dive into swe- bench verified, January 2025. Corecodebench: A configurable multi-scenario repository-level benchmark. CRUXEval: A benchmark for code reasoning, understanding and execution.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_34', type=<ChunkType.TEXT: 'text'>, content='class TestMorsel: morsel = Morsel()', summary='Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. Swe-bench: Can language models resolve real-world GitHub issues? Infibench: Evaluating the question-answering capabilities of code large language models.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_35', type=<ChunkType.TEXT: 'text'>, content='def test_cookie(): s=TestMorsel() smount(}iTTPAdapter(0, 0))', summary='Codecgraph is an open-source framework for repository-level code documentation generation. The project is based on the llm-powered Repoagent project.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_36', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a red arrow pointing up in the air', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_37', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a red object with a red ribbon', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_38', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a red letter t on a white background', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_39', type=<ChunkType.TEXT: 'text'>, content='auth.py', summary='Gittaskbench: A benchmark for code agents solving real-world tasks through code repository leveraging. Microsoft. Github copilot in vs code. 2025. URL https://code.visualstudio.com/docs/copilot/ overview.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_40', type=<ChunkType.TEXT: 'text'>, content='istified_file. G be from http.cookies import Morsel def _basic_auth(username): class BaseAdapter: . class HTTPAdapter(BaseAdapter): class HTTPAdapter(BaseAdapter): def __ init__(self): class TestMorse!: morsel = Morsel() self.auth() SS ra G ist ify def test_cookie(): s=TestMorsel() s.mount(\"http://\", HTTPAdapter(0, 0))', summary='Aims to evaluate large language models and agents for machine learning tasks on repository-level code. Openhands: An open platform for ai software developers as generalist agents.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_41', type=<ChunkType.TEXT: 'text'>, content='def _basic_auth(username):', summary='Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1–12, 2024.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_42', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='there is a blue object with a blue arrow on it', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_43', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='there is a blue toothbrush sitting on a counter top', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_44', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a blue arrow on a white background', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_45', type=<ChunkType.TEXT: 'text'>, content='-', summary='Execution fidelity measures whether the generated gistified file reproduces the same functional behavior as the original codebase under the given command. A rate of 100% indicates that the file is concise and contains primarily necessary lines that are executed.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_46', type=<ChunkType.TEXT: 'text'>, content='adapters. py from auth import _basic_auth', summary='The line existence rate measures the proportion of lines in the gistified file that are directly preserved from the original codebase. We classify each line of code into three categories: executable, potentially executable, and non-executable.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_47', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a close up of a cell phone with a green arrow pointing to the bottom', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_48', type=<ChunkType.TEXT: 'text'>, content='BaseAdapter: auth(self): er iaend)', summary='Code that is functionally identical may differ in formatting. To address this, we normalize each code block before performing line-wise matching. These normalizations ensure robustness by making the comparison focus on the code’s underlying structure and functionality rather than superficial formatting differences.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_49', type=<ChunkType.TEXT: 'text'>, content='Command', summary='We evaluate experiments with three agentic frameworks: mini-SWE-Agent, SWE- Agent, and Copilot. All experiments are run in the default Gistify setup, where the model is restricted from executing any commands (e.g., python, pytest)', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_50', type=<ChunkType.TEXT: 'text'>, content='pytest test_requests.py::test_cookie', summary='For each repository, we extract all available test cases, including parameterized ones. For experimental test runs, we group tests3 that share the same base structure but differ only in parameterization. In the main experiments, we used 25 test instances for each of the five codebases.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_51', type=<ChunkType.TEXT: 'text'>, content='Figure 1: The Gistify task: given a codebase and a command of entrypoint, the goal is to generate a', summary='We observe that when specific command-line parameters are provided, models often adapt the generated gistified file to those parameters. We group test cases based on the parameters provided to the command. See Appendix A.5 for more details.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_52', type=<ChunkType.TEXT: 'text'>, content='minimal, self-contained gistified code file that faithfully reproduces the original runtime behavior using code from the given codebase.', summary='Produce one file only: ‘{working dir}/concise.py’ (no extra files, no analysis, no commentary) Inline internal dependencies. Copy every function, class, or top-level code that is executed when running {problem statement}. Do not use ‘import’ statements for modules defined in {working dir}.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_53', type=<ChunkType.TEXT: 'text'>, content='way to measure codebase-level understanding. Gistify requires agents to follow the execution path through the codebase without bypassing modules, i.e., understanding how relevant objects are modified along the way, and identifying which classes or functions can be simplified or removed. Since even moderately sized codebases exceed the context window of current LLMs, success also requires effective search capabilities.', summary='We categorize errors into four types:Import Error Figure 8 shows an example of Import Error. This occurs when the model incorrectly imports the original repository (e.g., import requests) instead of inlining the required modules into the gistified file.File Creation Failure This error arises when theModel fails to generate the gistify file. This can happen in two ways: (1) the model exceeds the maximum step limit or (2) theModel completes within the time limit.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_54', type=<ChunkType.TEXT: 'text'>, content='The advantages that Gistify brings are multiple: first, it provides direct insight into the ability of models to reason at the codebase level with an understanding of runtime execution, rather than on isolated code snippets. Second, it is lightweight and broadly applicable: it requires only the repository and an entrypoint, without issued logs or pull requests. This allows automatic construction of challenging tasks for any arbitrary repositories, including private ones. Finally, gistified files themselves are valuable outputs: by compressing a specific feature of a large codebase into a minimal file, they can be applied to various downstream tasks, including automated debugging or error localization.', summary='Missing Test Function occurs when the generated gistified file does not contain the modules for specified test in the given command. Conceptually, this corresponds to a 0% line existence rate for the test function. We also observe an interesting behavior of GPT-5 where it tends to insert __name__ even though it is not provided in the original codebase.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_0_55', type=<ChunkType.TEXT: 'text'>, content='We conduct experiments across a variety of frameworks (mini-SWE-agent, SWE-agent, and Copilot) and models (GPT-5-mini, GPT-5, Claude-3.7-Sonnet, and Claude-Sonnet-4) and uncover several interesting findings. First, even widely used, high-performing frameworks and models struggle to create a successful gistified file, especially when execution traces are long and have high coverage on the repositories. Second, faithfully reproducing the test function in the generated file is a strong indicator of gistified performance, as it serves as the starting step for reasoning about execution traces. Third, enabling execution tools yields small but consistent performance gains, and additionally providing global code context and runtime information further boosts performance. Finally, agentic models benefit from dynamically deciding what to read and refine their reasoning through multi-step trajectories, outperforming static approaches.', summary='# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses /gpl -2.0.0/gpl.html# For details, please visit: http://pylint.com/Pylint/blob/main/LICENSE.py. # Copyright (c) 2018 Pylint -dev/ pylint-dev.py is a fork of the Pyleint project.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_0', type=<ChunkType.TEXT: 'text'>, content='2 Gistify', summary='Figure 6: Example of a successful gistified file. The code correctly handles all parameters, achieving 100% line–existence rate, a test F1 score of 100, and an execution rate of 65.5%.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_1', type=<ChunkType.TEXT: 'text'>, content='2.1 Task Definition', summary='This error refers to failures that occur during pytest execution, such as syntax errors or fixture-related issues. We explicitly separate those cases by first verifying the presence of the required test functions and running pytest only when they exist.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_2', type=<ChunkType.TEXT: 'text'>, content='As shown in Figure 1, when given a codebase and a command as input, the coding agent must generate a single gistified file that reproduces the runtime behavior of the original codebase under the given command. Specifically, the gistified file must satisfy the following requirements.', summary='import os from contextlib import contextmanager from pathlib import Path def discover_package_path(path, _search_paths): if isinstance(path , Path): p = path else: p = Path(path) if p.exists() and p.is_dir(): return p raise ValueError(\"Cannot\\xa0determine\\xa0package\\xa0path\") @contextmanager def augmented_sys_path(\\'project/tests\\', \\'data\\', \\'proJECT_ROOT_DIR\\') def test_discover_ package_path_source_root_as_parent(parent): parent = pth.parent new.append(str(parent), str(parent) new.path[:] = new yield finally: sys.path [:] = original @pytest.mark.parametrize( \"py_mod_base_name\", (\"__init__\"), ids=(\"explicit -namespace\"), )', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_3', type=<ChunkType.TEXT: 'text'>, content='Self-Contained: All necessary components from the given codebase must be included so that the gistified file can be executed standalone, i.e. without relying on the codebase. The model must identify all relevant modules and dependencies, demonstrating understanding of inter-file relationships.', summary='Table 5 shows the list of available tools in Github Copilot. B.3 Tools Available in GitHub Copilot: Gistify! Codebase-Level Understanding via Runtime Execution via Pytest.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_4', type=<ChunkType.TEXT: 'text'>, content='Execution Fidelity: Executing the gistified file must replicate the original codebase’s runtime behavior, ensuring the model captures the dynamic execution, not just static code patterns.', summary='GPT-5 shows a notably higher Average Pytest Pass Rate, indicating that among the ones they successfully generate, they tend to pass all pytest. For the Test F1 Score, Claude-4 shows the highest performance.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_6', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='We experiment with two variants of the prompt, Reading and Tracing. RepoGraph (Ouyang et al., 2024) is a plug-in module designed to help LLMs leverage the codebase-level structure. Tracing constructs a graph where each node represents a line of code.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_7', type=<ChunkType.TEXT: 'text'>, content='Minimalism: Only the code essential to reproducing the runtime behavior should be preserved, with unused functions and objects pruned. This requires fine-grained understanding of the code to identify which lines are actually executed and essential for the task.', summary=' copilot_getNotebook returns the list of Notebook cells with id, types, line ranges, language, execution info, and output mime types. edit_notebook_file Edit an existing Notebook file in the workspace. apply_patch Edit text files using a special diff/patch format. run_ notebook_cell Run a code cell in a notebook file and return the output.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_8', type=<ChunkType.TEXT: 'text'>, content='Grounded Preservation: No hallucinated code may be introduced. All content must be derived directly from the original codebase. This ensures the task evaluates the model’s understanding of the codebase, rather than its ability to generate arbitrary code that happens to satisfy the command.', summary='Read the contents of a file. open_simple_browser Preview or open a URL in VS Code’s Simple Browser. fetch_webpage Fetch main content from a webpage for summarization or analysis. think Think deeply about a request and log structured reasoning (no execution)', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_9', type=<ChunkType.TEXT: 'text'>, content='2.2 Evaluation Protocol', summary='Table 6: Average Pytest Pass Rate and Test F1 Score of different models using SWE-Agent. Table 5: Available tools and their descriptions. We note that many tools available to the agent are never used.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_10', type=<ChunkType.TEXT: 'text'>, content='There are two inputs to a Gistify task: i) a docker image containing the target codebase, for consistent evaluation; ii) an entrypoint, such as a pytest command on one of the tests in the codebase. Test cases are existing entrypoints one can easily leverage, but broadly, any command that the user would want to use to run a functionality of the existing codebase is allowed.', summary='The test TestGetNetrcAuth.test_works is converted from a pytest unit test into a standalone script. The Bash tool is a basic utility that allows the model to invoke any necessary Bash commands. The Edit and Execute tool is designed specifically for working with the gistified file.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_1_11', type=<ChunkType.TEXT: 'text'>, content='All models are prompted to generate a gistified file for the entrypoint. We can programmatically verify whether the expected behavior is preserved when the ground-truth test is run within this setup. Here, we focus on comparing outputs of test commands. Once the model generates the gistified file, to ensure that execution for evaluation is based on the original test, we integrate the test code from the original codebase to the gistified file and execute it. This ensures that the model does not cheat by modifying the test.', summary='Figure 17 shows an example of the behavior observed when adding the execution tool. Common patterns we observe are: (1) the model first runs the provided command to identify which files are accessed; and (2) the test runs the command to see which files have been accessed.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_0', type=<ChunkType.TEXT: 'text'>, content='2.3 Metrics', summary='Table 7: Analysis of tool usage during the Gistify task. Avg. tool usage view search execute GPT-5-mini 10.8 71.9 9.8 1.7 16.6 G PT-5 18.5 72.4 8.3 3.3 16.1 17.3 67.5 10.1 4.5 17.9 Claude-Sonnet-4 19.3 74.6 2.1 11.8 11.5 2.7.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_1', type=<ChunkType.TEXT: 'text'>, content='Once a gistified file is generated, we evaluate it using the given execution command. The evaluation considers three dimensions, aligned with the task requirements, to provide a comprehensive measure of a model’s ability to reason over an entire codebase and understand its execution behavior. See Appendix A.1 for more details.', summary='Table 7 shows the statistics on tool usage across models using SWE-bench. We group various tools into four categories: view, search, execute, and other. For all models, we compute usage rates both with and without execution enabled.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_2', type=<ChunkType.TEXT: 'text'>, content='Execution Fidelity is a binary metric where 1 means the gistified file runs successfully and produces the same output as the original codebase when executed under the given command; otherwise, it is 0. Failures include cases where the file is not runnable or yields different outputs. The comparison checks for tests pass/fail consistency and stdout/stderr matching.', summary='The ‘pytest’ command for the test currently being debugged in the given repo. The ‘repograph’ function searches in the mentioned repository with a specific function or class, and returns the def and ref relations.', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_3', type=<ChunkType.TEXT: 'text'>, content='Formally, let c denote the given command, C a given codebase, and G a gistified file. Define runs(c,C) as an indicator of whether c executes without crashing when running over C, and out(c,C) returns the set of outputs and error traces from running c with C. Then, execution fidelity is defined as', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_4', type=<ChunkType.TEXT: 'text'>, content='1[runs(c, G) A (out(e, G) = out(e,C))], (1)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_5', type=<ChunkType.TEXT: 'text'>, content='where 1[·] is the indicator function.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_6', type=<ChunkType.TEXT: 'text'>, content='Line Execution Rate measures minimality by calculating the fraction of lines in the gistified file that are actually executed under the given command. A 100% execution rate means all lines are essential, indicating a focused and concise file. This metric is only computed for files that run successfully, since the execution trace is required to determine which lines are run.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_7', type=<ChunkType.TEXT: 'text'>, content='Formally, let Lexec(G) be a list of executable lines (i.e., no comments) in G. Then, the Line Execution rate is defined as', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_8', type=<ChunkType.TEXT: 'text'>, content='1', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_9', type=<ChunkType.TEXT: 'text'>, content='|Lexec(G)| X ℓ∈Lexec(G) 1[ℓ is executed]. (2)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_10', type=<ChunkType.TEXT: 'text'>, content='Line Existence Rate measures the proportion of code in the gistified file that is directly preserved from the original codebase. Specifically, lines of code are grouped into blocks (classes, functions, or top-level units), and matches are computed block by block while respecting the code hierarchy. This helps avoiding false matches from common lines appearing in unrelated parts of the codebase. To ensure robustness, we normalize across common variations such as indentation, multi-line statements, and imports. A 100% existence rate indicates full fidelity to the original codebase without hallucination.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_12', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_13', type=<ChunkType.TEXT: 'text'>, content='Formally, let BG and BC be the sets of blocks in the gistified file and the original codebase, respectively. For a block b, let L(b) represent its set of lines. Then, the existence rate is defined as', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_14', type=<ChunkType.TEXT: 'text'>, content='P 1 b∈BG |L(b)| X b∈BG X ℓ∈L(b) 1{ℓ ∈ LC(b)}, (3)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_2_15', type=<ChunkType.TEXT: 'text'>, content='where 1{ℓ ∈ LC(b)} = 0, if no matching block exists in BC.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_0', type=<ChunkType.TEXT: 'text'>, content='3 Experiments', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_1', type=<ChunkType.TEXT: 'text'>, content='3.1 Setting', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_2', type=<ChunkType.TEXT: 'text'>, content='We conduct experiments using three widely adopted open-sourced frameworks. SWE-Agent (Yang et al., 2024) and GitHub Copilot (Microsoft, 2025) provide a rich scaffolding to LLM-based agents, enabling them to autonomously perform software engineering tasks. This includes a set of tools for creating and editing code files, navigating repositories, and executing tests. These frameworks also offer the LLM controllable cache management, and LLMs follow the standard tool-calling format. We also experiment with Mini-SWE- Agent (Yang et al., 2024), a lightweight framework where LLMs only have access to a bash terminal to solve the task. Commands are parsed from the agent output and executed directly. As the task objective is for the model to use reasoning over the execution flow rather than the ability of tool usage, for the agentic models, we exclude the execution tools (“python”, “pytest”) in the default setting where execution is disabled.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_3', type=<ChunkType.TEXT: 'text'>, content='Our evaluation spans four leading LLM variants: GPT-5 (OpenAI, 2025a), GPT-5-mini (OpenAI, 2025b), Claude-3.7-Sonnet (Anthropic, 2025a), and Claude-Sonnet-4 (Anthropic, 2025b), offering different cost / performance tradeoffs. For ease or reading, we will refer to the last two models as Claude-3.7 and Claude-4. We use a 128K token limit for all models. All experiments run are capped at 50 steps, after which whatever is generated at this moment in the gistifed file is submitted for evaluation.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_3_4', type=<ChunkType.TEXT: 'text'>, content='On the data side, we experiment with widely used GitHub repositories which are present in SWE- Bench (requests, pylint, flask, scikit-learn, seaborn). We also explore an additional repository, debug-gym (Yuan et al., 2025)1. This library is relatively new and importantly does not overlap with SWE-Bench. We extract and filter test sets for each repository. Namely, we remove tests whose execution is dependent on the test’s file location. For the main experiment, we evaluate over 25 tests for each of the 5 repositories. More details regarding the evaluation setup and prompt can be found in the Appendix A.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_0', type=<ChunkType.TEXT: 'text'>, content='3.2 Results', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_1', type=<ChunkType.TEXT: 'text'>, content='We begin by giving an overview of the main results presented in Table 1. We report results for our main evaluation protocol, where the model does not have access to execution tools (e.g. “python” and “pytest” commands), as well as the alternative. Examples of gistified files are in Appendix B.1.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_2', type=<ChunkType.TEXT: 'text'>, content='Strong models and frameworks still struggle with Gistify task. Across models and execution frameworks, performance remains limited: even the strongest model with strong framework (Copilot with Claude-4) achieves 58.7% average Execution Fidelity, a binary success/fail metric, indicating that reliably producing a correct gistified file is still challenging. Among the models evaluated, Claude-4 tends to perform best; however, performance drops sharply on the hard subsets (Section 4.2), suggesting that the benchmark can scale in difficulty and will remain a meaningful target as future models strengthen and require more challenging evaluations.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_3', type=<ChunkType.TEXT: 'text'>, content='Different model families exhibit distinct strengths. Claude-4 achieves the highest Line Existence scores, indicating that it most faithfully extracts relevant code from the original codebase. In contrast, GPT-5 produces the most concise outputs, with a substantially higher Line Execution rate than other models. We', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_4', type=<ChunkType.TEXT: 'text'>, content='1We provide a link to all the GitHub repositories used in this work in Table 4.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_6', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_7', type=<ChunkType.TEXT: 'text'>, content='Table 1: Average Performance over three agentic frameworks with four models. We evaluated over 25 tests over 5 repositories. Execution Fidelity is shown as w/o exec, and w execution tools. Line Existence and Execution are average across the two settings for clarity.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_8', type=<ChunkType.TABLE: 'table'>, content='Framework Model Execution Fidelity Line Existence Line Execution (wo exec / w. exec) GPT-5-mini 17.1 / 24.0 44.9 61.2 mini-SWE-agent GPT-5 Claude-3.7 51.0 / 54.0 38.7 / 43.3 56.8 66.0 83.1 69.2 Claude-4 54.0 / 55.3 67.0 75.7 GPT-5-mini 30.9 / 45.3 47.9 74.8 SWE-agent GPT-5 Claude-3.7 30.7 / 46.0 40.7 / 46.0 48.3 66.8 81.7 69.9 Claude-4 56.7 / 57.3 66.3 72.9 GPT-5-mini 58.0 / 55.3 62.4 77.8 Copilot GPT-5 Claude-3.7 58.7 / 60.7 43.3 / 56.0 66.9 63.0 81.4 74.4 Claude-4 58.7 / 61.3 69.6 80.3', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Framework</th><th>Model</th><th>(wo</th><th>Execution Fidelity exec / w. exec)</th><th>Line Existence</th><th>Line</th><th>Execution</th></tr></thead><tbody><tr><td rowspan=\"4\">a mini SWE-agent</td><td>GPT-5-mini</td><td></td><td>17.1 / 24.0</td><td>44.9</td><td></td><td>61.2</td></tr><tr><td>GPT-5</td><td></td><td>51.0 / 54.0</td><td>56.8</td><td></td><td>83.1</td></tr><tr><td>Oy, de-3.7</td><td></td><td>38.7 / 43.3</td><td>66.0</td><td></td><td>69.2</td></tr><tr><td>Claude-4</td><td></td><td>54.0 / 55.3</td><td>67.0</td><td></td><td>75.7</td></tr><tr><td rowspan=\"4\">SWE. SWE-agent</td><td>GPT-5-mini</td><td></td><td>30.9 / 45.3</td><td>47.9</td><td></td><td>74.8</td></tr><tr><td>GPT-5</td><td></td><td>30.7 / 46.0</td><td>48.3</td><td></td><td>81.7</td></tr><tr><td>Claude-3.7</td><td></td><td>40.7 / 46.0</td><td>66.8</td><td></td><td></td></tr><tr><td>Claude-4</td><td></td><td>56.7 / 57.3</td><td>66.3</td><td></td><td></td></tr><tr><td rowspan=\"4\">Copilot “opie</td><td>GPT-5-mini</td><td></td><td>58.0 / 55.3</td><td>62.4</td><td></td><td>77.8</td></tr><tr><td>GPT-5</td><td></td><td>58.7 / 60.7</td><td>66.9</td><td></td><td>81.4</td></tr><tr><td>Claude-3.7</td><td></td><td>43.3 / 56.0</td><td>63.0</td><td></td><td>TAA</td></tr><tr><td>Claude-4</td><td></td><td>58.7 / 61.3</td><td>69.6</td><td></td><td></td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_9', type=<ChunkType.TEXT: 'text'>, content='observe a similar trend for GPT-5-mini and Claude-3.7: in general, GPT models achieve higher Line Existence, whereas Claude models achieve higher Line Execution.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_10', type=<ChunkType.TEXT: 'text'>, content='Small(er) models perform well with scaffolding. We note that GPT-5-mini’s performance varies significantly across different evaluation settings, from 17% in a bash-only setup to 58% when provided with a large inventory of tools from the Copilot framework (see Appendix B.3 for a full list). We note that this performance increase is also reflected in the quality of the generated gist, where we see a notable increase in line existence and line execution.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_11', type=<ChunkType.TEXT: 'text'>, content='Frontier models (GPT-5 / Claude-4) are strong bash users. When looking at performance on mini-swe-agent, where the models only have access to a bash terminal to solve the task, both models perform relatively well, solving over half of the tasks. Importantly, this is not the case for smaller and previous-generation models.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_4_12', type=<ChunkType.TEXT: 'text'>, content='Execution tools are not a silver bullet. Overall, when comparing performance with and without execution in Table 1, we note that in most cases we observe only a small performance gain. We expected that current coding LLMs could better leverage execution tools: indeed, using tools specifically for runtime execution analysis, such as a debugger, could significantly help solving a gistify task. However, we are not seeing this behavior emerge, even from frontier models. We observed a sharp decrease in performance for the GPT-5 model when evaluated on SWE-Agent without execution tools. We performed a visual inspection and noticed formatting issues when rewriting the input test function. A detailled discussion can be found in Appendix B.2.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_0', type=<ChunkType.TEXT: 'text'>, content='3.3 Error Analysis Over Execution Failure', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_1', type=<ChunkType.TEXT: 'text'>, content='We proceed with an analysis of the underlying failure causes, in order to understand which aspect of the Gistify task different models struggle with. Table 2 shows that each model tends to fail for different reasons. See Appendix B.4 for detailed examples of each error case.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_2', type=<ChunkType.TEXT: 'text'>, content='Import Error occurs when the model incorrectly imports the original codebase (e.g., import requests) instead of inlining the required modules into the gistified file. We note that this error occurs even as coding LLMs are explicitly prompted not to import the specific packages in question. Perhaps surprisingly, the best performing model, Claude-4, commits this seemingly innocuous error the most out of all four models.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_3', type=<ChunkType.TEXT: 'text'>, content='File Creation Failure errors arise when the model fails to generate the gistified file. This can happen in two ways: the model exceeds the maximum step limit, or the model terminates the task without any file being generated.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_6', type=<ChunkType.TEXT: 'text'>, content='Table 2: Average error rates (%) of different failure reasons when running SWE-agent across models. Error cases are categorized into four groups. The numbers in parentheses indicate the number of errors for each category.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_7', type=<ChunkType.TABLE: 'table'>, content='Models Import Error File Creation Failure Missing Test Function Pytest Runtime Error GPT-5-mini 2.1 (2) 11.3 (11) 76.3 (72) 10.3 (10) GPT-5 5.2 (4) 10.4 (8) 77.9 (60) 6.5 (5) Claude-Sonnet-3.7 20.0 (10) 20.0 (10) 2.0 (1) 58.0 (29) Claude-Sonnet-4 32.5 (13) 10.0 (4) 7.5 (3) 50.0 (20)', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Models</th><th>Import Error</th><th>File Creation Failure</th><th>Test Function</th><th>Pytest Runtime Error</th></tr></thead><tbody><tr><td>GPT-5-mini</td><td>2.1 (2)</td><td>11.3 (11)</td><td></td><td>10.3 (10)</td></tr><tr><td>GPT-5</td><td>5.2 (4)</td><td>10.4 (8)</td><td></td><td>6.5 (5)</td></tr><tr><td>Claude-Sonnet-3.7</td><td>20.0 (10)</td><td>20.0 (10)</td><td></td><td>58.0 (29)</td></tr><tr><td>Claude-Sonnet-4</td><td>32.5 (13)</td><td>10.0 (4)</td><td></td><td>50.0 (20)</td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_8', type=<ChunkType.TEXT: 'text'>, content='Missing Test Function errors occur when the generated gistified file does not contain the function implemen- tation for the test specified in the given command, or implements the test in a different structure. This can happen when the model strips out the content of the test and executes it outside of the pytest wrapper, under e.g. if __name__ == __main__:. Claude models tend to avoid this mistake, while this is the main source of error for GPT-5 models, specifically under the SWE-agent framework. Importantly, we observe that this error does not happen at random, but rather alongside other execution errors; we attempted to add the missing test function, and it in most cases the test fails to run, i.e. it results in a runtime error. This aligns with the analysis in the next section, showing a strong correlation between the task’s success and the fidelity between the original and the generated tests.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_5_9', type=<ChunkType.TEXT: 'text'>, content='Pytest Runtime Error occurs when the execution of the generated file fails, either due to a runtime error or because the gistified output does not match the output from the original codebase. The results indicate this is the most common cause of error for the best performing model, Claude-4.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_0', type=<ChunkType.TEXT: 'text'>, content='3.4 Importance of Faithfully Preserving the Test Function', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_1', type=<ChunkType.TEXT: 'text'>, content='We observe that models frequently modify the test function, despite being provided with explicit instructions to copy without modification, except for unavoidable adjustments (e.g., removing imports). Again, to ensure consistent evaluation, we replace the test function in the gistified file with the original version before evaluation.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_2', type=<ChunkType.TEXT: 'text'>, content='To measure such modifications, we define the Test F1 Score as the line-level overlap between the test code of the original file and the gistified version. High Test F1 Score indicates that the model has successfully identified and copied the correct test function to the gistified file. We observe a strong correlation between Test F1 Score and execution fidelity (correlation=0.76, p=0.01); test instances with higher F1 scores are substantially more likely to produce a successful gistified file. We hypothesize that this arises because in the Gistify task, models often reason backwards from the test file, thereby if the model fails from identifying or copying the test function, the subsequent reasoning process is highly likely to fail.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_3', type=<ChunkType.TEXT: 'text'>, content='To better understand the impact of the first step—searching, viewing, and copying the test function—we conduct an ablation study where we remove potential failure at this stage. Specifically, we explicitly provide the correct test function body and signature in the prompt, so the model no longer needs to locate or copy it. This isolates the effect of errors in identifying the test function. In this setting, we observe that Test F1 Score improves highly from the base Gistify 68.4 to 85.3, along with execution fidelity (from 42.0% to 60.0%). This suggests that accurately handling the test function is a critical first step to do the Gistify task successfully. Detailed results are in Appendix B.5.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_4', type=<ChunkType.TEXT: 'text'>, content='4 Analysis', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_5', type=<ChunkType.TEXT: 'text'>, content='In this section, we analyze how different strategies and tools affect performance on the Gistify task, identify factors that contribute to its difficulty, and experiment with the use of a static coding LLM to gain a deeper understanding of the task. For all experiments, we evaluate 50 test instances drawn from the pylint codebase, a setting where the model generally exhibited modest performance. We use SWE-Agent paired with Claude-Sonnet-4.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_7', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_8', type=<ChunkType.TEXT: 'text'>, content='Table 3: Analysis of the effect of different strategies and tools (global information, execution) on the Gistify', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_9', type=<ChunkType.TEXT: 'text'>, content='task. We evaluate SWE-Agent with Claude 4 using 50 test instances from the pylint codebase. Max Steps Reached (%) indicates the percentage of runs that terminated because the maximum step limit was reached.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_10', type=<ChunkType.TABLE: 'table'>, content='Ablation Type Execution Fidelity Line Existence Base Gistify 42.0 65.0 58.3 14.6 Prompted Strategies Tracing Reading 48.0 50.0 75.4 77.6 62.8 62.6 0.0 3.9 Global Info (Tool) RepoGraph Tracing 52.0 56.0 76.1 75.1 60.1 65.1 6.0 0.0 Execution (Tool) Bash Edit And Execute 52.0 56.0 73.1 74.3 64.2 64.2 16.0 10.0', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Ablation</th><th>Type</th><th>| Execution Fidelity</th><th>Line</th><th>Existence</th><th>Line Execution</th><th>| Max Steps Reached (%</th></tr></thead><tbody><tr><td>Base</td><td>GISTIFY</td><td>42.0</td><td>65.0</td><td></td><td>58.3</td><td>| 14.6</td></tr><tr><td>Prompted Strateries</td><td>ewan</td><td>| 00</td><td>6</td><td></td><td>ao</td><td>| 38</td></tr><tr><td>cioatnw cay</td><td>Mpeaph</td><td>| 8</td><td></td><td></td><td>mm</td><td></td></tr><tr><td rowspan=\"2\">Execution (Tool)</td><td>And Execute</td><td>60</td><td>ts</td><td></td><td>1</td><td>| 100</td></tr><tr><td>Edit</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_6_11', type=<ChunkType.TEXT: 'text'>, content='Line Execution Max Steps Reached (%)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_0', type=<ChunkType.TEXT: 'text'>, content='4.1 Effect of Various Strategies and Tools', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_1', type=<ChunkType.TEXT: 'text'>, content='In this section, we analyze how different strategies and sources of information affect model performance. We begin with the simplest approach, modifying the prompt to guide the model (Prompt-Based Guidance), and then move to more explicit approaches that rely on additional tools: providing global context (Global Information via Tools) or feedback from code execution (Execution-Based Tools). Detailed descriptions of prompts and tools, along with examples, are provided in the Appendix C.1.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_2', type=<ChunkType.TEXT: 'text'>, content='Prompt-Based Guidance We first begin with the simplest approach: modifying the prompt to provide explicit task guidance. We experiment in two settings. In the former, we prompt the model to perform step-by-step reasoning, by first predicting the execution traces and then going over them, adding relevant code snippets along the way (tracing). In the latter, a similar approach is used, with explicit instructions on how to recursively determine the execution traces: starting from the test, identify the relevant components and read the files where they are defined, and repeat until the end (reading). As shown in Table 3, we observe that adding such strategies tends to enhance overall metrics, giving both better execution fidelity and more faithful code extractions, as measured by line existence.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_3', type=<ChunkType.TEXT: 'text'>, content='Global Information via Tools Building on the above observation, we next assess the effect of explicitly providing global context through external tools, rather than predicting it. We examine two tools: (1) RepoGraph (Ouyang et al., 2024), which constructs a graph of the codebase where each node represents a line of code and edges capture connections between lines, enabling graph-based search over the entire codebase; and (2) a Tracing tool that exposes gold execution traces obtained from running the given test command. Results in Table 3 show that both tools improve performance, with the Tracing tool yielding the largest gains. This finding suggests that access to the global context, especially the gold tracing information, substantially strengthens the model’s ability to perform runtime reasoning, as it can easily identify which file to look at.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_7_4', type=<ChunkType.TEXT: 'text'>, content='Execution-Based Tools In Section 3.2, we saw that enabling execution tools resulted in small but consistent gains overall. In this section, we examine whether having unrestricted access to a bash terminal is really necessary to observe these gains, or whether simply having access to execution logs of the generated file is enough. For this experiment we compare Bash access with a simple method that executes and prints the output of the gistified file whenever it is edited (Edit And Execute). No other execution tools are available to the agent, including runtime information about the ground truth test. The results are surprising: having access to fewer tools actually increases performance. Indeed, we note that when give access to a full set of bash commands, the coding LLM tends to explore more tools, increasing the overall trajectory length, and potentially reaching the maximum step limit.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_0', type=<ChunkType.TEXT: 'text'>, content='4.2 Tests with High Coverage are Harder to Gistify', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_1', type=<ChunkType.TEXT: 'text'>, content='In this section, we investigate what properties makes a given test hard to Gistify. We hypothesize that tests generating a longer and more complex execution trace would entail a harder task for the coding LLM. To this end, we investigate how two axes to measure a runtime execution’s difficulty affect performance: the length of trace, as measured by the number of function calls executed, and the number of unique files touched by', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_3', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_5', type=<ChunkType.TEXT: 'text'>, content='Performance according to Exec. Trace Difficulty', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_6', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a graph showing the number of files covered by a file', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_7', type=<ChunkType.FIGURE: 'figure'>, content='', summary=None, caption='a chart of the number of different types of lines in a bar chart', image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_8', type=<ChunkType.TEXT: 'text'>, content='(a) Difficulty of the Gistify task is measured as a function of the execution trace difficulty of the underlying test.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_9', type=<ChunkType.TEXT: 'text'>, content='(b) Performance of a static coding LLM and various agentic coding LLMs (mini-SWE-Agent, SWE-Agnet, Copilot).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_10', type=<ChunkType.TEXT: 'text'>, content='the tracing procedure. While these metrics correlate with one another, they will differ when, for example, a function is looped over many times or when the location of the relevant functions is in a single file versus across multiple files.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_8_11', type=<ChunkType.TEXT: 'text'>, content='For this experiment, we use again the same configuration as prior analysis, namely Claude-4 with 50 tests sampled from the pylint codebase. In Figure 2a, we see a clear correlation between the difficulty of a given Gistify task, and how complex the execution traces are, according to both metrics considered. We leverage this insight to create a Gistify-hard subset, where we select the 30 most difficult examples according to each. We end up with 57 unique datapoints (30 from pylint, 28 from sklearn, 6 from seaborn). On this subset, performance drops to 21%, as compared to 43%, the baseline weighted performance average following the same distribution over repositories. Overall, this selection criteria offers a promising direction for designing challenging evaluation scenarios with Gistify.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_0', type=<ChunkType.TEXT: 'text'>, content='4.3 Static Coding LLM', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_1', type=<ChunkType.TEXT: 'text'>, content='In this section, we experiment over how models perform in a static setup, where they have no access to tools and cannot iterate on the generated solution. As such static coding LLMs do not have tools, they cannot search or view files dynamically. Thereby, to measure a possible upper bound for non-agentic approaches, we provide as input all files that were accessed during the original program execution (gold files). Also, as they cannot iterate over multiple steps, they have to output everything at once and are therefore restricted by the context window of the LLM. Since solving the Gistify task involves touching multiple files, we observe in many cases that the inputs exceed the model’s maximum sequence length. Thus, we sample a subset of test examples where the combined content fits within the 128K token limit of the LLM. As shown in Figure 2b, agentic models outperform static ones even when the latter receive all relevant files. This suggests that selecting files dynamically over multiple iterations is more effective than providing everything at once, which can overwhelm the model2. However, interestingly, the static coding LLM setup achieves the highest Line Existence score. This is likely because the model can copy lines directly from input, yet it performs worse on Line Execution and Execution Fidelity, suggesting that models do not have a good understanding of the codebase, often copying lines that are incomplete or incorrect.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_4', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_6', type=<ChunkType.TEXT: 'text'>, content='5 Related Works', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_7', type=<ChunkType.TEXT: 'text'>, content='5.1 Codebase-level Understanding Benchmark', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_8', type=<ChunkType.TEXT: 'text'>, content='Previous work has introduced a variety of benchmarks to evaluate LLMs on codebase-level code understanding. These generally fall into three categories: question answering, code synthesis, and mapping natural language specifications to the entire codebase. Several benchmarks introduce codebase-level question-answering (Strich et al., 2024; Li et al., 2024b; Sahu et al., 2024; Chen et al., 2025; Hu et al., 2024; Fu et al., 2025). In these settings, the model must correctly answer questions that require an understanding of the codebase. The questions are drawn from various sources, including real-world GitHub issues and queries resembling those asked of tools like Copilot. Another line of work evaluates whether models can synthesize code by leveraging information distributed across multiple files in the codebase (Zhang et al., 2023; Liu et al., 2023b; Ding et al., 2023; Li et al., 2024a; Yu et al., 2024). These benchmarks include tasks such as retrieval-augmented completion, cross-file refactoring, and more specialized settings such as sketch-based coding or codebase evolution. Moreover, there is a line of benchmark that maps natural language specifications to entire code repositories, leveraging hierarchical or multi-stage representations to capture inter-file relationships and maintain consistency across a codebase (Tang et al., 2023; Zan et al., 2024; Ni et al., 2025). Our work tackles a more complex setting, where models must reason over full execution traces and examine multiple files, making the task challenging, and even widely used agentic models struggle alongside static ones.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_9_9', type=<ChunkType.TEXT: 'text'>, content='There are also works that isolates (or sandboxes) functionalities from the codebase, to simplify dependencies while preserving executability (Xie et al., 2025b; Jain et al., 2024). This sandboxing step is similar to Gistify in that it tries to construct a simplified file that has the feature extracted. However, the sandboxing step is done programmatically: relevant code snippets are extracted by leveraging the (static) call graph. In contrast, our work focuses on generating a simplified file using an LLM, thereby evaluating the model’s ability to reason about both code dependencies and runtime behavior. Notably, while prior work acknowledges cases in which static programmatic sandboxing fails (e.g., when functions have large dependency slices) and discards those examples, we consider them informative because they require reasoning about more complex runtime behavior. We further observe that these instances also present challenging examples for the Gistify task.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_0', type=<ChunkType.TEXT: 'text'>, content='5.2 Methods for Codebase-Level Understanding', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_1', type=<ChunkType.TEXT: 'text'>, content='Recent work on autonomous agents for codebase-level code understanding has focused on improving code navigation, reasoning, and generation through structured representations and planning. Approaches leverage structural information of code for function-call graphs, module-dependency graphs, and hierarchical code structures to provide models with core components of repositories (Wang et al., 2025; Liu et al., 2024). Another line of work integrate multi-step reasoning and state update policies to enable more effective planning over complex tasks (Bairi et al., 2024; Gautam et al., 2025). Additional methods combine various agents with multiple tools to streamline codebase-level exploration and task solving (Luo et al., 2024; Zhang et al., 2023; Shrivastava et al., 2023; Wang et al., 2024; Yang et al., 2024; Tang et al., 2023; aider, 2025; Microsoft, 2025; cursor, 2025).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_2', type=<ChunkType.TEXT: 'text'>, content='5.3 Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_3', type=<ChunkType.TEXT: 'text'>, content='Various works have introduced benchmarks to evaluate LLMs’ ability to reason over code execution at runtime (Gu et al., 2024; Chen et al., 2024; Xie et al., 2025a; Beger & Dutta, 2025; Hu et al., 2025). These benchmarks typically test whether models can predict execution traces or intermediate states such as variable values, control flow, or data dependencies—given code and inputs, or alternatively, infer inputs from code and outputs. Some benchmarks further extend this paradigm by leveraging execution traces to construct new problems through program composition, thereby varying complexity in a principled way. Beyond evaluation, execution traces have also been incorporated into training pipelines to strengthen models’ runtime reasoning abilities (Liu et al., 2023a; Ding et al., 2024). By augmenting pre-training and fine-tuning with execution states, paths, and coverage signals, these methods help models capture program dynamics and generalize to execution-aware tasks. At inference time, several frameworks leverage runtime feedback to iteratively guide models in debugging or completing partial programs, thereby improving performance on execution-driven', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_10_7', type=<ChunkType.TEXT: 'text'>, content='tasks (Zhong et al., 2024; Xue et al., 2024). In this work, we extend prior approaches by going beyond reasoning over execution traces to also reformulate programs; the model not only tracks execution but also identifies how to compress and organize code into a concise, coherent file. We further show that this capability serves as a useful tool at inference time, helping models better structure and complete execution-driven tasks.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_11_0', type=<ChunkType.TEXT: 'text'>, content='6 Discussion and Conclusion', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_11_1', type=<ChunkType.TEXT: 'text'>, content='In this paper, we introduced the Gistify task in which a coding LLM extracts a specific funtionality of a codebase into a single, self-contained file. Beyond serving as a standalone evaluation task that is easily applicable to arbitrary repositories and execution commands, the gistified file itself also opens several promising directions for research and practical applications. Large codebases often overwhelm automated agents due to their complex dependencies, and they especially struggle when tasked with fixing bugs that span multiple files (Ganhotra, 2025). In such scenarios, a gistified file would greatly reduce this challenge, and enable a more efficient reasoning about the codebase without navigating through unrelated code. In other words, this file could be leveraged in other downstream tasks, such as code refactoring or debugging, or even as a way to extract and share a minimal implementation of a specific codebase functionality.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_11_2', type=<ChunkType.TEXT: 'text'>, content='In summary, with coding LLMs increasingly being deployed in real-world software development, the need for automatically constructing evaluation setups that require codebase-level understanding of arbitrary repositories is growing. Through extensive experiments across a range of models and frameworks, we found that state-of-the-art LLMs still face challenges on the Gistify task, especially when faced with long, complex execution traces. Our analysis shows that incorporating global code context or execution-aware tools improves performance, and agentic coding LLM tend to handle the task more effectively by reasoning about which files to inspect using various tools. Beyond serving as a benchmark, the gistified files themselves are valuable artifacts. They distill the essential functionality of complex systems into a compact, executable form, making them easier to inspect and understand. Such files could support a range of practical applications, including debugging, refactoring, and code review, which we leave for future work.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_11_4', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_0', type=<ChunkType.TEXT: 'text'>, content='References', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_1', type=<ChunkType.TEXT: 'text'>, content='aider. Ai pair programming in your terminal. 2025. URL https://github.com/Aider-AI/aider?tab= readme-ov-file.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_2', type=<ChunkType.TEXT: 'text'>, content='Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, and Song Wang. Swe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_3', type=<ChunkType.TEXT: 'text'>, content='Anthropic. Claude sonnet 3.7. https://www.anthropic.com/news/claude-3-7-sonnet, 2025a. Hybrid reasoning model; accessed: 2025-09-25.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_4', type=<ChunkType.TEXT: 'text'>, content='Anthropic. Claude sonnet 4. https://www.anthropic.com/claude/sonnet, 2025b. Improved version over Sonnet 3.7; accessed: 2025-09-25.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_5', type=<ChunkType.TEXT: 'text'>, content='Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D C, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Balasubramanyan Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675–698, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_6', type=<ChunkType.TEXT: 'text'>, content='Claas Beger and Saikat Dutta. Coconut: Structural code understanding does not fall out of a tree. In 2025 IEEE/ACM International Workshop on Large Language Models for Code (LLM4Code), pp. 128–136. IEEE, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_7', type=<ChunkType.TEXT: 'text'>, content='Jialiang Chen, Kaifa Zhao, Jie Liu, Chao Peng, Jierui Liu, Hang Zhu, Pengfei Gao, Ping Yang, and Shuiguang Deng. Coreqa: uncovering potentials of language models in code repository question answering. arXiv preprint arXiv:2501.03447, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_8', type=<ChunkType.TEXT: 'text'>, content='Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, and Xin Xia. Reasoning runtime behavior of a program with llm: How far are we? arXiv preprint arXiv:2403.16437, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_9', type=<ChunkType.TEXT: 'text'>, content='cursor. cursor. 2025. URL https://cursor.com/.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_10', type=<ChunkType.TEXT: 'text'>, content='Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36:46701–46723, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_11', type=<ChunkType.TEXT: 'text'>, content='Yangruibo Ding, Benjamin Steenhoek, Kexin Pei, Gail Kaiser, Wei Le, and Baishakhi Ray. Traced: Execution- aware pre-training for source code. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1–12, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_12', type=<ChunkType.TEXT: 'text'>, content='Lingyue Fu, Hao Guan, Bolun Zhang, Haowei Yuan, Yaoming Zhu, Jun Xu, Zongyu Wang, Lin Qiu, Xunliang Cai, Xuezhi Cao, et al. Corecodebench: A configurable multi-scenario repository-level benchmark. arXiv preprint arXiv:2507.05281, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_13', type=<ChunkType.TEXT: 'text'>, content='Jatin Ganhotra. Do swe-agents solve multi-file issues like humans? a deep dive into swe- bench verified, January 2025. URL https://jatinganhotra.dev/blog/swe-agents/2025/01/05/ swe-bench-mutliple-files/. Blog post.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_14', type=<ChunkType.TEXT: 'text'>, content='Dhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, and Roshanak Zilouchian Moghaddam. Refac- torbench: Evaluating stateful reasoning in language agents through code. arXiv preprint arXiv:2503.07832, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_15', type=<ChunkType.TEXT: 'text'>, content='Alex Gu, Baptiste Roziere, Hugh James Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. CRUXEval: A benchmark for code reasoning, understanding and execution. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 16568–16621. PMLR, 21–27 Jul 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_16', type=<ChunkType.TEXT: 'text'>, content='Ruida Hu, Chao Peng, Jingyi Ren, Bo Jiang, Xiangxin Meng, Qinyun Wu, Pengfei Gao, Xinchen Wang, and Cuiyun Gao. Coderepoqa: A large-scale benchmark for software engineering question answering. arXiv preprint arXiv:2412.14764, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_18', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_20', type=<ChunkType.TEXT: 'text'>, content='Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, and Kaidi Xu. Dynacode: A dynamic complexity-aware code benchmark for evaluating large language models in code generation. arXiv preprint arXiv:2503.10452, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_21', type=<ChunkType.TEXT: 'text'>, content='Naman Jain, Manish Shetty, Tianjun Zhang, King Han, Koushik Sen, and Ion Stoica. R2e: Turning any github repository into a programming agent environment. In ICML, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_22', type=<ChunkType.TEXT: 'text'>, content='Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_23', type=<ChunkType.TEXT: 'text'>, content='Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_24', type=<ChunkType.TEXT: 'text'>, content='Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. Evocodebench: An evolving code generation benchmark aligned with real-world code repositories. arXiv preprint arXiv:2404.00599, 2024a.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_25', type=<ChunkType.TEXT: 'text'>, content='Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, and Hongxia Yang. Infibench: Evaluating the question-answering capabilities of code large language models. Advances in Neural Information Processing Systems, 37:128668–128698, 2024b.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_26', type=<ChunkType.TEXT: 'text'>, content='Shanchao Liang, Spandan Garg, and Roshanak Zilouchian Moghaddam. The swe-bench illusion: When state-of-the-art llms remember instead of reason. arXiv preprint arXiv:2506.12286, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_27', type=<ChunkType.TEXT: 'text'>, content='Chenxiao Liu, Shuai Lu, Weizhu Chen, Daxin Jiang, Alexey Svyatkovskiy, Shengyu Fu, Neel Sundaresan, and Nan Duan. Code execution with pre-trained language models. In Anna Rogers, Jordan Boyd- Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 4984–4999, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.308. URL https://aclanthology.org/2023.findings-acl.308/.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_28', type=<ChunkType.TEXT: 'text'>, content='Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto- completion systems. arXiv preprint arXiv:2306.03091, 2023b.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_29', type=<ChunkType.TEXT: 'text'>, content='Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, and Wenmeng Zhou. Codexgraph: Bridging large language models and code repositories via code graph databases. arXiv preprint arXiv:2408.03910, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_12_30', type=<ChunkType.TEXT: 'text'>, content='Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, et al. Repoagent: An llm-powered open-source framework for repository-level code documentation generation. arXiv preprint arXiv:2402.16667, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_0', type=<ChunkType.TEXT: 'text'>, content='Microsoft. Github copilot in vs code. 2025. URL https://code.visualstudio.com/docs/copilot/ overview.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_1', type=<ChunkType.TEXT: 'text'>, content='Ziyi Ni, Huacan Wang, Shuo Zhang, Shuo Lu, Ziyang He, Wang You, Zhenheng Tang, Yuntao Du, Bill Sun, Hongzhang Liu, et al. Gittaskbench: A benchmark for code agents solving real-world tasks through code repository leveraging. arXiv preprint arXiv:2508.18993, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_2', type=<ChunkType.TEXT: 'text'>, content='OpenAI. Gpt-5 technical overview. https://platform.openai.com/docs, 2025a. Accessed: 2025-09-25.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_3', type=<ChunkType.TEXT: 'text'>, content='OpenAI. Gpt-5 mini. https://platform.openai.com/docs/models/gpt-5-mini, 2025b. Compact variant of GPT-5; accessed: 2025-09-25.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_4', type=<ChunkType.TEXT: 'text'>, content='Siru Ouyang, Wenhao Yu, Kaixin Ma, Zilin Xiao, Zhihan Zhang, Mengzhao Jia, Jiawei Han, Hongming Zhang, and Dong Yu. Repograph: Enhancing ai software engineering with repository-level code graph. arXiv preprint arXiv:2410.14684, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_5', type=<ChunkType.TEXT: 'text'>, content='Surya Prakash Sahu, Madhurima Mandal, Shikhar Bharadwaj, Aditya Kanade, Petros Maniatis, and Shirish Shevade. Codequeries: A dataset of semantic queries over code. In Proceedings of the 17th Innovations in Software Engineering Conference, pp. 1–11, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_7', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_9', type=<ChunkType.TEXT: 'text'>, content='Disha Shrivastava, Denis Kocetkov, Harm De Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion: Training code models to understand your repository. arXiv preprint arXiv:2306.10998, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_10', type=<ChunkType.TEXT: 'text'>, content='Jan Strich, Florian Schneider, Irina Nikishina, and Chris Biemann. On improving repository-level code QA for large language models. In Xiyan Fu and Eve Fleisig (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 209–244, Bangkok, Thailand, August 2024. Association for Computational Linguistics. ISBN 979-8-89176-097-4. doi: 10.18653/v1/2024.acl-srw.28.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_11', type=<ChunkType.TEXT: 'text'>, content='Xiangru Tang, Yuliang Liu, Zefan Cai, Yanjun Shao, Junjie Lu, Yichi Zhang, Zexuan Deng, Helan Hu, Kaikai An, Ruijun Huang, et al. Ml-bench: Evaluating large language models and agents for machine learning tasks on repository-level code. arXiv preprint arXiv:2311.09835, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_12', type=<ChunkType.TEXT: 'text'>, content='Huacan Wang, Ziyi Ni, Shuo Zhang, Shuo Lu, Sen Hu, Ziyang He, Chen Hu, Jiaye Lin, Yifu Guo, Yuntao Du, et al. Repomaster: Autonomous exploration and understanding of github repositories for complex task solving. arXiv preprint arXiv:2505.21577, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_13', type=<ChunkType.TEXT: 'text'>, content='Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_14', type=<ChunkType.TEXT: 'text'>, content='Danning Xie, Mingwei Zheng, Xuwei Liu, Jiannan Wang, Chengpeng Wang, Lin Tan, and Xiangyu Zhang. Core: Benchmarking llms code reasoning capabilities through static analysis tasks. arXiv preprint arXiv:2507.05269, 2025a.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_15', type=<ChunkType.TEXT: 'text'>, content='Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried, and Carolyn Rose. Repost: Scalable repository-level coding environment construction with sandbox testing. Conference on Language Modeling, 2025b.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_16', type=<ChunkType.TEXT: 'text'>, content='Zhipeng Xue, Zhipeng Gao, Shaohua Wang, Xing Hu, Xin Xia, and Shanping Li. Selfpico: Self-guided partial code execution with llms. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pp. 1389–1401, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_17', type=<ChunkType.TEXT: 'text'>, content='John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik R Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_18', type=<ChunkType.TEXT: 'text'>, content='Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Qianxiang Wang, and Tao Xie. Codereval: A benchmark of pragmatic code generation with generative pre-trained models. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, pp. 1–12, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_19', type=<ChunkType.TEXT: 'text'>, content='Xingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, Lucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, et al. debug-gym: A text-based environment for interactive debugging. arXiv preprint arXiv:2503.21557, 2025.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_20', type=<ChunkType.TEXT: 'text'>, content='Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong, Xiaolin Chen, Bei Guan, et al. Codes: Natural language to code repository via multi-layer sketch. arXiv preprint arXiv:2403.16443, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_21', type=<ChunkType.TEXT: 'text'>, content='Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570, 2023.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_22', type=<ChunkType.TEXT: 'text'>, content='Li Zhong, Zilong Wang, and Jingbo Shang. Debug like a human: A large language model debugger via verifying runtime execution step-by-step. arXiv preprint arXiv:2402.16906, 2024.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_13_24', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_0', type=<ChunkType.TEXT: 'text'>, content='A Experimental Setting', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_1', type=<ChunkType.TEXT: 'text'>, content='A.1 Metrics', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_2', type=<ChunkType.TEXT: 'text'>, content='Execution Fidelity Execution fidelity measures whether the generated gistified file reproduces the same functional behavior as the original codebase under the given command. This includes producing the same number of test passes or failures, as well as consistent outputs and error handling. If the file’s behavior matches the original codebase, it is assigned 100%; otherwise, it receives 0%.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_3', type=<ChunkType.TEXT: 'text'>, content='Line Execution Rate The line execution rate measures the proportion of lines in the gistified file that are actually executed when running it under the given command. We first analyze the gistified file to identify which lines are executable (e.g., imports, function or class definitions) versus not-executable (e.g., comments). Using a tracing function, we then determine which of the executable lines are touched during execution. The line execution rate is computed as the fraction of executable lines that are executed. A rate of 100% indicates that the gistified file is concise and contains primarily necessary lines that are executed, while 0% indicates that non of the executable lines were touched. When calculating line execution rate, we exclude the tests where the self-containment is 0% as the goal of line execution rate is to evaluate the model’s ability to construct a concise, executable file, not to penalize failures in generating runnable code.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_4', type=<ChunkType.TEXT: 'text'>, content='We classify each line of code into three categories: executable, potentially executable, and non-executable. Executable lines include imports and functional code that can be directly run. Potentially executable lines are those that may or may not be executed during a run, such as the except block of a try-except statement or placeholders for classes and function definitions. Non-executable lines, such as comments, are those that have no effect on execution. To calculate the line execution rate, we first classify each line in the gistified file and then consider only the executable lines. Non-executable lines are ignored since their presence or absence does not affect execution outcomes, and potentially executable lines are excluded because they are often ambiguous (e.g., placeholders) and cannot be reliably judged as necessary or removable.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_5', type=<ChunkType.TEXT: 'text'>, content='Line Existence Rate The line existence rate measures the proportion of lines in the gistified file that are directly preserved from the original codebase. We first parse both the gistified file and the original codebase into blocks, where each block corresponds to a class or function. Within classes, functions are nested under their parent class, forming a hierarchy. Lines outside of any block (e.g., top-level statements) are treated as standalone units.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_6', type=<ChunkType.TEXT: 'text'>, content='For each block in the gistified file, we locate the corresponding block in the original codebase using its name and hierarchical position. If a matching block exists, we compare the two line by line to determine which lines are preserved; whether the lines in the gistified block appear in the corresponding original block. If no match is found, all lines in that block are treated as non-existent. For lines outside any block, existence is determined by direct comparison with top-level lines in the original codebase.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_7', type=<ChunkType.TEXT: 'text'>, content='An existence rate of 100% indicates perfect preservation of the original code without hallucinated content.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_14_8', type=<ChunkType.TEXT: 'text'>, content='Normalization for Line-wise Code Matching When checking the existence of a code line within a file, as our objective is to determine semantic equivalence rather than strict syntactical identity, we do normalization; code that is functionally identical may differ in formatting, such as multiline statements, indentations, or space, which can hinder direct line-wise comparison. To address this, we normalize each code block before performing line-wise matching. Specifically, we parse the code into an Abstract Syntax Tree (AST) and ignore comments; split combined import statements into individual imports; merge statements that span multiple lines into a single line; remove inline comments (e.g., for i in range(5): # comment); and eliminate indentation and redundant spaces. These normalizations ensure robustness by making the comparison focus on the code’s underlying structure and functionality rather than superficial formatting differences.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_0', type=<ChunkType.TEXT: 'text'>, content='A.2 Framework', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_1', type=<ChunkType.TEXT: 'text'>, content='We evaluate experiments with three agentic frameworks: mini-SWE-Agent (Yang et al., 2024), SWE- Agent (Yang et al., 2024), and Copilot (Microsoft, 2025). Unless otherwise noted, all experiments are run in the default Gistify setup, where the model is restricted from executing any commands (e.g., python, pytest).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_3', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_4', type=<ChunkType.TEXT: 'text'>, content='SWE-Agent and Copilot Agent enable LLMs to interact with a codebase through a suite of tools, including bash commands. These tools support capabilities such as viewing, searching, editing, and creating files or directories. In addition, Copilot Agent extends this functionality with browser integration, explicit reasoning, and API usage. mini-SWE-agent is a simplified variant of SWE-Agent that only supports bash commands. Despite its minimal design, it achieves strong performance on the SWE-Bench Verified benchmark (Jimenez et al., 2023). For both mini-SWE-Agent and SWE-Agent, we set the maximum number of steps to 50 and run them in the same Docker environment, using the current version of the repositories.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_5', type=<ChunkType.TEXT: 'text'>, content='A.3 Experimental Test Set Construction', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_6', type=<ChunkType.TEXT: 'text'>, content='Table 4: Details of the GitHub repositories used as the test set.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_7', type=<ChunkType.TABLE: 'table'>, content='Repository URL License flask https://github.com/pallets/flask BSD 3-Clause requests https://github.com/psf/requests Apache-2.0 pylint https://github.com/pylint-dev/pylint GPL 2.0 scikit-learn https://github.com/scikit-learn/scikit-learn BSD 3-Clause seaborn https://github.com/mwaskom/seaborn BSD 3-Clause debug-gym https://github.com/microsoft/debug-gym MIT', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Repository</th><th>URL</th><th>License</th></tr></thead><tbody><tr><td>flask</td><td>https: //github.com/pallets/flask</td><td>BSD 3-Clause</td></tr><tr><td>requests</td><td>https: //github.com/psf/requests</td><td>Apache-2.0</td></tr><tr><td>pylint</td><td>https: //github.com/pylint-dev/pylint</td><td>GPL 2.0</td></tr><tr><td>scikit-learn</td><td>| https://github.com/scikit-learn/scikit-learn</td><td>BSD 3-Clause</td></tr><tr><td>seaborn</td><td>https: //github.com/mwaskom/seaborn</td><td>BSD 3-Clause</td></tr><tr><td>debug-gym</td><td>https: //github.com/microsoft/debug-gym</td><td>MIT</td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_15_8', type=<ChunkType.TEXT: 'text'>, content='Table 4 summarizes the repositories used in our evaluation. For each repository, we begin by extracting all available test cases, including parameterized ones. For experimental test runs, we group tests3 that share the same base structure but differ only in parameterization, treating them as a single test. During evaluation, however, we execute all parameterized instances and measure how many are passed, thereby assessing execution fidelity. Finally, we filter out environment-dependent tests, such as those requiring relative file paths or fixed module locations. In the main experiments, we used 25 test instances for each of the five codebases, and the analysis was conducted using 50 test instances from the pylint codebase.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_0', type=<ChunkType.TEXT: 'text'>, content='A.4 Prompt for Gistify', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_1', type=<ChunkType.TEXT: 'text'>, content='Figure 3 shows the prompt used in the main experiments.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_2', type=<ChunkType.TEXT: 'text'>, content='A.5 Providing specific parameters to commands tends to make models generate parameter-specific gistified files', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_3', type=<ChunkType.TEXT: 'text'>, content='We observe that when specific command-line parameters are provided, models often adapt the generated gistified file to those parameters rather than producing a fully general solution. Examples of this parameter- specific behavior are shown in Figures 4 and 5. Accordingly, in our experiments, we group test cases based on the parameters provided to the command.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_4', type=<ChunkType.TEXT: 'text'>, content='B Results', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_5', type=<ChunkType.TEXT: 'text'>, content='B.1 Example of gistified file', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_6', type=<ChunkType.TEXT: 'text'>, content='Figure 6 and Figure 7 show two gistified files on the same test case with different models; each model succeed or fail in generating a gistified file with execution fidelity of 100% and 0%, respectively. In the successful case (Figure 6), the generated file handles both parameters correctly, achieving a 100% line existence rate, a 65.5% execution rate, and a test F1 score of 100. In contrast, the failed case (Figure 7) cannot execute due to a missing import pytest statement. Moreover, the hallucinated test function yields a test F1 score of 0, and the file shows a much lower line existence rate of 28%.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_7', type=<ChunkType.TEXT: 'text'>, content='3We adopt this grouping design as we observe that models often overfit to specific values when parameters are provided. See Appendix A.5 for more details.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_10', type=<ChunkType.TEXT: 'text'>, content='Prompt for Gistify', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_11', type=<ChunkType.TEXT: 'text'>, content='I’ve uploaded a python code repository in the directory {working dir}.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_12', type=<ChunkType.TEXT: 'text'>, content='There is an original test invocation (the command that reproduces behavior we want to preserve): problem statement Your job: create a single file named ‘concise.py’ saved at {working dir} that is **self-contained**, **minimal**, and **executable**, and when used in place of the original test run reproduces the same runtime behavior and outputs. Follow the instructions below when creating the file.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_13', type=<ChunkType.TEXT: 'text'>, content='OUTPUT - Produce one file only: ‘{working dir}/concise.py’. - The assistant must return only the contents of ‘concise.py’ (no extra files, no analysis, no commentary). HIGH-LEVEL RULES for creating ‘concise.py’ 1. Inline internal dependencies * Copy into ‘concise.py’ every function, class, or top-level code from the files inside {working dir} that is executed when running {problem statement}. * Do not use ‘import’ statements for modules defined in {working dir}. 2. Remove unexecuted lines * When copying lines in ‘concise.py’, keep only the lines that is actually executed when running {problem statement}. * Delete unused functions, classes, variables, if-else, imports, and unreachable branches. * Ensure the file remains syntactically correct and minimal after removal. 3. Preserve original source lines * Do not rewrite or reformat lines unless necessary to keep the files valid. * Do not arbitrary generate new lines that do not exist in the original {working dir} files. * You may adjust indentation, remove empty ‘else’‘ blocks, or adapt ‘try-except’ structures only when required to preserve correctness. 4. Keep external imports * Leave imports to external libraries, frameworks, or standard runtime libraries unchanged. * Only remove or inline dependencies that come from {working dir}. 5. No shortcuts or cheating * Do not stub, fake, or monkey-patch external modules. * Do not reimplement or newly add third-party libraries. * Do not hard-code outputs * Do not replace test logic with simplified equivalents 6. Preserve test behavior * The test function much remain unchanged, except for import adjustments needed to reference inlined code. * The output, exceptions, or exit codes must match the original run of {problem statement}. 7. Do not execute the code * Do not run or simulate the program (e.g., with ‘pytest’, ‘python’, or any other tools)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_16_14', type=<ChunkType.TEXT: 'text'>, content='Figure 3: Base Prompt Template for Gistify Task.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_0', type=<ChunkType.TEXT: 'text'>, content='B.2 Error analysis over execution failure', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_1', type=<ChunkType.TEXT: 'text'>, content='We categorize errors into four types:', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_2', type=<ChunkType.TEXT: 'text'>, content='Import Error Figure 8 shows an example of Import Error. This occurs when the model incorrectly imports the original repository (e.g., import requests) instead of inlining the required modules into the gistified file.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_3', type=<ChunkType.TEXT: 'text'>, content='File Creation Failure This error arises when the model fails to generate the gistified file. This can happen in two ways: (1) the model exceeds the maximum step limit or (2) the model completes within the time limit but still fails to generate the new file using the tool.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_6', type=<ChunkType.TEXT: 'text'>, content='@pytest.mark.parametrize(', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_7', type=<ChunkType.TEXT: 'text'>, content='\"value ,␣expected\", ( (\"application/xml\", (\"application/xml\", {})), ( \"application/json␣;␣charset=utf -8\", (\"application/json\", {\"charset\": \"utf -8\"}), ), (\"text/plain\", (\"text/plain\", {})), ... def test__parse_content_type_header(value , expected): assert _parse_content_type_header(value) == expected (a) Original Test Case def test__parse_content_type_header(): \"\"\"Test␣for␣the␣_parse_content_type_header␣function␣with␣application/json␣and␣charset=utf -8\"\"\" value = \"application/json␣;␣charset=utf -8\" expected = (\"application/json\", {\"charset\": \"utf -8\"}) assert _parse_content_type_header(value) == expected', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_8', type=<ChunkType.TEXT: 'text'>, content=')', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_9', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_10', type=<ChunkType.TEXT: 'text'>, content='Figure 4: Example of a model generating a parameter-specific gistified file when given a command that includes a parameter.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_11', type=<ChunkType.TEXT: 'text'>, content='@pytest.mark.parametrize(', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_12', type=<ChunkType.TEXT: 'text'>, content='\"url,␣expected\", ( (\"http://192.168.0.1:5000/\", True), ... (\"http://google.com:5000/v1.0/\", False), ), ) def test_should_bypass_proxies_no_proxy(url, expected , monkeypatch): \"\"\"Tests␣for␣function␣should_bypass_proxies␣to␣check␣if␣proxy ␣␣␣␣can␣be␣bypassed␣or␣not␣using␣the␣’no_proxy’␣argument ␣␣␣␣\"\"\" no_proxy = \"192.168.0.0/24 ,127.0.0.1 , localhost.localdomain ,172.16.1.1\" # Test ’no_proxy’ argument assert should_bypass_proxies(url, no_proxy=no_proxy) == expected', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_13', type=<ChunkType.TEXT: 'text'>, content='(a) Original Test Case', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_14', type=<ChunkType.TEXT: 'text'>, content='def test_should_bypass_proxies_no_proxy(url, expected , monkeypatch): \"\"\"Tests␣for␣function␣should_bypass_proxies␣to␣check␣if␣proxy ␣␣␣␣can␣be␣bypassed␣or␣not␣using␣the␣’no_proxy’␣argument ␣␣␣␣\"\"\" no_proxy = \"192.168.0.0/24 ,127.0.0.1 , localhost.localdomain ,172.16.1.1\" # Test ’no_proxy’ argument assert should_bypass_proxies(url, no_proxy=no_proxy) == expected', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_15', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_16', type=<ChunkType.TEXT: 'text'>, content='Figure 5: Example of a model generating a parameter-specific gistified file when given a command that includes a parameter.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_17', type=<ChunkType.TEXT: 'text'>, content='Missing Test Function This occurs when the generated gistified file does not contain the modules for specified test in the given command. It typically arises when the model fails to locate or copy the modules necessary for the test into the gistified file. Conceptually, this corresponds to a 0% line existence rate for the test function. Since the presence of the modules for the given test case is essential for validation, we classify this as an error.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_18', type=<ChunkType.TEXT: 'text'>, content='We also observe an interesting behavior of GPT-5 where it tends to insert __name__ == \"__main__\" even though it is not provided in the original codebase and even though it is explicitly mentioned that we will test on the provided command and expect the same output. They often remove the test function but move the lines in the test function under the \"__main__\" guard (e.g., Figure 10). We hypothesize that this may be because they are more familiar with codebases following this pattern. We also observe cases where the model', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_20', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_21', type=<ChunkType.TEXT: 'text'>, content='# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl -2.0.html', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_22', type=<ChunkType.TEXT: 'text'>, content='# For details: https://github.com/pylint -dev/pylint/blob/main/LICENSE # Copyright (c) https://github.com/pylint -dev/pylint/blob/main/CONTRIBUTORS.txt from __future__ import annotations import os from collections.abc import Sequence from typing import Any import pytest def discover_package_path(modulepath: str, source_roots: Sequence[str]) -> str: \"\"\"Discover␣package␣path␣from␣one␣its␣modules␣and␣source␣roots.\"\"\" dirname = os.path.realpath(os.path.expanduser(modulepath)) if not os.path.isdir(dirname): dirname = os.path.dirname(dirname) # Look for a source root that contains the module directory for source_root in source_roots: source_root = os.path.realpath(os.path.expanduser(source_root)) if os.path.commonpath([source_root , dirname]) in [dirname , source_root]: return source_root # Fall back to legacy discovery by looking for __init__.py upwards as # it’s the only way given that source root was not found or was not provided while True: if not os.path.exists(os.path.join(dirname , \"__init__.py\")): return dirname old_dirname = dirname dirname = os.path.dirname(dirname) if old_dirname == dirname: return os.getcwd() @pytest.mark.parametrize( \"py_mod_base_name\", (\"__init__\", \"impl\"), ids=(\"explicit -namespace\", \"implicit -namespace\"), ) def test_discover_package_path_source_root_as_parent( py_mod_base_name: str, tmp_path: Any, ) -> None: \"\"\"Test␣discover_package_path␣when␣source␣root␣is␣a␣parent␣of␣the␣module.\"\"\" # Create this temporary structure: # /tmp_path/ # project/ # my-package/ # __init__.py project_dir = tmp_path / \"project\" package_dir = project_dir / \"mypackage\" package_dir.mkdir(parents=True) (package_dir / f\"{py_mod_base_name}.py\").touch() # Test with project_dir as source root (parent of package) result = discover_package_path(str(package_dir), [str(project_dir)]) assert result == str(project_dir)', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_23', type=<ChunkType.TEXT: 'text'>, content='Figure 6: Example of a successful gistified file: the code correctly handles all parameters, achieving 100% line–existence rate, a test F1 score of 100, and an execution rate of 65.5%.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_17_24', type=<ChunkType.TEXT: 'text'>, content='attempts to “cheat” the task by injecting a mock, in-memory version of the original codebase package to satisfy import dependencies, rather than copying the necessary code inline (e.g., Figure 12).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_0', type=<ChunkType.TEXT: 'text'>, content='Pytest Runtime Error', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_1', type=<ChunkType.TEXT: 'text'>, content='This error refers to failures that occur during pytest execution, such as syntax errors or fixture-related issues (e.g., Figure 9). Although the absence of test functions is also one of pytest failures, we explicitly separate those cases by first verifying the presence of the required test functions and running pytest only when they exist.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_3', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_5', type=<ChunkType.TEXT: 'text'>, content='import os import sys from contextlib import contextmanager from pathlib import Path def discover_package_path(path , _search_paths): \"\"\" ␣␣␣␣Minimal␣version:␣accept␣a␣filesystem␣path␣(string␣or␣Path)␣and␣return␣a␣Path ␣␣␣␣to␣that␣directory.␣This␣matches␣the␣usage␣in␣the␣specific␣test␣where␣a ␣␣␣␣path␣to␣the␣tests␣data␣directory␣is␣passed. ␣␣␣␣\"\"\" if isinstance(path , Path): p = path else: p = Path(path) if p.exists() and p.is_dir(): return p raise ValueError(\"Cannot␣determine␣package␣path\") @contextmanager def augmented_sys_path(paths): \"\"\" ␣␣␣␣Minimal␣context␣manager:␣set␣sys.path␣to␣the␣parent␣directories␣of␣the ␣␣␣␣provided␣paths␣(in␣order),␣restoring␣the␣original␣sys.path␣on␣exit. ␣␣␣␣\"\"\" original = list(sys.path) try: new = [] for p in paths: pth = Path(p) # Ensure we use the parent directory (the \"source root\" in the test) parent = pth.parent new.append(str(parent)) sys.path[:] = new yield finally: sys.path[:] = original @pytest.mark.parametrize( \"py_mod_base_name\", (\"__init__\"), ids=(\"explicit -namespace\"), ) def test_discover_package_path_source_root_as_parent(): # This test’s behavior is preserved: it uses discover_package_path on the # tests data directory and ensures augmented_sys_path sets sys.path to its # parent (the project/tests directory). TEST_DATA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"tests\", \"data\")) PROJECT_ROOT_DIR = os.path.abspath(os.path.join(TEST_DATA_DIR , \"..\")) with augmented_sys_path([discover_package_path(TEST_DATA_DIR , [])]): assert sys.path == [PROJECT_ROOT_DIR]', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_18_6', type=<ChunkType.TEXT: 'text'>, content='Figure 7: Example of failed gistified file: the code fails to import pytest. The model hallucinates the function test_discover_package_path_source_root_as_parent(), resulting in a test F1 score of 0 and a low line–existence rate of 28.0%', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_0', type=<ChunkType.TEXT: 'text'>, content='B.3 Tools Available in GitHub Copilot', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_1', type=<ChunkType.TEXT: 'text'>, content='Table 5 shows the list of available tools in Github Copilot.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_2', type=<ChunkType.TEXT: 'text'>, content='B.4 Change Test', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_3', type=<ChunkType.TEXT: 'text'>, content='even high performing models and frameworks (especially GPT-5 and GPT-5-mini) seems to modify test codes even though explicitly mentioned not to. We observed three common modification: (1) removing the test function but move the lines in the test function under the \"__main__\" guard (e.g., Figure 10), (2) adding the \"__main__\" guard even though unnecessary (e.g., Figure 11), and (3) mocking a minimal in-memory package to bypass missing dependencies and force the test to run (e.g., Figure 12).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_6', type=<ChunkType.TEXT: 'text'>, content='@click.option(\"--all-methods\", is_flag=True , help=\"Show␣HEAD␣and␣OPTIONS␣methods.\") @with_appcontext def routes_command(sort , all_methods): \"\"\"Show␣all␣registered␣routes␣with␣endpoints␣and␣methods.\"\"\" from flask import current_app rules = list(current_app.url_map.iter_rules()) if not rules: click.echo(\"No␣routes␣were␣registered.\") return', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_7', type=<ChunkType.TEXT: 'text'>, content='Figure 8: Example of an Import Error: the gistified file imports from the original repository (e.g., from flask import current_app).', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_8', type=<ChunkType.TEXT: 'text'>, content='T = t.TypeVar(\"T\")', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_9', type=<ChunkType.TEXT: 'text'>, content='class ConfigAttribute(t.Generic[T]): \"\"\"Makes␣an␣attribute␣forward␣to␣the␣config\"\"\" def __init__( self , name: str, get_converter: t.Callable[[t.Any], T] | None = None ) -> None: self.__name__ = name self.get_converter = get_converter (a) Original Test Case class ConfigAttribute: def __init__( self , name: str, get_converter: t.Callable[[t.Any], T] | None = None ) -> None: self.__name__ = name', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_10', type=<ChunkType.TEXT: 'text'>, content='self.get_converter = get_converter', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_11', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_12', type=<ChunkType.TEXT: 'text'>, content='Figure 9: Example of an Pytest Runtime Error: gistified file fails with error message E TypeError: ’ConfigAttribute’ is not subscriptable type', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_13', type=<ChunkType.TEXT: 'text'>, content='B.5 Additional Metrics', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_14', type=<ChunkType.TEXT: 'text'>, content='Table 6 shows the result of additional evaluation metrics, including the Average Pytest Pass Rate, which is defined as the average test pass rate over cases with at least one successful run, and the Test F1 Score, which quantifies the line-wise F1 existence between the test functions in the original codebase and those in the gistified fie.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_19_15', type=<ChunkType.TEXT: 'text'>, content='GPT-5 shows a notably higher Average Pytest Pass Rate, indicating that among the ones they successfully generate, they tend to pass all pytest. For the Test F1 Score, Claude-4 shows the highest performance, aliging with the trend discussed in Section 3.4.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_0', type=<ChunkType.TEXT: 'text'>, content='C Analysis', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_1', type=<ChunkType.TEXT: 'text'>, content='C.1 Effect of various strategies and tools', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_2', type=<ChunkType.TEXT: 'text'>, content='Prompt-Based Guidance We experiment with two variants of the prompt, Reading and Tracing, where, on top of the base prompt (Figure 3), we add specific instructions of How to Operate to encourage reasoning using a particular strategy. The addition prompt detail of Reading is in Figure 14, and for Tracing is in Figure 15.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_3', type=<ChunkType.TEXT: 'text'>, content='Global Information via Tools We experiment with two tools that provide global information: RepoGraph and Tracing. Details of the information provided to the model about each tool are shown in Figure 16.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_4', type=<ChunkType.TEXT: 'text'>, content='RepoGraph (Ouyang et al., 2024) is a plug-in module designed to help LLMs leverage the codebase-level structure. It parses code at the line level, extracts relationships, and constructs a graph where each node represents a line of code and each edge encodes dependencies between code definitions and their references.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_6', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_7', type=<ChunkType.TABLE: 'table'>, content='Tool Description copilot_getNotebookSummary Returns the list of Notebook cells with id, types, line ranges, language, execution info, and output mime types. Useful for getting cell IDs, execution order, and outputs. edit_notebook_file Edit an existing Notebook file in the workspace. Supports inserting, deleting, or editing cells while preserving whitespace and indentation. apply_patch Edit text files using a special diff/patch format. Do not use for Jupyter notebooks. semantic_search Run a natural language search for relevant code or documentation comments in the workspace. create_directory Create a new directory structure in the workspace (like mkdir -p). create_file Create a new file with specified content. Automatically creates directories if they do not exist. file_search Search for files in the workspace by glob pattern (e.g., **/*.js). Returns matching paths only. test_search For a source file, find the corresponding test file, and vice versa. grep_search Fast text or regex search in the workspace. Useful for exact string or regex queries. run_notebook_cell Run a code cell in a notebook file and return the output. Avoid running Markdown cells. read_notebook_cell_output Retrieve the latest output for a notebook cell, even if not run in the current session. get_search_view_results Returns results from the search view. github_repo Search a GitHub repository for relevant code snippets. Use only for external repos, not local workspaces. insert_edit_into_file Insert or edit code in an existing file using minimal hints, avoiding duplication of unchanged code. install_extension Install an extension in VS Code. Used only during workspace creation. list_dir List the contents of a directory (folders and files). create_new_jupyter_notebook Generate a new Jupyter Notebook (.ipynb) in VS Code. create_new_workspace Set up a complete new project (scaffolding, dependencies, config, boilerplate). get_project_setup_info Provides project setup information for a VS Code workspace after workspace creation. read_file Read the contents of a file. Supports offsets and limits for large files. open_simple_browser Preview or open a URL in VS Code’s Simple Browser. test_failure Include test failure information in the prompt. think Think deeply about a request and log structured reasoning (no execution). Useful for planning, debugging, and brainstorming. get_vscode_api Retrieve comprehensive VS Code API documentation and references for exten- sion development. run_vscode_command Run a VS Code command by ID with arguments. Used mainly in workspace creation. fetch_webpage Fetch main content from a webpage for summarization or analysis.', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>copilot__getNotebookSummary</th><th>Returns the list of Notebook cells with id, types, line ranges, language, execution info, and output mime types. Useful for getting cell IDs, execution order, and outputs.</th></tr></thead><tbody><tr><td>edit_ notebook _file</td><td>Edit an existing Notebook file in the workspace. Supports inserting, deleting, or editing cells while preserving whitespace and indentation.</td></tr><tr><td>apply_patch</td><td>Edit text files using a special diff/patch format. notebooks. Do not use for Jupyter</td></tr><tr><td>semantic_search</td><td>Run a natural the workspace. anguage search for relevant code or documentation comments in</td></tr><tr><td>create_ directory</td><td>Create a new directory structure in the workspace (like mkdir -p).</td></tr><tr><td>create_file</td><td>Create a new file with specified content. Automatically creates directories if they do not exist.</td></tr><tr><td>file_ search</td><td>Search for files in the workspace by glob pattern (e.g., **/*.js). Returns matching paths only.</td></tr><tr><td>test__search</td><td>For a source file, find the corresponding test file, and vice versa.</td></tr><tr><td>grep_search</td><td>Fast text or regex search in the workspace. Useful for exact string or regex queries.</td></tr><tr><td>run_notebook_cell</td><td>Run a code cell in a notebook file and return the output. Markdown cells. Avoid running</td></tr><tr><td>read_notebook_cell_ output</td><td>Retrieve the latest output for a notebook cell, even if not run in the current session.</td></tr><tr><td>get_search_view_results</td><td>Returns results from the search view.</td></tr><tr><td>github_repo</td><td>Search a GitHub repository for relevant code snippets. Use only for external repos, not local workspaces.</td></tr><tr><td>insert__edit_into_file</td><td>Insert or edit code in an existing file using minimal hints, avoiding duplication of unchanged code.</td></tr><tr><td>install_ extension</td><td>Install an extension in VS Code. Used only during workspace creation.</td></tr><tr><td>list__dir</td><td>List the contents of a directory (folders and files).</td></tr><tr><td>create_new__jupyter_notebook</td><td>Generate a new Jupyter Notebook (.ipynb) in VS Code.</td></tr><tr><td>create_new__workspace</td><td>Set up a complete new project (scaffolding, dependencies, config, boilerplate).</td></tr><tr><td>get__project_setup_ info</td><td>Provides project setup information for a VS Code workspace after workspace creation.</td></tr><tr><td>read_file</td><td>Read the contents of a file. Supports offsets and limits for large files.</td></tr><tr><td>open_simple_ browser</td><td>Preview or open a URL in VS Code’s Simple Browser.</td></tr><tr><td>test__failure</td><td>Include test failure information in the prompt.</td></tr><tr><td>think</td><td>Think deeply about a request and log structured reasoning (no execution). Useful for planning, debugging, and brainstorming.</td></tr><tr><td>get_vscode_api</td><td>Retrieve comprehensive VS Code API documentation and references for exten- sion development.</td></tr><tr><td>tun_vscode_command</td><td>Run a VS Code command by ID with arguments. Used mainly in workspace creation.</td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_8', type=<ChunkType.TEXT: 'text'>, content='Table 5: Available tools and their descriptions. We note that many tools available to the agent are never used. Table 6: Average Pytest Pass Rate and Test F1 Score of different models using SWE-Agent on the main table (Table 1) test dataset.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_9', type=<ChunkType.TABLE: 'table'>, content='Models Execution Fidelity Average Pytest Pass Rate Test F1 Score GPT-5-mini 30.9 49.2 47.9 GPT-5 30.7 88.8 45.0 Claude-3.7 40.7 61.9 55.9 Claude-4 56.7 72.2 60.0', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Models</th><th>Execution Fidelity</th><th>| Average</th><th>Pytest Pass</th><th>Rate</th><th>Test Fy Scoré</th></tr></thead><tbody><tr><td>GPT-5-mini</td><td>30.9</td><td></td><td>49.2</td><td></td><td></td></tr><tr><td>GPT-5</td><td>30.7</td><td></td><td>88.8</td><td></td><td></td></tr><tr><td>Claude-3.7</td><td>40.7</td><td></td><td>61.9</td><td></td><td></td></tr><tr><td>Claude-4</td><td>56.7</td><td></td><td>72.2</td><td></td><td></td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_11', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_12', type=<ChunkType.TEXT: 'text'>, content='class TestGetNetrcAuth:', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_13', type=<ChunkType.TEXT: 'text'>, content='def test_works(self , tmp_path , monkeypatch): netrc_path = tmp_path / \".netrc\" monkeypatch.setenv(\"NETRC\", str(netrc_path)) with open(netrc_path , \"w\") as f: f.write(\"machine␣example.com␣login␣aaaa␣password␣bbbb\\\\n\") auth = get_netrc_auth(\"http://example.com/thing\") assert auth == (\"aaaa\", \"bbbb\") (a) Original Test Case if __name__ == \"__main__\": # Reproduce tests/test_utils.py::TestGetNetrcAuth::test_works with tempfile.TemporaryDirectory() as tmpdir: netrc_path = os.path.join(tmpdir , \".netrc\") os.environ[\"NETRC\"] = netrc_path with open(netrc_path , \"w\") as f: f.write(\"machine␣example.com␣login␣aaaa␣password␣bbbb\\\\n\") auth = get_netrc_auth(\"http://example.com/thing\")', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_14', type=<ChunkType.TEXT: 'text'>, content='(a) Original Test Case', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_20_15', type=<ChunkType.TEXT: 'text'>, content='assert auth == (\"aaaa\", \"bbbb\")', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_0', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_1', type=<ChunkType.TEXT: 'text'>, content='Figure 10: Test Modification Case 1: The test TestGetNetrcAuth.test_works is converted from a pytest unit test into a standalone script.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_2', type=<ChunkType.TEXT: 'text'>, content='# Test class and method - preserved unchanged class TestArgparseOptionsProviderMixin: \"\"\"Tests␣for␣the␣argparse␣implementation␣of␣OptionsProviderMixIn. ␣␣␣␣The␣logger␣checker␣is␣used␣as␣an␣example␣checker␣for␣this␣implementation. ␣␣␣␣\"\"\" @staticmethod def test_logger_without_options() -> None: \"\"\"Check␣that␣we␣raise␣messages␣when␣we␣do␣not␣supply␣any␣options.\"\"\" with pytest.raises(SystemExit) as ex: Run([LOGGING_TEST]) assert ex.value.code == 2 # Main execution for pytest if __name__ == \"__main__\": test = TestArgparseOptionsProviderMixin() test.test_logger_without_options()', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_3', type=<ChunkType.TEXT: 'text'>, content='Figure 11: Test Modification Case 2: Adding unnecessary \"__main__\" guard', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_4', type=<ChunkType.TEXT: 'text'>, content='Thereby, when given a specific module, it returns the relationship with other modules as represented within the constructed graph.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_5', type=<ChunkType.TEXT: 'text'>, content='Tracing is a tool that uses the tracer provided from the sys module to execute a command and track which components of the codebase are accessed. When the model uses the tool with a specific command, the tool provides the model with the files and functions touched when running the command, in the order in which they are encountered.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_6', type=<ChunkType.TEXT: 'text'>, content='Execution-Based Tools We experiment with two execution-based tools: the Bash tool and the Edit and Execute tool.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_7', type=<ChunkType.TEXT: 'text'>, content='The Bash tool is a basic utility that allows the model to invoke any necessary Bash commands. In contrast, the Edit and Execute tool is designed specifically for working with the gistified file: it enables the model to create or modify the gistified file and optionally execute it to verify changes.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_8', type=<ChunkType.TEXT: 'text'>, content='The primary difference between the two tools is their scope of execution. The Bash tool can run commands on both the original codebase and the gistified file, whereas the Edit and Execute tool is restricted to executing only the gistified file.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_9', type=<ChunkType.TEXT: 'text'>, content='We include an example of the behavior observed when adding the execution tool in Figure 17. Common patterns we observe are: (1) the model first runs the provided command to identify which files are accessed', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_11', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_12', type=<ChunkType.TEXT: 'text'>, content='# Create a minimal in-memory ’requests’ package with required submodules. requests_mod = types.ModuleType(’requests’) requests_mod.__path__ = [] compat_mod = types.ModuleType(’requests.compat’) structures_mod = types.ModuleType(’requests.structures’) # Populate compat with only what’s needed by this test suite import paths. compat_mod.Mapping = Mapping compat_mod.MutableMapping = MutableMapping compat_mod.urljoin = urljoin # Populate structures with the classes. structures_mod.CaseInsensitiveDict = CaseInsensitiveDict structures_mod.LookupDict = LookupDict # Wire the package hierarchy and register in sys.modules. requests_mod.compat = compat_mod requests_mod.structures = structures_mod sys.modules[’requests’] = requests_mod sys.modules[’requests.compat’] = compat_mod sys.modules[’requests.structures’] = structures_mod if __name__ == ’__main__’: import pytest', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_13', type=<ChunkType.TEXT: 'text'>, content='raise SystemExit(pytest.main([’-q’, ’tests/test_structures.py::TestCaseInsensitiveDict::test_list’]))', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_14', type=<ChunkType.TEXT: 'text'>, content='Figure 12: Test Modification Case 3: Manually mocking a minimal in-memory package to bypass missing dependencies and force the test to run.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_15', type=<ChunkType.TEXT: 'text'>, content='@pytest.mark.parametrize( \"value ,␣expected\", ( (’foo=\"is␣a␣fish\",␣bar=\"as␣well\"’, {\"foo\": \"is␣a␣fish\", \"bar\": \"as␣well\"}), (\"key_without_value\", {\"key_without_value\": None}), ), ) def test_parse_dict_header(value , expected): assert parse_dict_header(value) == expected', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_16', type=<ChunkType.TEXT: 'text'>, content='(a) Original Test Case', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_21_17', type=<ChunkType.TEXT: 'text'>, content='assert parse_dict_header(’foo=\"is␣a␣fish\",␣bar=\"as␣well\"’) == {\"foo\": \"is␣a␣fish\", \"bar\": \"as␣well\"} assert parse_dict_header(\"key_without_value\") == {\"key_without_value\": None}', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_0', type=<ChunkType.TEXT: 'text'>, content='(b) Gistified File', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_1', type=<ChunkType.TEXT: 'text'>, content='Figure 13: The test function test_parse_dict_header is simplified: in the original, @pytest.mark.parametrize to feed multiple input/expected pairs into one function; in the gistified version, this is replaced with two direct assert statements, one per case. it used', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_2', type=<ChunkType.TEXT: 'text'>, content='Table 7: Analysis of tool usage during the Gistify task', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_3', type=<ChunkType.TABLE: 'table'>, content='Models Avg. tool usage view search execute GPT-5-mini 10.8 71.9 9.8 1.7 16.6 GPT-5 18.5 72.4 8.3 3.3 16.1 17.3 67.5 10.1 4.5 17.9 Claude-Sonnet-4 19.3 74.6 2.1 11.8 11.5', summary='', caption=None, image_paths=None, metadata={'html': '<table><thead><tr><th>Models</th><th>Avg.</th><th>tool usage</th><th>| view</th><th>search</th><th>execute</th><th>other</th></tr></thead><tbody><tr><td>GPT-5-mini</td><td></td><td>10.8</td><td>71.9</td><td>9.8</td><td>L7</td><td>16.6</td></tr><tr><td>GPT-5</td><td></td><td>18.5</td><td>72.4</td><td>8.3</td><td>3.3</td><td>16.1</td></tr><tr><td>Slaude-Sonnet-3.7</td><td></td><td>17.3</td><td>67.5</td><td>10.1</td><td>4.5</td><td>17.9</td></tr><tr><td>Claude-Sonnet-4</td><td></td><td>19.3</td><td>74.6</td><td>2.1</td><td>11.8</td><td>11.5</td></tr></tbody></table>'}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_4', type=<ChunkType.TEXT: 'text'>, content='Claude-Sonnet-3.7', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_5', type=<ChunkType.TEXT: 'text'>, content='other', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_6', type=<ChunkType.TEXT: 'text'>, content='and to gather execution feedback; (2) after creating a file, it iteratively executes it to verify that the generated gistified file behaves as expected; and (3) it repeatedly compares the outputs of the gistified file and the original codebase under the given command. We also observe that, due to this iterative checking process, enabling the execution tool often leads the model to terminate because it reaches the maximum step limit.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_8', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_9', type=<ChunkType.TEXT: 'text'>, content='Behavior Reading', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_10', type=<ChunkType.TEXT: 'text'>, content='How to Operate:', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_11', type=<ChunkType.TEXT: 'text'>, content='1. Examine the test file and the test function used for {problem statement}', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_12', type=<ChunkType.TEXT: 'text'>, content='2. Identify which module used by these functions are defined in {working dir}', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_13', type=<ChunkType.TEXT: 'text'>, content='3. Copy and inline the code from those modules into ‘concise.py’', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_14', type=<ChunkType.TEXT: 'text'>, content='4. Check these modules for any internal functions or classes and inline them as needed.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_15', type=<ChunkType.TEXT: 'text'>, content='5. Repeat this process recursively until all internal dependencies are inlined.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_16', type=<ChunkType.TEXT: 'text'>, content='6. Do not forget to copy and paste external imports.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_17', type=<ChunkType.TEXT: 'text'>, content='Figure 14: Prompt for Reading strategy.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_18', type=<ChunkType.TEXT: 'text'>, content='Trace Reasoning', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_19', type=<ChunkType.TEXT: 'text'>, content='How to Operate:', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_20', type=<ChunkType.TEXT: 'text'>, content='1. Predict the execution traces.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_21', type=<ChunkType.TEXT: 'text'>, content='2. Follow the traces and inline (copy) only the necessary executed lines into ‘concise.py’', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_22', type=<ChunkType.TEXT: 'text'>, content='3. Repeat until all traces are fully handled.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_23', type=<ChunkType.TEXT: 'text'>, content='Figure 15: Prompt for Tracing strategy.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_24', type=<ChunkType.TEXT: 'text'>, content='C.2 Tool Usage Rates', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_25', type=<ChunkType.TEXT: 'text'>, content='Table 7 shows the statistics on tool usage across models using SWE-bench. We group various tools into four categories: view, search, execute, and other, which includes all remaining tools. For all models, we compute usage rates both with and without execution enabled, and then average across the two settings.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_22_26', type=<ChunkType.TEXT: 'text'>, content='Among all models, Claude-4 exhibits the highest average tool usage for each test cases, followed by GPT-5, Claude-3.7, and GPT-5-mini. In terms of specific functionality, Claude-4 shows the highest rate of both view and execute tool usage, while Claude-3.7 shows the highest usage of the search tool. To generate a high-quality gistified file, a model must effectively view relevant files and copy only the necessary content. The strong performance of Claude-4 on line existence may be related to its high usage of the view tool. Also, the execution tool tends to support correctness verification of the generated file, which would lead to high execution fidelity.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_0', type=<ChunkType.TEXT: 'text'>, content='Tool Information', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_1', type=<ChunkType.TEXT: 'text'>, content='* search repo (repograph): searches in the mentioned repository with a specific function or class, and returns the def and ref relations for the search term.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_2', type=<ChunkType.TEXT: 'text'>, content='* trace info (tracing): returns the trace log when running the ‘pytest’ command for the test currently being debugged in the given repo', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_3', type=<ChunkType.TEXT: 'text'>, content='Figure 16: Details of the information provided to the model about each tool.', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_5', type=<ChunkType.TEXT: 'text'>, content='Gistify! Codebase-Level Understanding via Runtime Execution', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_6', type=<ChunkType.TEXT: 'text'>, content='{ \"role\": \"assistant\", \"tool_calls\": { \"arguments\": \"{\\\\\"command\\\\\":\\\\\"cd␣/app␣&&␣python␣-m␣pytest␣ concise.py::TestDeprecationOptions::test_new_names␣-v\\\\\"}\", \"name\": \"bash\" } }, { \"role\": \"tool\", \"content\": [ { \"type\": \"text\", \"text\": \"OBSERVATION:␣test␣session␣starts␣...AssertionError\\\\n...pylint:␣error:␣unrecognized␣ arguments:␣/app/regrtest_data/empty.py\\\\n␣FAILED␣ concise.py::TestDeprecationOptions::test_new_names␣...\" } }, ... (fix concise.py based on observation) ... { \"role\": \"assistant\", \"tool_calls\": { \"arguments\": \"{\\\\\"command\\\\\":\\\\\"cd␣/app␣&&␣python␣-m␣pytest␣ tests/config/test_argparse_config.py::TestDeprecationOptions::test_new_names␣-v␣-s\\\\\"}\", \"name\": \"bash\" } }, ... { \"role\": \"assistant\", \"content\": \"Excellent!␣The␣messages␣are␣identical.␣Let␣me␣verify␣my␣implementation␣removes␣all␣ unused␣imports␣and␣only␣includes␣what’s␣needed:\"', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_7', type=<ChunkType.TEXT: 'text'>, content='}', summary='', caption=None, image_paths=None, metadata={}), DocumentChunk(paper_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59', chunk_id='bd485724-c549-4a89-8bc0-83c4e5f9fe59_23_8', type=<ChunkType.TEXT: 'text'>, content='Figure 17: Trajectory when including execution tool. Models often iteratively execute the given codebase or generated gistified file to ensure that it operates expectedly.', summary='', caption=None, image_paths=None, metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(chunks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arxiv-insight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
